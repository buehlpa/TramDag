{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with GPU support.\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "proj_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if proj_root not in sys.path:\n",
    "    sys.path.insert(0, proj_root)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import logistic\n",
    "from scipy.special import logit\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Train with GPU support.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU found, train with CPU support.\")\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# own utils\n",
    "from utils.graph import *\n",
    "from utils.tram_models import *\n",
    "from utils.tram_model_helpers import *\n",
    "from utils.tram_data import *\n",
    "from utils.continous import *\n",
    "from utils.sampling_tram_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgp_sklearn(nobs=1000, nvars=7, seed=42):\n",
    "    X, _ = make_blobs(n_samples=nobs, n_features=nvars, centers=1,cluster_std=1.0, random_state=seed)\n",
    "    cols = [f'x{i+1}' for i in range(nvars)]\n",
    "    return pd.DataFrame(X, columns=cols)\n",
    "\n",
    "df=dgp_sklearn(nobs=1000, nvars=2, seed=42)\n",
    "print(df.info())\n",
    "\n",
    "\n",
    "data_type= {'x1':'cont','x2':'cont'} # cont:continous, ord:ordinal, oher:everything else than images\n",
    "\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "quantiles = train_df.quantile([0.05, 0.95])\n",
    "min_vals = quantiles.loc[0.05]\n",
    "max_vals = quantiles.loc[0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee587c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_write_test_dict_to_json(input:dict, test_name,file_path = 'twovars_model_loader_test_dict.json'):\n",
    "    \"\"\"\n",
    "    input has to bea dictionary with the following structure:\n",
    "                                \"input\":{\n",
    "                                                'x1': {\n",
    "                                                'data_type': 'cont',\n",
    "                                                'node_type': 'source',\n",
    "                                                'parents': [],\n",
    "                                                'parents_datatype': {},\n",
    "                                                'transformation_terms_in_h()': {},\n",
    "                                                'transformation_term_nn_models_in_h()': {}},\n",
    "                                                \n",
    "                                For n nodes\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON file: {e}\")\n",
    "    \n",
    "    data[test_name]={}\n",
    "    data[test_name].setdefault(\"input\", {})\n",
    "    data[test_name].setdefault(\"output\", {})\n",
    "\n",
    "    \n",
    "    data[test_name][\"input\"] = input\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for node_name in input:\n",
    "        model = get_fully_specified_tram_model(node_name, input, verbose=False).to(device)\n",
    "        data[test_name][\"output\"][node_name] = repr(model)\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "        print(f\"Updated outputs written to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "        \n",
    "        \n",
    "def run_model_loader_test(test_name: str, testdict_path: str, device: torch.device = None):\n",
    "    \"\"\"\n",
    "    General test for fully_specified_tram_model loader based on ground-truth JSON.\n",
    "\n",
    "    Args:\n",
    "        test_name: Key in the JSON file identifying the test case.\n",
    "        testdict_path: Path to the JSON file containing input and expected output.\n",
    "        device: Torch device to move models to; defaults to CUDA if available, else CPU.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If any model's repr() does not match the expected output.\n",
    "        ValueError: If the test_name is not found in the JSON file.\n",
    "    \"\"\"\n",
    "    # Load ground-truth data\n",
    "    with open(testdict_path, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    if test_name not in test_data:\n",
    "        raise ValueError(f\"Test name '{test_name}' not found in {testdict_path}\")\n",
    "    \n",
    "\n",
    "    inputs = test_data[test_name]['input']\n",
    "    expected_outputs = test_data[test_name].get('output', {})\n",
    "\n",
    "    # Determine device\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Iterate and compare\n",
    "    for node_name in inputs:\n",
    "        model = get_fully_specified_tram_model(node_name, inputs, verbose=False).to(device)\n",
    "        actual_repr = repr(model)\n",
    "        expected_repr = expected_outputs.get(node_name)\n",
    "\n",
    "        assert expected_repr is not None, (\n",
    "            f\"No expected output found for node '{node_name}' \"\n",
    "            f\"in '{testdict_path}'.\"\n",
    "        )\n",
    "        assert actual_repr == expected_repr, (\n",
    "            f\"Mismatch for node '{node_name}':\\n\"\n",
    "            f\"  Expected: {expected_repr}\\n\"\n",
    "            f\"  Actual:   {actual_repr}\"\n",
    "        )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2827f9",
   "metadata": {},
   "source": [
    "# Testing 2 variables\n",
    "4 tests SI LS, CS and CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ff5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS\n",
    "input={\n",
    "        'x1': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x2': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'sink',\n",
    "        'parents': ['x1'],\n",
    "        'parents_datatype': {'x1': 'cont'},\n",
    "        'transformation_terms_in_h()': {'x1': 'ls'},\n",
    "        'transformation_term_nn_models_in_h()': {'x1': 'LinearShift'}}\n",
    "        }\n",
    "\n",
    "load_and_write_test_dict_to_json(input, \"test_2\",file_path = 'twovars_model_loader_test_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5724ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS\n",
    "input={\n",
    "        'x1': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x2': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'sink',\n",
    "        'parents': ['x1'],\n",
    "        'parents_datatype': {'x1': 'cont'},\n",
    "        'transformation_terms_in_h()': {'x1': 'ls'},\n",
    "        'transformation_term_nn_models_in_h()': {'x1': 'ComplexShiftDefaultTabular'}}\n",
    "        }\n",
    "\n",
    "load_and_write_test_dict_to_json(input, \"test_2\",file_path = 'twovars_model_loader_test_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42475056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CI\n",
    "input={\n",
    "        'x1': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x2': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'sink',\n",
    "        'parents': ['x1'],\n",
    "        'parents_datatype': {'x1': 'cont'},\n",
    "        'transformation_terms_in_h()': {'x1': 'ci'},\n",
    "        'transformation_term_nn_models_in_h()': {'x1': 'ComplexInterceptDefaultTabular'}}\n",
    "        }\n",
    "\n",
    "load_and_write_test_dict_to_json(input, \"test_3\",file_path = 'twovars_model_loader_test_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00f9d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tests all the 3 cases for SI LS CS and CI\n",
    "\n",
    "for test in ['test_1', 'test_2', 'test_3']:\n",
    "    run_model_loader_test(test, 'twovars_model_loader_test_dict.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a7840",
   "metadata": {},
   "source": [
    "# 3vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "43628c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_base_model_class(class_name: str):\n",
    "    \"\"\"\n",
    "    Strip trailing digits from a class name to get its base name.\n",
    "    e.g. \"cs12\" → \"cs\"\n",
    "    \"\"\"\n",
    "    for i, c in enumerate(class_name):\n",
    "        if c.isdigit():\n",
    "            return class_name[:i]\n",
    "    return class_name\n",
    "\n",
    "def group_by_base(term_dict, prefixes):\n",
    "    \"\"\"\n",
    "    Group features by their h_term “base,” but if the h_term is exactly\n",
    "    equal to one of the prefixes (e.g. \"cs\" or \"ls\"), keep each feature separate.\n",
    "\n",
    "    :param term_dict: { feature_name: { 'h_term': h_term, ... }, ... }\n",
    "    :param prefixes:  single prefix or iterable of prefixes, e.g. \"cs\" or (\"cs\",\"ls\")\n",
    "    :return: defaultdict(list) mapping group key → list of (feature_name, conf) pairs\n",
    "    \"\"\"\n",
    "    if isinstance(prefixes, str):\n",
    "        prefixes = (prefixes,)\n",
    "    groups = defaultdict(list)\n",
    "\n",
    "    for feat, conf in term_dict.items():\n",
    "        h_term = conf['h_term']\n",
    "        for prefix in prefixes:\n",
    "            if h_term.startswith(prefix):\n",
    "                # Case 1: exact prefix match → separate group per feature\n",
    "                if h_term == prefix:\n",
    "                    key = feat\n",
    "                # Case 2: prefix plus a digit → group by prefix+first digit, e.g. \"cs11\",\"cs12\" → \"cs1\"\n",
    "                elif len(h_term) > len(prefix) and h_term[len(prefix)].isdigit():\n",
    "                    key = h_term[:len(prefix) + 1]\n",
    "                # Case 3: anything else (e.g. \"csA\", \"lsXyz\") → group by full h_term\n",
    "                else:\n",
    "                    key = h_term\n",
    "                groups[key].append((feat, conf))\n",
    "                break\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "# Example integration in your tram‐model builder:\n",
    "def get_fully_specified_tram_model(node: str, target_nodes: dict, verbose=True):\n",
    "    # Source nodes get a simple intercept only\n",
    "    if target_nodes[node]['node_type'] == 'source':\n",
    "        nn_int = SimpleIntercept()\n",
    "        model = TramModel(nn_int, None)\n",
    "        if verbose:\n",
    "            print(\"Source → SimpleIntercept only\")\n",
    "        return model\n",
    "\n",
    "    # Otherwise gather terms and model names\n",
    "    _, terms_dict, model_names_dict = ordered_parents(node, target_nodes)\n",
    "    model_dict = merge_transformation_dicts(terms_dict, model_names_dict)\n",
    "\n",
    "    # Split intercepts vs. shifts\n",
    "    intercepts_dict = {\n",
    "        k: v for k, v in model_dict.items()\n",
    "        if \"ci\" in v['h_term'] or \"si\" in v['h_term']\n",
    "    }\n",
    "    shifts_dict = {\n",
    "        k: v for k, v in model_dict.items()\n",
    "        if \"ci\" not in v['h_term'] and \"si\" not in v['h_term']\n",
    "    }\n",
    "\n",
    "    # Build intercept network\n",
    "    intercept_groups = group_by_base(intercepts_dict, prefixes=(\"ci\", \"si\"))\n",
    "    if not intercept_groups:\n",
    "        nn_int = SimpleIntercept()\n",
    "    else:\n",
    "        if len(intercept_groups) > 1:\n",
    "            raise ValueError(\"Multiple intercept models detected; only one is supported.\")\n",
    "        feats = next(iter(intercept_groups.values()))\n",
    "        cls_name = feats[0][1]['class_name']\n",
    "        base = get_base_model_class(cls_name)\n",
    "        nn_int = globals()[base](n_features=len(feats))\n",
    "\n",
    "    # Build shift networks (handles both \"cs\" and \"ls\")\n",
    "    shift_groups = group_by_base(shifts_dict, prefixes=(\"cs\", \"ls\"))\n",
    "    nn_shifts = []\n",
    "    for feats in shift_groups.values():\n",
    "        cls_name = feats[0][1]['class_name']\n",
    "        base = get_base_model_class(cls_name)\n",
    "        nn_shifts.append(globals()[base](n_features=len(feats)))\n",
    "\n",
    "    # Combine into TramModel\n",
    "    tram_model = TramModel(nn_int, nn_shifts)\n",
    "    if verbose:\n",
    "        print(\"Constructed TRAM model:\", tram_model)\n",
    "    return tram_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f10aee70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated outputs written to threevars_model_loader_test_dict.json\n"
     ]
    }
   ],
   "source": [
    "# TODO here is a bug when 2 linera shifts are used in the same model, e.g. when we have a model like this:\n",
    "\n",
    "#  X3 ~ LS(X1) + LS(X2)\n",
    "input={\n",
    "        'x1': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x2': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x3': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'sink',\n",
    "        'parents': ['x1','x2'],\n",
    "        'parents_datatype': {'x1': 'cont','x2': 'cont'},\n",
    "        'transformation_terms_in_h()': {'x1': 'ls','x2': 'ls'},\n",
    "        'transformation_term_nn_models_in_h()': {'x1': 'LinearShift','x2': 'LinearShift'}}\n",
    "        }\n",
    "\n",
    "load_and_write_test_dict_to_json(input, \"test_1\", file_path = 'threevars_model_loader_test_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8c6626e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated outputs written to threevars_model_loader_test_dict.json\n"
     ]
    }
   ],
   "source": [
    "#  X3 ~ CS(X1) + LS(X2)\n",
    "input={\n",
    "        'x1': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x2': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x3': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'sink',\n",
    "        'parents': ['x1','x2'],\n",
    "        'parents_datatype': {'x1': 'cont','x2': 'cont'},\n",
    "        'transformation_terms_in_h()': {'x1': 'cs','x2': 'ls'},\n",
    "        'transformation_term_nn_models_in_h()': {'x1': 'ComplexShiftDefaultTabular','x2': 'LinearShift'}}\n",
    "        }\n",
    "\n",
    "load_and_write_test_dict_to_json(input, \"test_2\", file_path = 'threevars_model_loader_test_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4b7050d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated outputs written to threevars_model_loader_test_dict.json\n"
     ]
    }
   ],
   "source": [
    "#  X3 ~ CS(X1) + CS(X2)\n",
    "input={\n",
    "        'x1': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x2': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x3': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'sink',\n",
    "        'parents': ['x1','x2'],\n",
    "        'parents_datatype': {'x1': 'cont','x2': 'cont'},\n",
    "        'transformation_terms_in_h()': {'x1': 'cs','x2': 'cs'},\n",
    "        'transformation_term_nn_models_in_h()': {'x1': 'ComplexShiftDefaultTabular','x2': 'ComplexShiftDefaultTabular'}}\n",
    "        }\n",
    "\n",
    "load_and_write_test_dict_to_json(input, \"test_3\", file_path = 'threevars_model_loader_test_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6feb5209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated outputs written to threevars_model_loader_test_dict.json\n"
     ]
    }
   ],
   "source": [
    "#  X3 ~ CS(X1) + CI(X2)\n",
    "input={\n",
    "        'x1': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x2': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x3': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'sink',\n",
    "        'parents': ['x1','x2'],\n",
    "        'parents_datatype': {'x1': 'cont','x2': 'cont'},\n",
    "        'transformation_terms_in_h()': {'x1': 'cs','x2': 'ci'},\n",
    "        'transformation_term_nn_models_in_h()': {'x1': 'ComplexShiftDefaultTabular','x2': 'ComplexInterceptDefaultTabular'}}\n",
    "        }\n",
    "\n",
    "load_and_write_test_dict_to_json(input, \"test_4\", file_path = 'threevars_model_loader_test_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c8655ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated outputs written to threevars_model_loader_test_dict.json\n"
     ]
    }
   ],
   "source": [
    "#  X3 ~ CS11(X1) + CS12(X2)\n",
    "input={\n",
    "        'x1': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x2': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x3': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'sink',\n",
    "        'parents': ['x1','x2'],\n",
    "        'parents_datatype': {'x1': 'cont','x2': 'cont'},\n",
    "        'transformation_terms_in_h()': {'x1': 'cs11','x2': 'cs12'},\n",
    "        'transformation_term_nn_models_in_h()': {'x1': 'ComplexShiftDefaultTabular11','x2': 'ComplexShiftDefaultTabular12'}}\n",
    "        }\n",
    "\n",
    "load_and_write_test_dict_to_json(input, \"test_5\", file_path = 'threevars_model_loader_test_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6814e3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated outputs written to threevars_model_loader_test_dict.json\n"
     ]
    }
   ],
   "source": [
    "#  X3 ~ CI11(X1) + CI12(X2)\n",
    "input={\n",
    "        'x1': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x2': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x3': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'sink',\n",
    "        'parents': ['x1','x2'],\n",
    "        'parents_datatype': {'x1': 'cont','x2': 'cont'},\n",
    "        'transformation_terms_in_h()': {'x1': 'ci11','x2': 'ci12'},\n",
    "        'transformation_term_nn_models_in_h()': {'x1': 'ComplexInterceptDefaultTabular11','x2': 'ComplexInterceptDefaultTabular12'}}\n",
    "        }\n",
    "\n",
    "load_and_write_test_dict_to_json(input, \"test_6\", file_path = 'threevars_model_loader_test_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5a84be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tests all the 3 cases for SI LS CS and CI\n",
    "\n",
    "for test in ['test_1', 'test_2', 'test_3', 'test_4', 'test_5', 'test_6']:\n",
    "    run_model_loader_test(test, 'threevars_model_loader_test_dict.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8946173e",
   "metadata": {},
   "source": [
    "11 variables\n",
    "\n",
    "testing cases for multiple groups\n",
    "\n",
    "like ci11 ci12 , cs11 cs12, cs21 cs22, ls , cs , cs31, cs32 , "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "46e6973f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated outputs written to tenvars_model_loader_test_dict.json\n"
     ]
    }
   ],
   "source": [
    "#  X3 ~ CI11(X1) + CI12(X2)\n",
    "input={\n",
    "        'x1': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x2': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x3': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x4': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x5': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x6': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x7': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x8': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x9': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x10': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'sink',\n",
    "        'parents': ['x1','x2'],\n",
    "        'parents_datatype': {'x1': 'cont','x2': 'cont','x2': 'cont','x3': 'cont','x4': 'cont','x5': 'cont','x6': 'cont','x7': 'cont','x8': 'cont','x9': 'cont'},\n",
    "        'transformation_terms_in_h()': {'x1': 'ci12',\n",
    "                                        'x2': 'cs12',\n",
    "                                        'x3': 'ls',\n",
    "                                        'x4': 'cs11',\n",
    "                                        'x5': 'cs',\n",
    "                                        'x6': 'cs22',\n",
    "                                        'x7': 'cs21',\n",
    "                                        'x8': 'cs32',\n",
    "                                        'x9': 'ci11'},\n",
    "        \n",
    "        'transformation_term_nn_models_in_h()': {'x1': 'ComplexInterceptDefaultTabular12',\n",
    "                                                 'x2': 'ComplexShiftDefaultTabular12',\n",
    "                                                 'x3': 'LinearShift',\n",
    "                                                 'x4': 'ComplexShiftDefaultTabular11',\n",
    "                                                 'x5': 'ComplexShiftDefaultTabular',\n",
    "                                                 'x6': 'ComplexShiftDefaultTabular22',\n",
    "                                                 'x7': 'ComplexShiftDefaultTabular21',\n",
    "                                                 'x8': 'ComplexShiftDefaultTabular',\n",
    "                                                 'x9': 'ComplexInterceptDefaultTabular11'}}\n",
    "        }\n",
    "\n",
    "load_and_write_test_dict_to_json(input, \"test_1\", file_path = 'tenvars_model_loader_test_dict.json')\n",
    "\n",
    "# BUG 2 should be two groups of complex shifts-> fix and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ee342485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TramModel(\n",
      "  (nn_int): ComplexInterceptDefaultTabular(\n",
      "    (fc1): Linear(in_features=2, out_features=8, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (relu2): ReLU()\n",
      "    (fc3): Linear(in_features=8, out_features=20, bias=False)\n",
      "  )\n",
      "  (nn_shift): ModuleList(\n",
      "    (0): ComplexShiftDefaultTabular(\n",
      "      (fc1): Linear(in_features=1, out_features=64, bias=True)\n",
      "      (relu1): ReLU()\n",
      "      (dropout1): Dropout(p=0.3, inplace=False)\n",
      "      (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (relu2): ReLU()\n",
      "      (dropout2): Dropout(p=0.3, inplace=False)\n",
      "      (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (relu3): ReLU()\n",
      "      (dropout3): Dropout(p=0.3, inplace=False)\n",
      "      (fc4): Linear(in_features=64, out_features=1, bias=False)\n",
      "    )\n",
      "    (1-2): 2 x ComplexShiftDefaultTabular(\n",
      "      (fc1): Linear(in_features=2, out_features=64, bias=True)\n",
      "      (relu1): ReLU()\n",
      "      (dropout1): Dropout(p=0.3, inplace=False)\n",
      "      (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (relu2): ReLU()\n",
      "      (dropout2): Dropout(p=0.3, inplace=False)\n",
      "      (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (relu3): ReLU()\n",
      "      (dropout3): Dropout(p=0.3, inplace=False)\n",
      "      (fc4): Linear(in_features=64, out_features=1, bias=False)\n",
      "    )\n",
      "    (3): ComplexShiftDefaultTabular(\n",
      "      (fc1): Linear(in_features=1, out_features=64, bias=True)\n",
      "      (relu1): ReLU()\n",
      "      (dropout1): Dropout(p=0.3, inplace=False)\n",
      "      (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (relu2): ReLU()\n",
      "      (dropout2): Dropout(p=0.3, inplace=False)\n",
      "      (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (relu3): ReLU()\n",
      "      (dropout3): Dropout(p=0.3, inplace=False)\n",
      "      (fc4): Linear(in_features=64, out_features=1, bias=False)\n",
      "    )\n",
      "    (4): LinearShift(\n",
      "      (fc): Linear(in_features=1, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"TramModel(\\n  (nn_int): ComplexInterceptDefaultTabular(\\n    (fc1): Linear(in_features=2, out_features=8, bias=True)\\n    (relu1): ReLU()\\n    (fc2): Linear(in_features=8, out_features=8, bias=True)\\n    (relu2): ReLU()\\n    (fc3): Linear(in_features=8, out_features=20, bias=False)\\n  )\\n  (nn_shift): ModuleList(\\n    (0): ComplexShiftDefaultTabular(\\n      (fc1): Linear(in_features=1, out_features=64, bias=True)\\n      (relu1): ReLU()\\n      (dropout1): Dropout(p=0.3, inplace=False)\\n      (fc2): Linear(in_features=64, out_features=128, bias=True)\\n      (relu2): ReLU()\\n      (dropout2): Dropout(p=0.3, inplace=False)\\n      (fc3): Linear(in_features=128, out_features=64, bias=True)\\n      (relu3): ReLU()\\n      (dropout3): Dropout(p=0.3, inplace=False)\\n      (fc4): Linear(in_features=64, out_features=1, bias=False)\\n    )\\n    (1-2): 2 x ComplexShiftDefaultTabular(\\n      (fc1): Linear(in_features=2, out_features=64, bias=True)\\n      (relu1): ReLU()\\n      (dropout1): Dropout(p=0.3, inplace=False)\\n      (fc2): Linear(in_features=64, out_features=128, bias=True)\\n      (relu2): ReLU()\\n      (dropout2): Dropout(p=0.3, inplace=False)\\n      (fc3): Linear(in_features=128, out_features=64, bias=True)\\n      (relu3): ReLU()\\n      (dropout3): Dropout(p=0.3, inplace=False)\\n      (fc4): Linear(in_features=64, out_features=1, bias=False)\\n    )\\n    (3): ComplexShiftDefaultTabular(\\n      (fc1): Linear(in_features=1, out_features=64, bias=True)\\n      (relu1): ReLU()\\n      (dropout1): Dropout(p=0.3, inplace=False)\\n      (fc2): Linear(in_features=64, out_features=128, bias=True)\\n      (relu2): ReLU()\\n      (dropout2): Dropout(p=0.3, inplace=False)\\n      (fc3): Linear(in_features=128, out_features=64, bias=True)\\n      (relu3): ReLU()\\n      (dropout3): Dropout(p=0.3, inplace=False)\\n      (fc4): Linear(in_features=64, out_features=1, bias=False)\\n    )\\n    (4): LinearShift(\\n      (fc): Linear(in_features=1, out_features=1, bias=False)\\n    )\\n  )\\n)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ce304",
   "metadata": {},
   "outputs": [],
   "source": [
    "input=\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad8623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93848a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = {'x2': 'cont','x3': 'cont','x4': 'cont','x5': 'cont','x6': 'cont','x7': 'cont','x8': 'cont','x9': 'cont'}\n",
    "adj_matrix = np.array([\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"ls\"],   # x1 → x2 (ci), x1 → x3 (ls)\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"cs12\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"cs21\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"cs22\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"ci12\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"ci11\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"cs11\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"0\"]\n",
    "], dtype=object)\n",
    "\n",
    "plot_seed = 42\n",
    "plot_dag(adj_matrix, data_type, seed=plot_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are Ci or Compelx shifts in the models. If yes define the modelnames\n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "plot_nn_names_matrix(nn_names_matrix,data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in conf_dict:\n",
    "\n",
    "tram_model = get_fully_specified_tram_model(node, conf_dict, verbose=True).to(device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
