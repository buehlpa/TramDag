{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with GPU support.\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "proj_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if proj_root not in sys.path:\n",
    "    sys.path.insert(0, proj_root)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import logistic\n",
    "from scipy.special import logit\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Train with GPU support.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU found, train with CPU support.\")\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# own utils\n",
    "from utils.graph import *\n",
    "from utils.tram_models import *\n",
    "from utils.tram_model_helpers import *\n",
    "from utils.tram_data import *\n",
    "from utils.continous import *\n",
    "from utils.sampling_tram_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgp_sklearn(nobs=1000, nvars=7, seed=42):\n",
    "    X, _ = make_blobs(n_samples=nobs, n_features=nvars, centers=1,cluster_std=1.0, random_state=seed)\n",
    "    cols = [f'x{i+1}' for i in range(nvars)]\n",
    "    return pd.DataFrame(X, columns=cols)\n",
    "\n",
    "df=dgp_sklearn(nobs=1000, nvars=2, seed=42)\n",
    "print(df.info())\n",
    "\n",
    "\n",
    "data_type= {'x1':'cont','x2':'cont'} # cont:continous, ord:ordinal, oher:everything else than images\n",
    "\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "quantiles = train_df.quantile([0.05, 0.95])\n",
    "min_vals = quantiles.loc[0.05]\n",
    "max_vals = quantiles.loc[0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee587c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_write_test_dict_to_json(input:dict, test_name,file_path = 'twovars_model_loader_test_dict.json'):\n",
    "    \"\"\"\n",
    "    input has to bea dictionary with the following structure:\n",
    "                                \"input\":{\n",
    "                                                'x1': {\n",
    "                                                'data_type': 'cont',\n",
    "                                                'node_type': 'source',\n",
    "                                                'parents': [],\n",
    "                                                'parents_datatype': {},\n",
    "                                                'transformation_terms_in_h()': {},\n",
    "                                                'transformation_term_nn_models_in_h()': {}},\n",
    "                                                \n",
    "                                For n nodes\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON file: {e}\")\n",
    "    \n",
    "    data[test_name]={}\n",
    "    data[test_name].setdefault(\"input\", {})\n",
    "    data[test_name].setdefault(\"output\", {})\n",
    "\n",
    "    \n",
    "    data[test_name][\"input\"] = input\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for node_name in input:\n",
    "        model = get_fully_specified_tram_model(node_name, input, verbose=False).to(device)\n",
    "        data[test_name][\"output\"][node_name] = repr(model)\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "        print(f\"Updated outputs written to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "        \n",
    "        \n",
    "def run_model_loader_test(test_name: str, testdict_path: str, device: torch.device = None):\n",
    "    \"\"\"\n",
    "    General test for fully_specified_tram_model loader based on ground-truth JSON.\n",
    "\n",
    "    Args:\n",
    "        test_name: Key in the JSON file identifying the test case.\n",
    "        testdict_path: Path to the JSON file containing input and expected output.\n",
    "        device: Torch device to move models to; defaults to CUDA if available, else CPU.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If any model's repr() does not match the expected output.\n",
    "        ValueError: If the test_name is not found in the JSON file.\n",
    "    \"\"\"\n",
    "    # Load ground-truth data\n",
    "    with open(testdict_path, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    if test_name not in test_data:\n",
    "        raise ValueError(f\"Test name '{test_name}' not found in {testdict_path}\")\n",
    "    \n",
    "\n",
    "    inputs = test_data[test_name]['input']\n",
    "    expected_outputs = test_data[test_name].get('output', {})\n",
    "\n",
    "    # Determine device\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Iterate and compare\n",
    "    for node_name in inputs:\n",
    "        model = get_fully_specified_tram_model(node_name, inputs, verbose=False).to(device)\n",
    "        actual_repr = repr(model)\n",
    "        expected_repr = expected_outputs.get(node_name)\n",
    "\n",
    "        assert expected_repr is not None, (\n",
    "            f\"No expected output found for node '{node_name}' \"\n",
    "            f\"in '{testdict_path}'.\"\n",
    "        )\n",
    "        assert actual_repr == expected_repr, (\n",
    "            f\"Mismatch for node '{node_name}':\\n\"\n",
    "            f\"  Expected: {expected_repr}\\n\"\n",
    "            f\"  Actual:   {actual_repr}\"\n",
    "        )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2827f9",
   "metadata": {},
   "source": [
    "# Testing 2 variables\n",
    "4 tests SI LS, CS and CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ff5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS\n",
    "input={\n",
    "        'x1': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x2': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'sink',\n",
    "        'parents': ['x1'],\n",
    "        'parents_datatype': {'x1': 'cont'},\n",
    "        'transformation_terms_in_h()': {'x1': 'ls'},\n",
    "        'transformation_term_nn_models_in_h()': {'x1': 'LinearShift'}}\n",
    "        }\n",
    "\n",
    "load_and_write_test_dict_to_json(input, \"test_2\",file_path = 'twovars_model_loader_test_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5724ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS\n",
    "input={\n",
    "        'x1': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x2': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'sink',\n",
    "        'parents': ['x1'],\n",
    "        'parents_datatype': {'x1': 'cont'},\n",
    "        'transformation_terms_in_h()': {'x1': 'ls'},\n",
    "        'transformation_term_nn_models_in_h()': {'x1': 'ComplexShiftDefaultTabular'}}\n",
    "        }\n",
    "\n",
    "load_and_write_test_dict_to_json(input, \"test_2\",file_path = 'twovars_model_loader_test_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42475056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CI\n",
    "input={\n",
    "        'x1': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x2': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'sink',\n",
    "        'parents': ['x1'],\n",
    "        'parents_datatype': {'x1': 'cont'},\n",
    "        'transformation_terms_in_h()': {'x1': 'ci'},\n",
    "        'transformation_term_nn_models_in_h()': {'x1': 'ComplexInterceptDefaultTabular'}}\n",
    "        }\n",
    "\n",
    "load_and_write_test_dict_to_json(input, \"test_3\",file_path = 'twovars_model_loader_test_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00f9d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tests all the 3 cases for SI LS CS and CI\n",
    "\n",
    "for test in ['test_1', 'test_2', 'test_3']:\n",
    "    run_model_loader_test(test, 'twovars_model_loader_test_dict.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a7840",
   "metadata": {},
   "source": [
    "# 3vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model_class(class_name: str):\n",
    "    # Strip digits to get the base class name\n",
    "    for i, c in enumerate(class_name):\n",
    "        if c.isdigit():\n",
    "            return class_name[:i]\n",
    "    return class_name\n",
    "\n",
    "# --------- Group features by h_term base ---------\n",
    "def group_by_base(term_dict, prefixes):\n",
    "    if isinstance(prefixes, str):\n",
    "        prefixes = (prefixes,)\n",
    "    groups = defaultdict(list)\n",
    "    for feat, conf in term_dict.items():\n",
    "        h_term = conf['h_term']\n",
    "        for prefix in prefixes:\n",
    "            if h_term.startswith(prefix):\n",
    "                if len(h_term) > len(prefix) and h_term[len(prefix)].isdigit():\n",
    "                    key = h_term[:len(prefix)+1]\n",
    "                else:\n",
    "                    key = h_term\n",
    "                groups[key].append((feat, conf))\n",
    "                break\n",
    "    return groups\n",
    "\n",
    "\n",
    "def get_fully_specified_tram_model(node:str,target_nodes:dict,verbose=True): \n",
    "\n",
    "    ### iF node is a source -> Modeling as SimpleIntercept\n",
    "    if target_nodes[node]['node_type'] == 'source':\n",
    "        nn_int = SimpleIntercept()\n",
    "        tram_model = TramModel(nn_int, None)  \n",
    "        if verbose:\n",
    "            print('>>>>>>>>>>>>  source node --> only  modelled only  by si') if verbose else None\n",
    "            print(tram_model)\n",
    "        return tram_model\n",
    "    \n",
    "    else:\n",
    "        # read terms and model names form the config\n",
    "        _,terms_dict,model_names_dict=ordered_parents(node, target_nodes)\n",
    "        print(terms_dict)\n",
    "        print(model_names_dict)\n",
    "        \n",
    "        # Combine terms and model names and divide in intercept and shift terms\n",
    "        model_dict=merge_transformation_dicts(terms_dict, model_names_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # separate intercept and shift terms\n",
    "        intercepts_dict = {k: v for k, v in model_dict.items() if \"ci\" in v['h_term'] or 'si' in v['h_term']}        \n",
    "        shifts_dict = {k: v for k, v in model_dict.items() if \"ci\" not in v['h_term'] and  'si' not in v['h_term']}        \n",
    "        \n",
    "        \n",
    "        print(intercepts_dict)\n",
    "        print(shifts_dict)\n",
    "        \n",
    "        # make sure that nns are correctly defined afterwards\n",
    "        nn_int, nn_shifts_list = None, []\n",
    "        \n",
    "        \n",
    "        # --------- INTERCEPT TERM ---------\n",
    "        intercept_groups = group_by_base(intercepts_dict, 'ci')\n",
    "\n",
    "        if not intercept_groups:\n",
    "            print('>>>>>>>>>>>> No ci detected --> intercept defaults to si') if verbose else None\n",
    "            nn_int = SimpleIntercept()\n",
    "        else:\n",
    "            if len(intercept_groups) > 1:\n",
    "                raise ValueError(\"Multiple intercept models detected; only one is supported.\")\n",
    "\n",
    "\n",
    "            group = list(intercept_groups.values())[0]\n",
    "            \n",
    "            print(f'group; {group}')\n",
    "            \n",
    "            any_class_name = group[0][1]['class_name']\n",
    "            base_class_name = get_base_model_class(any_class_name)\n",
    "\n",
    "            model_cls = globals()[base_class_name]\n",
    "            n_features = len(group)\n",
    "            nn_int = model_cls(n_features=n_features)\n",
    "\n",
    "        # --------- SHIFT TERMS ---------\n",
    "        shift_groups = group_by_base(shifts_dict, prefixes=('cs', 'ls'))\n",
    "        \n",
    "        print(shift_groups)\n",
    "\n",
    "        for group in shift_groups.values():\n",
    "            any_class_name = group[0][1]['class_name']\n",
    "            base_class_name = get_base_model_class(any_class_name)\n",
    "\n",
    "            model_cls = globals()[base_class_name]\n",
    "            n_features = len(group)\n",
    "            model = model_cls(n_features=n_features)\n",
    "            nn_shifts_list.append(model)\n",
    "\n",
    "        # --------- COMBINE TO NN CLASS ---------\n",
    "        tram_model = TramModel(nn_int, nn_shifts_list)\n",
    "        print('>>> TRAM MODEL:\\n',tram_model) if verbose else None\n",
    "        return tram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f10aee70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('x1', 'ls'), ('x2', 'ls')])\n",
      "OrderedDict([('x1', 'LinearShift'), ('x2', 'LinearShift')])\n",
      "{}\n",
      "{'x1': {'h_term': 'ls', 'class_name': 'LinearShift'}, 'x2': {'h_term': 'ls', 'class_name': 'LinearShift'}}\n",
      "defaultdict(<class 'list'>, {'ls': [('x1', {'h_term': 'ls', 'class_name': 'LinearShift'}), ('x2', {'h_term': 'ls', 'class_name': 'LinearShift'})]})\n",
      "Updated outputs written to threevars_model_loader_test_dict.json\n"
     ]
    }
   ],
   "source": [
    "# TODO here is a bug when 2 linera shifts are used in the same model, e.g. when we have a model like this:\n",
    "\n",
    "#  X3 ~ LS(X1) + LS(X2)\n",
    "input={\n",
    "        'x1': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x2': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'source',\n",
    "        'parents': [],\n",
    "        'parents_datatype': {},\n",
    "        'transformation_terms_in_h()': {},\n",
    "        'transformation_term_nn_models_in_h()': {}},\n",
    "        'x3': {\n",
    "        'data_type': 'cont',\n",
    "        'node_type': 'sink',\n",
    "        'parents': ['x1','x2'],\n",
    "        'parents_datatype': {'x1': 'cont','x2': 'cont'},\n",
    "        'transformation_terms_in_h()': {'x1': 'ls','x2': 'ls'},\n",
    "        'transformation_term_nn_models_in_h()': {'x1': 'LinearShift','x2': 'LinearShift'}}\n",
    "        }\n",
    "\n",
    "load_and_write_test_dict_to_json(input, \"test_1\", file_path = 'threevars_model_loader_test_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ce304",
   "metadata": {},
   "outputs": [],
   "source": [
    "input=\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad8623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93848a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = {'x2': 'cont','x3': 'cont','x4': 'cont','x5': 'cont','x6': 'cont','x7': 'cont','x8': 'cont','x9': 'cont'}\n",
    "adj_matrix = np.array([\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"ls\"],   # x1 → x2 (ci), x1 → x3 (ls)\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"cs12\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"cs21\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"cs22\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"ci12\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"ci11\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"cs11\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"0\"]\n",
    "], dtype=object)\n",
    "\n",
    "plot_seed = 42\n",
    "plot_dag(adj_matrix, data_type, seed=plot_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are Ci or Compelx shifts in the models. If yes define the modelnames\n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "plot_nn_names_matrix(nn_names_matrix,data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in conf_dict:\n",
    "\n",
    "tram_model = get_fully_specified_tram_model(node, conf_dict, verbose=True).to(device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
