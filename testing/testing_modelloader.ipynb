{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with GPU support.\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "proj_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if proj_root not in sys.path:\n",
    "    sys.path.insert(0, proj_root)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import logistic\n",
    "from scipy.special import logit\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Train with GPU support.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU found, train with CPU support.\")\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# own utils\n",
    "from utils.graph import *\n",
    "from utils.tram_models import *\n",
    "from utils.tram_model_helpers import *\n",
    "from utils.tram_data import *\n",
    "from utils.continous import *\n",
    "from utils.sampling_tram_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgp_sklearn(nobs=1000, nvars=7, seed=42):\n",
    "    X, _ = make_blobs(n_samples=nobs, n_features=nvars, centers=1,cluster_std=1.0, random_state=seed)\n",
    "    cols = [f'x{i+1}' for i in range(nvars)]\n",
    "    return pd.DataFrame(X, columns=cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2827f9",
   "metadata": {},
   "source": [
    "# Testing 2 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77414fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dgp_sklearn(nobs=1000, nvars=2, seed=42)\n",
    "print(df.info())\n",
    "\n",
    "\n",
    "data_type= {'x1':'cont','x2':'cont'} # cont:continous, ord:ordinal, oher:everything else than images\n",
    "\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "quantiles = train_df.quantile([0.05, 0.95])\n",
    "min_vals = quantiles.loc[0.05]\n",
    "max_vals = quantiles.loc[0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc3875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_nodes={\n",
    "            'x1': {'Modelnr': 0,\n",
    "            'data_type': 'cont',\n",
    "            'node_type': 'source',\n",
    "            'parents': [],\n",
    "            'parents_datatype': {},\n",
    "            'transformation_terms_in_h()': {},\n",
    "            'min': None,\n",
    "            'max': None,\n",
    "            'transformation_term_nn_models_in_h()': {}},\n",
    "            'x2': {'Modelnr': 1,\n",
    "            'data_type': 'cont',\n",
    "            'node_type': 'sink',\n",
    "            'parents': ['x1'],\n",
    "            'parents_datatype': {'x1': 'cont'},\n",
    "            'transformation_terms_in_h()': {'x1': 'ls'},\n",
    "            'min': None,\n",
    "            'max': None,\n",
    "            'transformation_term_nn_models_in_h()': {'x1': 'LinearShift'}}\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e04c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in inputs:\n",
    "    model = get_fully_specified_tram_model(node, inputs, verbose=False).to(device)\n",
    "    # capture the representation directly\n",
    "    all_outputs[test_key][node] = repr(model)\n",
    "    \n",
    "with open(\"tram_model_repr_outputs.json\", \"w\") as f:\n",
    "        json.dump(all_outputs, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e99131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary has been saved to twovars_model_loader_test_dict.json\n"
     ]
    }
   ],
   "source": [
    "twovars_model_loader_test_dict={\"test_1\":{            \n",
    "                                \"input\":{\n",
    "                                                'x1': {\n",
    "                                                'data_type': 'cont',\n",
    "                                                'node_type': 'source',\n",
    "                                                'parents': [],\n",
    "                                                'parents_datatype': {},\n",
    "                                                'transformation_terms_in_h()': {},\n",
    "                                                'transformation_term_nn_models_in_h()': {}},\n",
    "                                                'x2': {\n",
    "                                                'data_type': 'cont',\n",
    "                                                'node_type': 'sink',\n",
    "                                                'parents': ['x1'],\n",
    "                                                'parents_datatype': {'x1': 'cont'},\n",
    "                                                'transformation_terms_in_h()': {'x1': 'ls'},\n",
    "                                                'transformation_term_nn_models_in_h()': {'x1': 'LinearShift'}}\n",
    "                                                },\n",
    "                                \"output\":{}    \n",
    "                                    }\n",
    "                        }\n",
    "\n",
    "\n",
    "# Save to JSON file\n",
    "file_path = 'twovars_model_loader_test_dict.json'\n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(twovars_model_loader_test_dict, f, indent=4)\n",
    "\n",
    "print(f\"Dictionary has been saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54d6960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated outputs written to twovars_model_loader_test_dict.json\n"
     ]
    }
   ],
   "source": [
    "file_path = 'twovars_model_loader_test_dict.json'\n",
    "\n",
    "# Load the existing JSON data\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Grab the inputs dict\n",
    "inputs = data[\"test_1\"][\"input\"]\n",
    "\n",
    "# Determine device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Ensure the output dict exists\n",
    "data[\"test_1\"].setdefault(\"output\", {})\n",
    "\n",
    "# Iterate over each node, build the model, and store its repr\n",
    "for node_name in inputs:\n",
    "    model = get_fully_specified_tram_model(node_name, inputs, verbose=False).to(device)\n",
    "    data[\"test_1\"][\"output\"][node_name] = repr(model)\n",
    "\n",
    "# Save the updated JSON back to file\n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "print(f\"Updated outputs written to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c72377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing JSON data\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "305f6c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_1': {'input': {'x1': {'data_type': 'cont',\n",
       "    'node_type': 'source',\n",
       "    'parents': [],\n",
       "    'parents_datatype': {},\n",
       "    'transformation_terms_in_h()': {},\n",
       "    'transformation_term_nn_models_in_h()': {}},\n",
       "   'x2': {'data_type': 'cont',\n",
       "    'node_type': 'sink',\n",
       "    'parents': ['x1'],\n",
       "    'parents_datatype': {'x1': 'cont'},\n",
       "    'transformation_terms_in_h()': {'x1': 'ls'},\n",
       "    'transformation_term_nn_models_in_h()': {'x1': 'LinearShift'}}},\n",
       "  'output': {'x1': 'TramModel(\\n  (nn_int): SimpleIntercept(\\n    (fc): Linear(in_features=1, out_features=20, bias=False)\\n  )\\n)',\n",
       "   'x2': 'TramModel(\\n  (nn_int): SimpleIntercept(\\n    (fc): Linear(in_features=1, out_features=20, bias=False)\\n  )\\n  (nn_shift): ModuleList(\\n    (0): LinearShift(\\n      (fc): Linear(in_features=1, out_features=1, bias=False)\\n    )\\n  )\\n)'}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02abd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ground-truth data\n",
    "with open('twovars_model_loader_test_dict.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "inputs = test_data[\"test_1\"][\"input\"]\n",
    "expected_outputs = test_data[\"test_1\"][\"output\"]\n",
    "\n",
    "# Determine device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def test_twovars_model_loader_ground_truth():\n",
    "    \"\"\"\n",
    "    For each node in the test inputs, build the fully specified TRAM model,\n",
    "    compare its repr() against the expected ground-truth, and fail if they differ.\n",
    "    \"\"\"\n",
    "    for node_name in inputs:\n",
    "        model = get_fully_specified_tram_model(node_name, inputs, verbose=False).to(device)\n",
    "        actual_repr = repr(model)\n",
    "        expected_repr = expected_outputs.get(node_name)\n",
    "        assert expected_repr is not None, (\n",
    "            f\"No expected output found for node '{node_name}' in ground-truth JSON.\"\n",
    "        )\n",
    "        assert actual_repr == expected_repr, (\n",
    "            f\"Mismatch for node '{node_name}':\\n\"\n",
    "            f\"Expected: {expected_repr}\\n\"\n",
    "            f\"Actual:   {actual_repr}\"\n",
    "        )\n",
    "        \n",
    "test_twovars_model_loader_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b3c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for node in model_loader_test_dict[\"test_1\"]['input']:\n",
    "    print( get_fully_specified_tram_model(node, model_loader_test_dict[\"test_1\"]['input'], verbose=False).to(device))\n",
    "    print(model_loader_test_dict[\"test_1\"]['output'][node])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4094bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model_class(class_name: str):\n",
    "    # Strip digits to get the base class name\n",
    "    for i, c in enumerate(class_name):\n",
    "        if c.isdigit():\n",
    "            return class_name[:i]\n",
    "    return class_name\n",
    "\n",
    "# --------- Group features by h_term base ---------\n",
    "def group_by_base(term_dict, prefixes):\n",
    "    if isinstance(prefixes, str):\n",
    "        prefixes = (prefixes,)\n",
    "    groups = defaultdict(list)\n",
    "    for feat, conf in term_dict.items():\n",
    "        h_term = conf['h_term']\n",
    "        for prefix in prefixes:\n",
    "            if h_term.startswith(prefix):\n",
    "                if len(h_term) > len(prefix) and h_term[len(prefix)].isdigit():\n",
    "                    key = h_term[:len(prefix)+1]\n",
    "                else:\n",
    "                    key = h_term\n",
    "                groups[key].append((feat, conf))\n",
    "                break\n",
    "    return groups\n",
    "\n",
    "\n",
    "def get_fully_specified_tram_model(node:str,target_nodes:dict,verbose=True): \n",
    "\n",
    "    ### iF node is a source -> Modeling as SimpleIntercept\n",
    "    if target_nodes[node]['node_type'] == 'source':\n",
    "        nn_int = SimpleIntercept()\n",
    "        tram_model = TramModel(nn_int, None)  \n",
    "        if verbose:\n",
    "            print('>>>>>>>>>>>>  source node --> only  modelled only  by si') if verbose else None\n",
    "            print(tram_model)\n",
    "        return tram_model\n",
    "    \n",
    "    else:\n",
    "        # read terms and model names form the config\n",
    "        _,terms_dict,model_names_dict=ordered_parents(node, target_nodes)\n",
    "        \n",
    "        # Combine terms and model names and divide in intercept and shift terms\n",
    "        model_dict=merge_transformation_dicts(terms_dict, model_names_dict)\n",
    "        \n",
    "        # separate intercept and shift terms\n",
    "        intercepts_dict = {k: v for k, v in model_dict.items() if \"ci\" in v['h_term'] or 'si' in v['h_term']}        \n",
    "        shifts_dict = {k: v for k, v in model_dict.items() if \"ci\" not in v['h_term'] and  'si' not in v['h_term']}        \n",
    "        \n",
    "        # make sure that nns are correctly defined afterwards\n",
    "        nn_int, nn_shifts_list = None, []\n",
    "        \n",
    "        \n",
    "        # --------- INTERCEPT TERM ---------\n",
    "        intercept_groups = group_by_base(intercepts_dict, 'ci')\n",
    "\n",
    "        if not intercept_groups:\n",
    "            print('>>>>>>>>>>>> No ci detected --> intercept defaults to si') if verbose else None\n",
    "            nn_int = SimpleIntercept()\n",
    "        else:\n",
    "            if len(intercept_groups) > 1:\n",
    "                raise ValueError(\"Multiple intercept models detected; only one is currently supported.\")\n",
    "\n",
    "            group = list(intercept_groups.values())[0]\n",
    "            any_class_name = group[0][1]['class_name']\n",
    "            base_class_name = get_base_model_class(any_class_name)\n",
    "\n",
    "            model_cls = globals()[base_class_name]\n",
    "            n_features = len(group)\n",
    "            nn_int = model_cls(n_features=n_features)\n",
    "\n",
    "        # --------- SHIFT TERMS ---------\n",
    "        shift_groups = group_by_base(shifts_dict, prefixes=('cs', 'ls'))\n",
    "\n",
    "        for group in shift_groups.values():\n",
    "            any_class_name = group[0][1]['class_name']\n",
    "            base_class_name = get_base_model_class(any_class_name)\n",
    "\n",
    "            model_cls = globals()[base_class_name]\n",
    "            n_features = len(group)\n",
    "            model = model_cls(n_features=n_features)\n",
    "            nn_shifts_list.append(model)\n",
    "\n",
    "        # --------- COMBINE TO NN CLASS ---------\n",
    "        tram_model = TramModel(nn_int, nn_shifts_list)\n",
    "        print('>>> TRAM MODEL:\\n',tram_model) if verbose else None\n",
    "        return tram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93848a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = {'x2': 'cont','x3': 'cont','x4': 'cont','x5': 'cont','x6': 'cont','x7': 'cont','x8': 'cont','x9': 'cont'}\n",
    "adj_matrix = np.array([\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"ls\"],   # x1 → x2 (ci), x1 → x3 (ls)\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"cs12\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"cs21\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"cs22\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"ci12\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"ci11\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"cs11\"],\n",
    "    [\"0\",  \"0\",  \"0\",  \"0\", \"0\",  \"0\",  \"0\",  \"0\"]\n",
    "], dtype=object)\n",
    "\n",
    "plot_seed = 42\n",
    "plot_dag(adj_matrix, data_type, seed=plot_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are Ci or Compelx shifts in the models. If yes define the modelnames\n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "plot_nn_names_matrix(nn_names_matrix,data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in conf_dict:\n",
    "\n",
    "tram_model = get_fully_specified_tram_model(node, conf_dict, verbose=True).to(device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
