{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e076b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with GPU support.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from scipy.stats import logistic\n",
    "# from scipy.special import logit\n",
    "\n",
    "import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import Dataset\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Train with GPU support.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU found, train with CPU support.\")\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# own utils\n",
    "from utils.graph import *\n",
    "from utils.tram_models import *\n",
    "from utils.tram_model_helpers import *\n",
    "from utils.tram_data import *\n",
    "from utils.loss_continous import *\n",
    "from utils.sampling_tram_data import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923b899",
   "metadata": {},
   "source": [
    "# 1. Experiments and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b984f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new configuration file at /home/bule/TramDag/dev_experiment_logs/ordinal_dev/configuration.json\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"ordinal_dev\"   ## <--- set experiment name\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "\n",
    "LOG_DIR=\"/home/bule/TramDag/dev_experiment_logs\"\n",
    "EXPERIMENT_DIR = os.path.join(LOG_DIR, experiment_name)\n",
    "DATA_PATH = EXPERIMENT_DIR # <----------- change to different source if needed\n",
    "CONF_DICT_PATH = os.path.join(EXPERIMENT_DIR, f\"configuration.json\")\n",
    "\n",
    "os.makedirs(EXPERIMENT_DIR,exist_ok=True)\n",
    "# check if configration dict already exists if not create:\n",
    "\n",
    "if os.path.exists(CONF_DICT_PATH):\n",
    "    configuration_dict=load_configuration_dict(CONF_DICT_PATH)\n",
    "    print(f\"Loaded existing configuration from {CONF_DICT_PATH}\")\n",
    "else:\n",
    "    configuration_dict=create_and_write_new_configuration_dict(experiment_name,CONF_DICT_PATH,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)\n",
    "    print(f\"Created new configuration file at {CONF_DICT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76549b97",
   "metadata": {},
   "source": [
    "# 2.  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f916214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed: 42\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed: 42\n",
      "   x1  x2        x3\n",
      "0   1   0 -0.377983\n",
      "1   0   0 -0.826880\n",
      "2   1   1  1.273521\n",
      "3   1   1  1.867366\n",
      "4   1   1  2.413585\n",
      "Configuration updated successfully.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def dgp(n_obs, doX=[None, None, None], f=lambda x: x, seed=None):\n",
    "    \"\"\"\n",
    "    Modified data-generating process.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_obs : int\n",
    "        Number of observations to generate.\n",
    "    doX : list of length 3\n",
    "        If doX[i] is not None, fixes X_{i+1} to that constant for all draws.\n",
    "    f : callable\n",
    "        A function of X2 used in generating X3 (defaults to identity).\n",
    "    seed : int or None\n",
    "        If provided, sets the NumPy random seed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns ['x1', 'x2', 'x3'], where x1 and x2 are\n",
    "        binary ordered categoricals, and x3 is continuous.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        print(f\"Setting seed: {seed}\")\n",
    "\n",
    "    # --- 1. Sample binary X1 ---\n",
    "    if doX[0] is None:\n",
    "        x1_num = np.random.binomial(n=1, p=0.9, size=n_obs)\n",
    "    else:\n",
    "        x1_num = np.full(n_obs, doX[0], dtype=int)\n",
    "    x1 = pd.Categorical(x1_num, categories=[0, 1], ordered=True)\n",
    "\n",
    "    # --- 2. Sample binary X2 ---\n",
    "    if doX[1] is None:\n",
    "        x2_num = np.random.binomial(n=1, p=0.45, size=n_obs)\n",
    "    else:\n",
    "        x2_num = np.full(n_obs, doX[1], dtype=int)\n",
    "    x2 = pd.Categorical(x2_num, categories=[0, 1], ordered=True)\n",
    "\n",
    "    # --- 3. Sample continuous X3 ---\n",
    "    if doX[2] is None:\n",
    "        noise = np.random.normal(loc=0.0, scale=1.0, size=n_obs)\n",
    "        x3 = 0 + 0.5 * x1_num + f(x2_num) + noise\n",
    "    else:\n",
    "        x3 = np.full(n_obs, doX[2], dtype=float)\n",
    "\n",
    "    return pd.DataFrame({'x1': x1, 'x2': x2, 'x3': x3})\n",
    "\n",
    "\n",
    "\n",
    "df = dgp(n_obs=1_000, seed=42)\n",
    "\n",
    "\n",
    "plt.hist(df['x1'], density=True)\n",
    "plt.xticks([0, 1])\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.title('Relative Frequency Histogram of x3')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.hist(df['x2'], density=True)\n",
    "plt.xticks([0, 1,])\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.title('Relative Frequency Histogram of x3')\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "sns.pairplot(df)\n",
    "\n",
    "EXP_DATA_PATH=os.path.join(DATA_PATH, f\"{experiment_name}.csv\")\n",
    "if not os.path.exists(EXP_DATA_PATH):\n",
    "    df = dgp(n_obs=1_000, seed=42)\n",
    "    df['x1']=df['x1'].astype(int)\n",
    "    df['x2']=df['x2'].astype(int)\n",
    "\n",
    "    print(df.head())\n",
    "    df.to_csv(EXP_DATA_PATH, index=False)\n",
    "else:\n",
    "    df = pd.read_csv(EXP_DATA_PATH)\n",
    "    df['x2']=df['x2'].astype(int)\n",
    "    df['x1']=df['x1'].astype(int)\n",
    "\n",
    "    print(f\"Loaded data from {EXP_DATA_PATH}\")\n",
    "\n",
    "\n",
    "data_type= {'x1':'ord','x2':'ord','x3':'cont'} # cont:continous, ord:ordinal, oher:everything else than images\n",
    "\n",
    "write_data_type_to_configuration(data_type, CONF_DICT_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d527e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   x1      1000 non-null   int64  \n",
      " 1   x2      1000 non-null   int64  \n",
      " 2   x3      1000 non-null   float64\n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 23.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e5a0fb",
   "metadata": {},
   "source": [
    "## 2.1 train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f2733e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 800, Validation size: 100, Test size: 100\n"
     ]
    }
   ],
   "source": [
    "# 1. Split the data\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# 2. Compute quantiles from training data\n",
    "quantiles = train_df.quantile([0.05, 0.95])\n",
    "min_vals = quantiles.loc[0.05]\n",
    "max_vals = quantiles.loc[0.95]\n",
    "\n",
    "# 3. Normalize all sets using training quantiles\n",
    "def normalize_with_quantiles(df, min_vals, max_vals):\n",
    "    return (df - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "# train_df = normalize_with_quantiles(train_df, min_vals, max_vals)\n",
    "# val_df = normalize_with_quantiles(val_df, min_vals, max_vals)\n",
    "# test_df = normalize_with_quantiles(test_df, min_vals, max_vals)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b97f69",
   "metadata": {},
   "source": [
    "# 3. Define graph Structure\n",
    "\n",
    "- define graph and which shift and intercept terms to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fedcd902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matrix found. Please fill out the DAG and click 'Generate'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eaea64e19d9437a88c15a072252d9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=\"Fill in the adjacency matrix (upper triangle only). Use 'ls', 'cs', etc.\"), GridBo"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_adj_matrix(CONF_DICT_PATH,seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7328e1",
   "metadata": {},
   "source": [
    "# 4. Configuration for the Models\n",
    "\n",
    "- all SI and LS model are generated outmatically since these are shallow NN's\n",
    "- CI and CS have to be defined by the User and can be Passed for each model, -> generate default networks which are generated automaitcally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "314149ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd9d6d8cef14620a751138a178839d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Edit only the existing model names (non-zero entries).'), GridBox(children=(Label("
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_nn_names_matrix(CONF_DICT_PATH, seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a19d569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': {'Modelnr': 0, 'data_type': 'ord', 'levels': 2, 'node_type': 'source', 'parents': [], 'parents_datatype': {}, 'transformation_terms_in_h()': {}, 'min': 0.0, 'max': 1.0, 'transformation_term_nn_models_in_h()': {}}, 'x2': {'Modelnr': 1, 'data_type': 'ord', 'levels': 2, 'node_type': 'internal', 'parents': ['x1'], 'parents_datatype': {'x1': 'ord'}, 'transformation_terms_in_h()': {'x1': np.str_('ls')}, 'min': 0.0, 'max': 1.0, 'transformation_term_nn_models_in_h()': {'x1': np.str_('LinearShift')}}, 'x3': {'Modelnr': 2, 'data_type': 'cont', 'node_type': 'sink', 'parents': ['x1', 'x2'], 'parents_datatype': {'x1': 'ord', 'x2': 'ord'}, 'transformation_terms_in_h()': {'x1': np.str_('ls'), 'x2': np.str_('ls')}, 'min': -0.8829760308059689, 'max': 2.7365581647527457, 'transformation_term_nn_models_in_h()': {'x1': np.str_('LinearShift'), 'x2': np.str_('LinearShift')}}}\n",
      "Configuration updated successfully.\n"
     ]
    }
   ],
   "source": [
    "write_nodes_information_to_configuration_v2(CONF_DICT_PATH, min_vals, max_vals,levels_dict={'x1': 2,'x2': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eed791d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x3",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "feeae76e-6e9b-4456-964e-fbbfb1ed4ecd",
       "rows": [
        [
         "0",
         "1",
         "0",
         "-0.37798258675656105"
        ],
        [
         "1",
         "0",
         "0",
         "-0.82688035403986"
        ],
        [
         "2",
         "1",
         "1",
         "1.2735211080984732"
        ],
        [
         "3",
         "1",
         "1",
         "1.867365506816178"
        ],
        [
         "4",
         "1",
         "1",
         "2.4135846262332494"
        ],
        [
         "5",
         "1",
         "1",
         "0.6968210493196552"
        ],
        [
         "6",
         "1",
         "1",
         "2.9926885689595215"
        ],
        [
         "7",
         "1",
         "1",
         "1.2288763989574312"
        ],
        [
         "8",
         "1",
         "0",
         "0.47863270565156385"
        ],
        [
         "9",
         "1",
         "0",
         "-0.24721167914782338"
        ],
        [
         "10",
         "1",
         "0",
         "-1.9242402602729416"
        ],
        [
         "11",
         "0",
         "1",
         "1.8840453963610497"
        ],
        [
         "12",
         "1",
         "1",
         "2.2368438970924336"
        ],
        [
         "13",
         "1",
         "0",
         "0.2186724428817866"
        ],
        [
         "14",
         "1",
         "1",
         "1.5669907172248718"
        ],
        [
         "15",
         "1",
         "1",
         "2.0159392177040996"
        ],
        [
         "16",
         "1",
         "0",
         "-1.0625458568865427"
        ],
        [
         "17",
         "1",
         "1",
         "0.9709473226537052"
        ],
        [
         "18",
         "1",
         "1",
         "2.2942646796218273"
        ],
        [
         "19",
         "1",
         "0",
         "-0.7542894231692494"
        ],
        [
         "20",
         "1",
         "1",
         "1.7935579321666293"
        ],
        [
         "21",
         "1",
         "0",
         "-0.856581804623405"
        ],
        [
         "22",
         "1",
         "1",
         "1.9664299824335263"
        ],
        [
         "23",
         "1",
         "1",
         "1.4643585177025165"
        ],
        [
         "24",
         "1",
         "1",
         "-0.11513181590550947"
        ],
        [
         "25",
         "1",
         "0",
         "1.6647393544571714"
        ],
        [
         "26",
         "1",
         "1",
         "0.7654084232019508"
        ],
        [
         "27",
         "1",
         "1",
         "0.689747562985619"
        ],
        [
         "28",
         "1",
         "0",
         "0.7005691972309429"
        ],
        [
         "29",
         "1",
         "1",
         "2.648637349428715"
        ],
        [
         "30",
         "1",
         "0",
         "-0.515821818978397"
        ],
        [
         "31",
         "1",
         "1",
         "1.5616798498479716"
        ],
        [
         "32",
         "1",
         "0",
         "0.928816500330573"
        ],
        [
         "33",
         "0",
         "0",
         "0.6931056072631441"
        ],
        [
         "34",
         "0",
         "0",
         "0.1764415552772252"
        ],
        [
         "35",
         "1",
         "1",
         "1.1329721609166776"
        ],
        [
         "36",
         "1",
         "0",
         "-0.3275902206487197"
        ],
        [
         "37",
         "1",
         "1",
         "1.5861438830463748"
        ],
        [
         "38",
         "1",
         "0",
         "-0.5721390119856069"
        ],
        [
         "39",
         "1",
         "0",
         "-2.4213504834946074"
        ],
        [
         "40",
         "1",
         "0",
         "0.9365598041576678"
        ],
        [
         "41",
         "1",
         "1",
         "2.4039348410861954"
        ],
        [
         "42",
         "1",
         "0",
         "-1.8629324355418384"
        ],
        [
         "43",
         "0",
         "1",
         "-0.009730704709937532"
        ],
        [
         "44",
         "1",
         "1",
         "2.119154261005198"
        ],
        [
         "45",
         "1",
         "1",
         "3.5574954697116987"
        ],
        [
         "46",
         "1",
         "0",
         "0.5207937076754079"
        ],
        [
         "47",
         "1",
         "0",
         "-0.22800294122913445"
        ],
        [
         "48",
         "1",
         "1",
         "1.3171035588951598"
        ],
        [
         "49",
         "1",
         "0",
         "1.8748764191481848"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 1000
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.377983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.826880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.273521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.867366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.413585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.935226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.021383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.588443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.585983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.379924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     x1  x2        x3\n",
       "0     1   0 -0.377983\n",
       "1     0   0 -0.826880\n",
       "2     1   1  1.273521\n",
       "3     1   1  1.867366\n",
       "4     1   1  2.413585\n",
       "..   ..  ..       ...\n",
       "995   1   1  0.935226\n",
       "996   0   1  2.021383\n",
       "997   1   0  2.588443\n",
       "998   0   0 -1.585983\n",
       "999   1   0 -1.379924\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea84f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericDataset_v3(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        target_col,\n",
    "        target_nodes=None,\n",
    "        parents_dataype_dict=None,\n",
    "        transform=None,\n",
    "        transformation_terms_in_h=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        df: pd.DataFrame\n",
    "        target_col: str\n",
    "        target_nodes: dict mapping each node  metadata (including 'data_type')\n",
    "        parents_dataype_dict: dict var_name  \"cont\"|\"ord\"|\"other\"\n",
    "        transform: torchvision transform for images\n",
    "        transformation_terms_in_h: dict for intercept logic\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.target_col = target_col\n",
    "        self.target_nodes = target_nodes or {}\n",
    "        self.parents_dataype_dict = parents_dataype_dict or {}\n",
    "        self.variables = list(self.parents_dataype_dict.keys())\n",
    "        self.transform = transform\n",
    "        self.transformation_terms_in_h = transformation_terms_in_h or {}\n",
    "                # If we know this target is ordinal, record #classes\n",
    "        if (\n",
    "            self.target_nodes\n",
    "            and self.target_col in self.target_nodes\n",
    "            and self.target_nodes[self.target_col].get('data_type') == \"ord\"\n",
    "        ):\n",
    "            self.target_num_classes = int(self.df[self.target_col].nunique())\n",
    "        else:\n",
    "            self.target_num_classes = None\n",
    "\n",
    "        self._check_binary_values_of_df()\n",
    "        \n",
    "    def _transform_y(self,row):\n",
    "        # handle y\n",
    "        if self.target_num_classes is not None:\n",
    "            # ordinal source  one-hot\n",
    "            raw = row[self.target_col]\n",
    "            y_int = int(raw)\n",
    "            y = F.one_hot(torch.tensor(y_int, dtype=torch.long), num_classes=self.target_num_classes).float()\n",
    "            \n",
    "        else:\n",
    "            # continuous or other\n",
    "            y = torch.tensor(row[self.target_col], dtype=torch.float32)\n",
    "        return y\n",
    "\n",
    "    def _check_binary_values_of_df(self):\n",
    "        for var in self.variables:\n",
    "            dtype = self.parents_dataype_dict[var]\n",
    "            if dtype != \"ord\":\n",
    "                continue\n",
    "\n",
    "            unique_vals = set(self.df[var].dropna().unique())\n",
    "            if len(unique_vals) == 2:\n",
    "                if unique_vals != {0, 1}:\n",
    "                    raise ValueError(\n",
    "                        f\"Variable '{var}' is marked as ordinal with 2 classes, \"\n",
    "                        f\"but values are {unique_vals}. Please provide binary ordinal variables as 0 and 1.\"\n",
    "                    )\n",
    "            elif len(unique_vals) < 2:\n",
    "                raise ValueError(\n",
    "                    f\"Variable '{var}' has fewer than 2 unique values: {unique_vals}. \"\n",
    "                    \"Binary ordinal variables must have exactly two distinct values: 0 and 1.\"\n",
    "                )\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        x_data = []\n",
    "\n",
    "        # --- SOURCE NODE: no parents  x = [1.0] ---\n",
    "        if not self.parents_dataype_dict:\n",
    "            x_data = [torch.tensor(1.0)]\n",
    "            y=self._transform_y(row)\n",
    "            return tuple(x_data), y\n",
    "\n",
    "        # --- INTERCEPT if needed ---\n",
    "        if all('i' not in str(v) for v in self.transformation_terms_in_h.values()):\n",
    "            x_data.append(torch.tensor(1.0))\n",
    "\n",
    "        # --- BUILD FEATURES ---\n",
    "        for var in self.variables:\n",
    "            dtype = self.parents_dataype_dict[var]\n",
    "            ## Continous  feature\n",
    "            if dtype == \"cont\":\n",
    "                x_data.append(torch.tensor(row[var], dtype=torch.float32))\n",
    "                \n",
    "            ## Ordinal feature , if it has more thatn 2 classes it uses onehotencodig, if binary use just 0 and 1\n",
    "            elif dtype == \"ord\":\n",
    "                x_ord = int(row[var])\n",
    "                var_num_classes=int(self.df[var].nunique())\n",
    "                if var_num_classes>2:\n",
    "                    x_ord_onehot = F.one_hot(torch.tensor(x_ord, dtype=torch.long),num_classes=var_num_classes).float()\n",
    "                    x_data.append(x_ord_onehot)\n",
    "                \n",
    "                else:\n",
    "                    x_data.append(torch.tensor(x_ord, dtype=torch.long))\n",
    "                \n",
    "            else:  # \"other\"\n",
    "                img = Image.open(row[var]).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                x_data.append(img)\n",
    "\n",
    "        # --- BUILD TARGET ---\n",
    "        y=self._transform_y(row)\n",
    "            \n",
    "            \n",
    "        return tuple(x_data), y\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloader_v3(node, target_nodes, train_df, val_df, batch_size=32, verbose=False):\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    if target_nodes[node]['node_type'] == 'source':\n",
    "        if verbose:\n",
    "            print('Source node  features are just a constant 1.')\n",
    "        train_ds = GenericDataset_v3(train_df,target_col=node,target_nodes=target_nodes,parents_dataype_dict=None,transform=transform)\n",
    "        val_ds = GenericDataset_v3(val_df,target_col=node,target_nodes=target_nodes,parents_dataype_dict=None,transform=transform)\n",
    "        \n",
    "    else:\n",
    "        parents_dataype_dict, transformation_terms_in_h, _ = ordered_parents(node, target_nodes)\n",
    "        if verbose:\n",
    "            print(f\"Parents dtype: {parents_dataype_dict}\")\n",
    "        train_ds = GenericDataset_v3(train_df,target_col=node,target_nodes=target_nodes,parents_dataype_dict=parents_dataype_dict,transform=transform,transformation_terms_in_h=transformation_terms_in_h)\n",
    "        val_ds = GenericDataset_v3(val_df,target_col=node,target_nodes=target_nodes,parents_dataype_dict=parents_dataype_dict,transform=transform,transformation_terms_in_h=transformation_terms_in_h)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f049b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_TRAINING=True\n",
    "train_list=['x1','x2','x3']#['x2']#'x1','x2']#,'x3']#['x1']#['x1','x2','x3']#,#,['x1','x2','x3'] # <-  set the nodes which have to be trained , useful if further training is required else lsit all vars\n",
    "\n",
    "batch_size = 1#4112\n",
    "epochs = 10# <- if you want a higher numbe rof epochs, set the number higher and it loads the old model and starts from there\n",
    "learning_rate=0.01\n",
    "use_scheduler =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54370430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1\n",
      "Source node  features are just a constant 1.\n",
      "[[tensor([1.])], tensor([[0., 1.]])]\n",
      "x2\n",
      "Parents dtype: OrderedDict([('x1', 'ord')])\n",
      "[[tensor([1.]), tensor([1])], tensor([[0., 1.]])]\n",
      "x3\n",
      "Parents dtype: OrderedDict([('x1', 'ord'), ('x2', 'ord')])\n",
      "[[tensor([1.]), tensor([1]), tensor([1])], tensor([2.2308])]\n"
     ]
    }
   ],
   "source": [
    "# For each NODE \n",
    "configuration_dict = load_configuration_dict(CONF_DICT_PATH)\n",
    "target_nodes = configuration_dict['nodes']\n",
    "\n",
    "\n",
    "for node in target_nodes:\n",
    "    \n",
    "    print(node)\n",
    "    \n",
    "\n",
    "    ########################## 3. Create Dataloaders ########################\n",
    "    train_loader, val_loader = get_dataloader_v3(node, target_nodes, train_df, val_df, batch_size=batch_size, verbose=True)\n",
    "    print(next(iter(train_loader)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c12bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_TRAINING=True\n",
    "train_list=['x1','x2','x3']#['x2']#'x1','x2']#,'x3']#['x1']#['x1','x2','x3']#,#,['x1','x2','x3'] # <-  set the nodes which have to be trained , useful if further training is required else lsit all vars\n",
    "\n",
    "batch_size = 512#4112\n",
    "epochs = 10# <- if you want a higher numbe rof epochs, set the number higher and it loads the old model and starts from there\n",
    "learning_rate=0.01\n",
    "use_scheduler =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95df0da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----*----------*-------------*--------------- Node: x1 ------------*-----------------*-------------------*--\n",
      "Source  SimpleIntercept only\n",
      "Source node  features are just a constant 1.\n",
      "No existing model found. Starting fresh...\n",
      "Saved new best model.\n",
      "Epoch 1/10 | Train Loss: 0.9854 | Val Loss: 1.0091\n",
      "  [Train: 0.75s | Val: 0.22s | Save: 0.00s | Total: 0.97s]\n",
      "Saved new best model.\n",
      "Epoch 2/10 | Train Loss: 0.9792 | Val Loss: 0.9974\n",
      "  [Train: 0.29s | Val: 0.22s | Save: 0.00s | Total: 0.51s]\n",
      "Saved new best model.\n",
      "Epoch 3/10 | Train Loss: 0.9688 | Val Loss: 0.9857\n",
      "  [Train: 0.29s | Val: 0.22s | Save: 0.00s | Total: 0.51s]\n",
      "Saved new best model.\n",
      "Epoch 4/10 | Train Loss: 0.9526 | Val Loss: 0.9742\n",
      "  [Train: 0.30s | Val: 0.22s | Save: 0.00s | Total: 0.53s]\n",
      "Saved new best model.\n",
      "Epoch 5/10 | Train Loss: 0.9438 | Val Loss: 0.9628\n",
      "  [Train: 0.30s | Val: 0.24s | Save: 0.00s | Total: 0.54s]\n",
      "Saved new best model.\n",
      "Epoch 6/10 | Train Loss: 0.9346 | Val Loss: 0.9515\n",
      "  [Train: 0.30s | Val: 0.22s | Save: 0.00s | Total: 0.53s]\n",
      "Saved new best model.\n",
      "Epoch 7/10 | Train Loss: 0.9236 | Val Loss: 0.9404\n",
      "  [Train: 0.29s | Val: 0.21s | Save: 0.00s | Total: 0.50s]\n",
      "Saved new best model.\n",
      "Epoch 8/10 | Train Loss: 0.9120 | Val Loss: 0.9293\n",
      "  [Train: 0.31s | Val: 0.23s | Save: 0.00s | Total: 0.54s]\n",
      "Saved new best model.\n",
      "Epoch 9/10 | Train Loss: 0.9037 | Val Loss: 0.9184\n",
      "  [Train: 0.29s | Val: 0.22s | Save: 0.00s | Total: 0.52s]\n",
      "Saved new best model.\n",
      "Epoch 10/10 | Train Loss: 0.8918 | Val Loss: 0.9076\n",
      "  [Train: 0.29s | Val: 0.22s | Save: 0.00s | Total: 0.52s]\n",
      "\n",
      "----*----------*-------------*--------------- Node: x2 ------------*-----------------*-------------------*--\n",
      "Constructed TRAM model: TramModel(\n",
      "  (nn_int): SimpleIntercept(\n",
      "    (fc): Linear(in_features=1, out_features=1, bias=False)\n",
      "  )\n",
      "  (nn_shift): ModuleList(\n",
      "    (0): LinearShift(\n",
      "      (fc): Linear(in_features=1, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Parents dtype: OrderedDict([('x1', 'ord')])\n",
      "No existing model found. Starting fresh...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected mat1 and mat2 to have the same dtype, but got: long int != float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m     scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m########################## 7. Training Loop ##############################\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtrain_val_loop_v3\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m               \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m               \u001b[49m\u001b[43mtarget_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m               \u001b[49m\u001b[43mNODE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m               \u001b[49m\u001b[43mtram_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m               \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m               \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m               \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m               \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m               \u001b[49m\u001b[43muse_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m               \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m               \u001b[49m\u001b[43msave_linear_shifts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m               \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m               \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TramDag/utils/tram_model_helpers.py:654\u001b[0m, in \u001b[0;36mtrain_val_loop_v3\u001b[0;34m(node, target_nodes, NODE_DIR, tram_model, train_loader, val_loader, epochs, optimizer, use_scheduler, scheduler, save_linear_shifts, verbose, device)\u001b[0m\n\u001b[1;32m    651\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    653\u001b[0m int_input, shift_list \u001b[38;5;241m=\u001b[39m preprocess_inputs(x, ordered_transformation_terms_in_h\u001b[38;5;241m.\u001b[39mvalues(), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 654\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtram_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mint_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mint_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshift_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# print(f\"y_pred shape: {y_pred}, y shape: {y}\")\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# print(f'min_max:{min_max}')\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_nodes[node][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mord\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tramdag/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tramdag/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/TramDag/utils/tram_models.py:44\u001b[0m, in \u001b[0;36mTramModel.forward\u001b[0;34m(self, int_input, shift_input)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, shift_model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_shift):\n\u001b[1;32m     43\u001b[0m     shift_model \u001b[38;5;241m=\u001b[39m shift_model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 44\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mshift_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshift_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     shift_out\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint_out\u001b[39m\u001b[38;5;124m'\u001b[39m: int_out, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshift_out\u001b[39m\u001b[38;5;124m'\u001b[39m: shift_out}\n",
      "File \u001b[0;32m~/anaconda3/envs/tramdag/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tramdag/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/TramDag/utils/tram_models.py:123\u001b[0m, in \u001b[0;36mLinearShift.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tramdag/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tramdag/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/tramdag/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected mat1 and mat2 to have the same dtype, but got: long int != float"
     ]
    }
   ],
   "source": [
    "# For each NODE \n",
    "configuration_dict = load_configuration_dict(CONF_DICT_PATH)\n",
    "target_nodes = configuration_dict['nodes']\n",
    "\n",
    "\n",
    "for node in target_nodes:\n",
    "    \n",
    "    print(f'\\n----*----------*-------------*--------------- Node: {node} ------------*-----------------*-------------------*--')\n",
    "    ########################## 0. Skip nodes ###############################\n",
    "    if node not in train_list:# Skip if node is not in train_list\n",
    "        print(f\"Skipping node {node} as it's not in the training list.\")\n",
    "        continue\n",
    "    if (target_nodes[node]['node_type'] == 'source') and (target_nodes[node]['node_type'] == 'other'):# Skip unsupported types\n",
    "        print(f\"Node type : other , is not supported yet\")\n",
    "        continue\n",
    "\n",
    "    ########################## 1. Setup Paths ###############################\n",
    "    NODE_DIR = os.path.join(EXPERIMENT_DIR, f'{node}')\n",
    "    os.makedirs(NODE_DIR, exist_ok=True)\n",
    "    \n",
    "    # Check if training is complete\n",
    "    if not check_if_training_complete(node, NODE_DIR, epochs):\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    ########################## 2. Create Model ##############################\n",
    "    tram_model = get_fully_specified_tram_model_v2(node, target_nodes, verbose=True).to(device)\n",
    "\n",
    "    \n",
    "    ########################## 3. Create Dataloaders ########################\n",
    "    train_loader, val_loader = get_dataloader_v2(node, target_nodes, train_df, val_df, batch_size=batch_size, verbose=True)\n",
    "\n",
    "    ########################## 5. Optimizer & Scheduler ######################.\n",
    "    \n",
    "    optimizer =torch.optim.Adam(tram_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    ########################## 7. Training Loop ##############################\n",
    "    \n",
    "    train_val_loop_v3(\n",
    "                   node,\n",
    "                   target_nodes,\n",
    "                   NODE_DIR,\n",
    "                   tram_model,\n",
    "                   train_loader,\n",
    "                   val_loader,\n",
    "                   epochs,\n",
    "                   optimizer,\n",
    "                   use_scheduler,\n",
    "                   scheduler,\n",
    "                   save_linear_shifts=False,\n",
    "                   verbose=1,\n",
    "                   device=device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13c140",
   "metadata": {},
   "source": [
    "# 6 Inspect Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f4e2c7",
   "metadata": {},
   "source": [
    "## 6.1 Loss vs epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a93ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training_history(target_nodes,EXPERIMENT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b5922",
   "metadata": {},
   "source": [
    "## 6.2 inspect transformation function for source nodes h()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7116405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hdag_for_source_nodes_v2(target_nodes,EXPERIMENT_DIR,device,xmin_plot=0,xmax_plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee2a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a058e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.loss_ordinal import transform_intercepts_ordinal\n",
    "\n",
    "def show_hdag_for_single_source_node_ordinal(node,target_nodes,EXPERIMENT_DIR,device):\n",
    "        verbose=False\n",
    "        n=1000\n",
    "        #### 0.  paths\n",
    "        NODE_DIR = os.path.join(EXPERIMENT_DIR, f'{node}')\n",
    "        \n",
    "        ##### 1.  load model \n",
    "        model_path = os.path.join(NODE_DIR, \"best_model.pt\")\n",
    "        tram_model = get_fully_specified_tram_model_v2(node, target_nodes, verbose=verbose)\n",
    "        tram_model = tram_model.to(device)\n",
    "        tram_model.load_state_dict(torch.load(model_path))\n",
    "        _, ordered_transformation_terms_in_h, _=ordered_parents(node, target_nodes)\n",
    "        \n",
    "        #### 2. Sampling Dataloader\n",
    "        dataset = SamplingDataset(node=node,EXPERIMENT_DIR=EXPERIMENT_DIR,number_of_samples=n, target_nodes=target_nodes, transform=None)\n",
    "        sample_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "        output_list = []\n",
    "        with torch.no_grad():\n",
    "            for x in tqdm(sample_loader, desc=f\"h() for  {node}\"):\n",
    "                x = [xi.to(device) for xi in x]\n",
    "                int_input, shift_list = preprocess_inputs(x,ordered_transformation_terms_in_h, device=device)\n",
    "                model_outputs = tram_model(int_input=int_input, shift_input=shift_list)\n",
    "                output_list.append(model_outputs)\n",
    "                break\n",
    "        if verbose:\n",
    "            print(\"source node, Defaults to SI and 1 as inputs\")\n",
    "            \n",
    "        int_out =     output_list[0]['int_out'][0]  # Shape: (20,)\n",
    "        \n",
    "        theta_single=transform_intercepts_ordinal(int_out)\n",
    "        thetas_expanded = theta_single.repeat(n, 1).to(device)  # Shape: (n, 20)\n",
    "        return thetas_expanded\n",
    "        \n",
    "        \n",
    "thetas_expanded=show_hdag_for_single_source_node_ordinal('x1',target_nodes,EXPERIMENT_DIR,device)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da2b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_trafo_standart_logistic_v2(target_nodes, EXPERIMENT_DIR, train_df, val_df, device, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e17a4b",
   "metadata": {},
   "source": [
    "### Coefficient estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f050b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # enable 3D plotting\n",
    "\n",
    "# --- Your existing setup ---\n",
    "verbose    = False\n",
    "batch_size = 4112\n",
    "device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# this x is only for overlaying your target curve in 2D plots\n",
    "x1d = torch.linspace(-1, 1, steps=1000).unsqueeze(1).to(device)  # (1000, 1)\n",
    "def f(x):\n",
    "    return 0.75 * np.arctan(5 * (x + 0.12))\n",
    "\n",
    "for node in target_nodes:\n",
    "    print(f\"\\n---- check CS of {node} ----\")\n",
    "    if target_nodes[node]['node_type'] == 'source':\n",
    "        print(\"Node type: source  skipping.\")\n",
    "        continue\n",
    "\n",
    "    # load your model\n",
    "    NODE_DIR   = os.path.join(EXPERIMENT_DIR, node)\n",
    "    model_path = os.path.join(NODE_DIR, \"best_model.pt\")\n",
    "    tram_model = get_fully_specified_tram_model(node, target_nodes, verbose=verbose).to(device)\n",
    "    tram_model.load_state_dict(torch.load(model_path))\n",
    "    tram_model.eval()\n",
    "\n",
    "\n",
    "    for i, module in enumerate(tram_model.nn_shift):\n",
    "        name = module.__class__.__name__\n",
    "        print(f\"\\nModule {i}: {name}\")\n",
    "        print(module)\n",
    "\n",
    "        if name == 'LinearShift':\n",
    "            print(\"  LinearShift weights:\")\n",
    "            print(module.fc.weight.data)\n",
    "            continue\n",
    "\n",
    "        # read wanted input dims\n",
    "        in_feats = module.fc1.in_features\n",
    "        print(f\"  expects input shape = (batch_size, {in_feats})\")\n",
    "\n",
    "        # 2-input case  3D surface plot\n",
    "        if in_feats == 2:\n",
    "            N = 100\n",
    "            a = torch.linspace(-0, 2, steps=N, device=device)\n",
    "            b = torch.linspace(-0, 2, steps=N, device=device)\n",
    "            A, B = torch.meshgrid(a, b, indexing='ij')       # both (N, N)\n",
    "            grid = torch.stack([A, B], dim=-1).view(-1, 2)   # (N*N, 2)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                Y = module(grid).view(N, N).cpu().numpy()    # back to (N, N)\n",
    "\n",
    "            A_np = A.cpu().numpy()\n",
    "            B_np = B.cpu().numpy()\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax  = fig.add_subplot(111, projection='3d')\n",
    "            ax.plot_surface(A_np, B_np, Y, cmap='viridis', edgecolor='none')\n",
    "            ax.set_xlabel('input 1')\n",
    "            ax.set_ylabel('input 2')\n",
    "            ax.set_zlabel(f'{name}(x, x)')\n",
    "            ax.set_title(f\"{node} | Module {i}: {name} (3D surface)\")\n",
    "            plt.show()\n",
    "\n",
    "        # 1-input case  2D scatter\n",
    "        else:\n",
    "            N = 1000\n",
    "            lin = torch.linspace(-1, 1, steps=N, device=device)\n",
    "            dummy = lin.unsqueeze(1).repeat(1, in_feats)      # (N, in_feats)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y = module(dummy).squeeze().cpu().numpy()\n",
    "\n",
    "            plt.figure()\n",
    "            plt.scatter(dummy[:, 0].cpu().numpy(), y, s=5, label=f\"{name} output\")\n",
    "            # overlay target\n",
    "            plt.scatter(x1d.cpu().numpy(), -f(x1d.cpu().numpy()), c=\"red\", s=5, label=\"target\")\n",
    "            plt.xlabel(\"input value\")\n",
    "            plt.ylabel(\"output value\")\n",
    "            plt.title(f\"{node} | Module {i}: {name} (2D)\")\n",
    "            plt.legend()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eeb428",
   "metadata": {},
   "source": [
    "# 7. Sample from Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_full_dag_chandru(target_nodes_dict,\n",
    "                            EXPERIMENT_DIR,\n",
    "                            device,\n",
    "                            do_interventions={},\n",
    "                            n= 10_000,\n",
    "                            batch_size = 32,\n",
    "                            delete_all_previously_sampled=True,\n",
    "                            verbose=True):\n",
    "    \"\"\"\n",
    "    Samples data for all nodes in a DAG defined by `conf_dict`, ensuring that each node's\n",
    "    parents are sampled before the node itself. Supports interventions on any subset of nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    conf_dict : dict\n",
    "        Dictionary defining the DAG. Each key is a node name, and each value is a config\n",
    "        dict that includes at least:\n",
    "            - 'node_type': str, either 'source' or other\n",
    "            - 'parents': list of parent node names\n",
    "            - 'min': float, minimum allowed value for the node\n",
    "            - 'max': float, maximum allowed value for the node\n",
    "\n",
    "    EXPERIMENT_DIR : str\n",
    "        Base directory where all per-node directories are located.\n",
    "\n",
    "    device : torch.device\n",
    "        The device to run computations on (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    do_interventions : dict, optional\n",
    "        A dictionary specifying interventions for some nodes. Keys are node names (str),\n",
    "        values are floats. For each intervened node, the specified value is used as the\n",
    "        sampled value for all samples, and the model is bypassed. e.g. {'x1':1.0}\n",
    "\n",
    "    n : int, optional\n",
    "        Number of samples to draw for each node (default is 10_000).\n",
    "\n",
    "    batch_size : int, optional\n",
    "        Batch size for model evaluation during sampling (default is 32).\n",
    "\n",
    "    delete_all_previously_sampled : bool, optional\n",
    "        If True, removes previously sampled data before starting (default is True).\n",
    "\n",
    "    verbose : bool, optional\n",
    "        If True, prints debug/status information (default is True).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function ensures that nodes are only sampled after their parents.\n",
    "    - Nodes with `node_type='source'` are treated as having no parents.\n",
    "    - If a node is in `do_interventions`, `sampled_chandrupatla.pt` and a dummy `latents.pt`\n",
    "      are created, enabling downstream nodes to proceed.\n",
    "    - Sampling is done using a vectorized root-finding method (Chandrupatla's algorithm).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # delete the previolusly sampled data\n",
    "    if delete_all_previously_sampled:\n",
    "        delete_all_samplings(target_nodes_dict, EXPERIMENT_DIR)\n",
    "    \n",
    "    \n",
    "    # repeat process until all nodes are sampled\n",
    "    processed_nodes=[] # stack\n",
    "    while set(processed_nodes) != set(target_nodes_dict.keys()): \n",
    "        for node in target_nodes_dict: # for each node in the conf dict\n",
    "            if node in processed_nodes:\n",
    "                if verbose :\n",
    "                    print('node is already  in sampled list')\n",
    "                continue\n",
    "            \n",
    "            _, ordered_transformation_terms_in_h, _=ordered_parents(node, target_nodes_dict)\n",
    "\n",
    "            \n",
    "            print(f'\\n----*----------*-------------*--------Sample Node: {node} ------------*-----------------*-------------------*--') \n",
    "            \n",
    "            ## 1. Paths \n",
    "            NODE_DIR = os.path.join(EXPERIMENT_DIR, f'{node}')\n",
    "            SAMPLING_DIR = os.path.join(NODE_DIR, 'sampling')\n",
    "            os.makedirs(SAMPLING_DIR, exist_ok=True)\n",
    "            \n",
    "            \n",
    "            ## 2. Check if sampled and latents already exist \n",
    "            if check_sampled_and_latents(NODE_DIR, rootfinder='chandrupatla', verbose=verbose):\n",
    "                processed_nodes.append(node)\n",
    "                continue\n",
    "            \n",
    "            ## 3. logic to make sure parents are always sampled first\n",
    "            skipping_node = False\n",
    "            if target_nodes_dict[node]['node_type'] != 'source':\n",
    "                for parent in target_nodes_dict[node]['parents']:\n",
    "                    if not check_sampled_and_latents(os.path.join(EXPERIMENT_DIR, parent), rootfinder='chandrupatla', verbose=verbose):\n",
    "                        skipping_node = True\n",
    "                        break\n",
    "                    \n",
    "            if skipping_node:\n",
    "                print(f\"Skipping {node} as parent {parent} is not sampled yet.\")\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            \n",
    "            ## INTERVENTION, if node is to be intervened on , data is just saved\n",
    "            if node in do_interventions.keys():\n",
    "                intervention_value = do_interventions[node]\n",
    "                intervention_vals = torch.full((n,), intervention_value)\n",
    "                sampled_path = os.path.join(SAMPLING_DIR, \"sampled_chandrupatla.pt\")\n",
    "                torch.save(intervention_vals, sampled_path)\n",
    "                ### dummy latents jsut for the check , not needed\n",
    "                dummy_latents = torch.full((n,), float('nan'))  \n",
    "                latents_path = os.path.join(SAMPLING_DIR, \"latents.pt\")\n",
    "                torch.save(dummy_latents, latents_path)\n",
    "                processed_nodes.append(node)\n",
    "                \n",
    "            ## no intervention, based on the sampled data from the parents though the latents for each node the observational distribution is generated    \n",
    "            else:\n",
    "                ### sampling latents\n",
    "                latent_sample = torch.tensor(logistic.rvs(size=n), dtype=torch.float32).to(device)\n",
    "                #latent_sample = truncated_logistic_sample(n=n, low=0, high=1, device=device)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"-- sampled latents\")\n",
    "                \n",
    "                ### load modelweights\n",
    "                model_path = os.path.join(NODE_DIR, \"best_model.pt\")\n",
    "                tram_model = get_fully_specified_tram_model(node, target_nodes_dict, verbose=verbose).to(device)\n",
    "                tram_model.load_state_dict(torch.load(model_path))\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"-- loaded modelweights\")\n",
    "                \n",
    "                \n",
    "                # TODO samples for ordinal target\n",
    "                \n",
    "                if \n",
    "                    \n",
    "                dataset = SamplingDataset(node=node, EXPERIMENT_DIR=EXPERIMENT_DIR, rootfinder='chandrupatla', number_of_samples=n, conf_dict=target_nodes_dict, transform=None)\n",
    "                sample_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "                \n",
    "                output_list = []\n",
    "                with torch.no_grad():\n",
    "                    for x in tqdm(sample_loader, desc=f\"h() for samples in  {node}\"):\n",
    "                        x = [xi.to(device) for xi in x]\n",
    "                        \n",
    "                        print(f\"x {x}\")\n",
    "                        int_input, shift_list = preprocess_inputs(x,ordered_transformation_terms_in_h.values(), device=device)\n",
    "                        \n",
    "                    \n",
    "                        if int_input is not None:\n",
    "                            print(f\"int_input {int_input.shape}\")\n",
    "                        if shift_list is not None:\n",
    "                            print(f\"shift_list {[s.shape for s in shift_list]}\") \n",
    "                        \n",
    "                        print([t.shape for t in x])\n",
    "                        \n",
    "                        model_outputs = tram_model(int_input=int_input, shift_input=shift_list)\n",
    "                        \n",
    "                        print(f\"model_outputs {model_outputs}\")\n",
    "                        \n",
    "                        output_list.append(model_outputs)\n",
    "                        \n",
    "                if target_nodes_dict[node]['node_type'] == 'source':\n",
    "                    if verbose:\n",
    "                        print(\"source node, Defaults to SI and 1 as inputs\")\n",
    "                    theta_single = output_list[0]['int_out'][0]\n",
    "                    theta_single = transform_intercepts_continous(theta_single)\n",
    "                    thetas_expanded = theta_single.repeat(n, 1)\n",
    "                    shifts = torch.zeros(n, device=device)\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(\"node has parents, previously sampled data is loaded for each pa(node)\")\n",
    "                    y_pred = merge_outputs(output_list, skip_nan=True)\n",
    "                    shifts = y_pred['shift_out']\n",
    "                    if shifts is None:\n",
    "                        print(\"shift_out was None; defaulting to zeros.\")\n",
    "                        shifts = torch.zeros(n, device=device)\n",
    "                    thetas = y_pred['int_out']\n",
    "                    thetas_expanded = transform_intercepts_continous(thetas).squeeze()\n",
    "                    shifts = shifts.squeeze()\n",
    "                \n",
    "                \n",
    "                \n",
    "                low = torch.full((n,), -1e5, device=device)\n",
    "                high = torch.full((n,), 1e5, device=device)\n",
    "                min_vals = torch.tensor(target_nodes_dict[node]['min'], dtype=torch.float32).to(device)\n",
    "                max_vals = torch.tensor(target_nodes_dict[node]['max'], dtype=torch.float32).to(device)\n",
    "                min_max = torch.stack([min_vals, max_vals], dim=0)\n",
    "                \n",
    "                ## Root finder using Chandrupatla's method\n",
    "                def f_vectorized(targets):\n",
    "                    return vectorized_object_function(\n",
    "                        thetas_expanded,\n",
    "                        targets,\n",
    "                        shifts,\n",
    "                        latent_sample,\n",
    "                        k_min=min_max[0],\n",
    "                        k_max=min_max[1]\n",
    "                    )\n",
    "                    \n",
    "                root = chandrupatla_root_finder(\n",
    "                    f_vectorized,\n",
    "                    low,\n",
    "                    high,\n",
    "                    max_iter=10_000,\n",
    "                    tol=1e-9\n",
    "                )\n",
    "                \n",
    "                ## Saving\n",
    "                sampled_path = os.path.join(SAMPLING_DIR, \"sampled_chandrupatla.pt\")\n",
    "                latents_path = os.path.join(SAMPLING_DIR, \"latents.pt\")\n",
    "                \n",
    "                if torch.isnan(root).any():\n",
    "                    print(f'Caution! Sampling for {node} consists of NaNs')\n",
    "                    \n",
    "                torch.save(root, sampled_path)\n",
    "                torch.save(latent_sample, latents_path)\n",
    "                \n",
    "                processed_nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_full_dag_chandru(target_nodes,\n",
    "                            EXPERIMENT_DIR,\n",
    "                            device,\n",
    "                            n= 100,\n",
    "                            batch_size = 2,\n",
    "                            delete_all_previously_sampled=True,\n",
    "                            verbose=True)     \n",
    "\n",
    "# TODO Fix bug with x6 sampling , training works, sampling has issue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0068c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training funktionerit \n",
    "#   (nn_int): ComplexInterceptDefaultTabular(\n",
    "#     (fc1): Linear(in_features=2, out_features=8, bias=True)\n",
    "#     (relu1): ReLU()\n",
    "#     (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
    "#     (relu2): ReLU()\n",
    "#     (fc3): Linear(in_features=8, out_features=20, bias=False)\n",
    "#   )\n",
    "#   (nn_shift): ModuleList(\n",
    "#     (0-1): 2 x ComplexShiftDefaultTabular(\n",
    "#       (fc1): Linear(in_features=1, out_features=64, bias=True)\n",
    "#       (relu1): ReLU()\n",
    "#       (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
    "#       (relu2): ReLU()\n",
    "#       (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
    "#       (relu3): ReLU()\n",
    "#       (fc4): Linear(in_features=64, out_features=1, bias=False)\n",
    "#     )\n",
    "#     (2): LinearShift(\n",
    "#       (fc): Linear(in_features=1, out_features=1, bias=False)\n",
    "#     )\n",
    "#   )\n",
    "# )\n",
    "# Existing model found. Loading weights and history...\n",
    "# Continuing training from epoch 3...\n",
    "# int_input torch.Size([512, 2])  \n",
    "# \n",
    "# shift_list [torch.Size([512, 1]), torch.Size([512, 1]), torch.Size([512, 1])]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## sampling does not work\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2fc3ba",
   "metadata": {},
   "source": [
    "## 7.3 Inspect Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples_vs_true(test_df,target_nodes,EXPERIMENT_DIR,rootfinder='chandrupatla')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6be30f",
   "metadata": {},
   "source": [
    "## 7.4 Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6986aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return -0.3*x  \n",
    "\n",
    "intervention_df=dgp(10_000, doX=[-1.0, None, None], seed=-1)\n",
    "sns.pairplot(intervention_df)\n",
    "plt.suptitle(\"\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_full_dag_chandru(target_nodes,\n",
    "                            EXPERIMENT_DIR,\n",
    "                            device,\n",
    "                            do_interventions={'x1':-1.0},\n",
    "                            n= 10_000,\n",
    "                            batch_size = 32,\n",
    "                            delete_all_previously_sampled=True,\n",
    "                            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cb87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples_vs_true(intervention_df,target_nodes,EXPERIMENT_DIR,rootfinder='chandrupatla')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
