{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of Continous Loss Function from R File python conversion.qmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train$df_orig\n",
    "# tf.Tensor(\n",
    "# [[ 0.38709584 -0.18872575  3.9523134 ]\n",
    "#  [ 0.7616431  -0.11664478 -1.4364368 ]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "# min max for tensors q5 nd q95\n",
    "# tf.Tensor([ 0.4058232  -0.18512172 -1.1669992 ], shape=(3), dtype=float32)\n",
    "# tf.Tensor([ 0.74291575 -0.12024883  3.6828759 ], shape=(3), dtype=float32)\n",
    "\n",
    "# h_params\n",
    "# tf.Tensor(\n",
    "# [[[ 0.          0.         -0.05309175 -0.02268984 -0.00073227\n",
    "#     0.14473043 -0.19736235 -0.01326796  0.01716295  0.04003201\n",
    "#    -0.03761906  0.02019005  0.02103117  0.01475964  0.02561173\n",
    "#     0.04465405  0.07558934 -0.03455871  0.03127078 -0.06442269\n",
    "#    -0.03358709 -0.0046008 ]\n",
    "\n",
    "#   [ 0.          0.03179859 -0.03748546  0.03065695  0.01450718\n",
    "#     0.05038024  0.08786372 -0.01381071  0.08681878  0.03438636\n",
    "#    -0.11747339 -0.07255729 -0.02515779 -0.06152298  0.01535351\n",
    "#     0.01071469  0.00725646 -0.02823549 -0.01658672 -0.031664\n",
    "#     0.06521034 -0.05619556]\n",
    "\n",
    "#   [ 0.06369114  0.          0.07416828 -0.02284147  0.02354072\n",
    "#     0.06744125 -0.09243326  0.03462008  0.08162504  0.09570658\n",
    "#    -0.06736961 -0.04148068  0.03080387  0.07890061 -0.0014052\n",
    "#     0.0158043   0.06599072  0.01494174 -0.05903206 -0.00370814\n",
    "#     0.00470866  0.04583171]]\n",
    "\n",
    "#  [[ 0.          0.         -0.05309175 -0.02268984 -0.00073227\n",
    "#     0.14473043 -0.19736235 -0.01326796  0.01716295  0.04003201\n",
    "#    -0.03761906  0.02019005  0.02103117  0.01475964  0.02561173\n",
    "#     0.04465405  0.07558934 -0.03455871  0.03127078 -0.06442269\n",
    "#    -0.03358709 -0.0046008 ]\n",
    "\n",
    "#   [ 0.          0.06256635 -0.03748546  0.03065695  0.01450718\n",
    "#     0.05038024  0.08786372 -0.01381071  0.08681878  0.03438636\n",
    "#    -0.11747339 -0.07255729 -0.02515779 -0.06152298  0.01535351\n",
    "#     0.01071469  0.00725646 -0.02823549 -0.01658672 -0.031664\n",
    "#     0.06521034 -0.05619556]\n",
    "\n",
    "#   [ 0.06369113  0.          0.07416828 -0.02284142  0.02354072\n",
    "#     0.06744127 -0.09243318  0.0346201   0.08162504  0.09570658\n",
    "#    -0.06736961 -0.04148067  0.03080386  0.07890059 -0.0014052\n",
    "#     0.01580431  0.06599071  0.01494177 -0.05903205 -0.00370815\n",
    "#     0.00470866  0.04583171]]], shape=(2, 3, 22), dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R -> to_Theta, Python transform_intercepts_continpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_intercepts_continous(theta_tilde:torch.Tensor) -> torch.Tensor:\n",
    "    \n",
    "    \"\"\"\n",
    "    Transforms the unordered theta_tilde to ordered theta values for the bernstein polynomial\n",
    "    E.G: \n",
    "    theta_1 = theta_tilde_1\n",
    "    theta_2 = theta_tilde_1 + exp(theta_tilde_2)\n",
    "    ..\n",
    "    :param theta_tilde: The unordered theta_tilde values\n",
    "    :return: The ordered theta values\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the shift based on the last dimension size\n",
    "    last_dim_size = theta_tilde.shape[-1]\n",
    "    shift = torch.log(torch.tensor(2.0)) * last_dim_size / 2\n",
    "\n",
    "    # Get the width values by applying softplus from the second position onward\n",
    "    widths = torch.nn.functional.softplus(theta_tilde[..., 1:])\n",
    "\n",
    "    # Concatenate the first value (raw) with the softplus-transformed widths\n",
    "    widths = torch.cat([theta_tilde[..., [0]], widths], dim=-1)\n",
    "\n",
    "    # Return the cumulative sum minus the shift\n",
    "    return torch.cumsum(widths, dim=-1) - shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta tilde\n",
    "\n",
    "\n",
    "# tf.Tensor(\n",
    "# [[[-0.05309175 -0.02268984 -0.00073227  0.14473043 -0.19736235\n",
    "#    -0.01326796  0.01716295  0.04003201 -0.03761906  0.02019005\n",
    "#     0.02103117  0.01475964  0.02561173  0.04465405  0.07558934\n",
    "#    -0.03455871  0.03127078 -0.06442269 -0.03358709 -0.0046008 ]\n",
    "#   [-0.03748546  0.03065695  0.01450718  0.05038024  0.08786372\n",
    "#    -0.01381071  0.08681878  0.03438636 -0.11747339 -0.07255729\n",
    "#    -0.02515779 -0.06152298  0.01535351  0.01071469  0.00725646\n",
    "#    -0.02823549 -0.01658672 -0.031664    0.06521034 -0.05619556]\n",
    "#   [ 0.07416828 -0.02284147  0.02354072  0.06744125 -0.09243326\n",
    "#     0.03462008  0.08162504  0.09570658 -0.06736961 -0.04148068\n",
    "#     0.03080387  0.07890061 -0.0014052   0.0158043   0.06599072\n",
    "#     0.01494174 -0.05903206 -0.00370814  0.00470866  0.04583171]]\n",
    "\n",
    "#  [[-0.05309175 -0.02268984 -0.00073227  0.14473043 -0.19736235\n",
    "#    -0.01326796  0.01716295  0.04003201 -0.03761906  0.02019005\n",
    "#     0.02103117  0.01475964  0.02561173  0.04465405  0.07558934\n",
    "#    -0.03455871  0.03127078 -0.06442269 -0.03358709 -0.0046008 ]\n",
    "#   [-0.03748546  0.03065695  0.01450718  0.05038024  0.08786372\n",
    "#    -0.01381071  0.08681878  0.03438636 -0.11747339 -0.07255729\n",
    "#    -0.02515779 -0.06152298  0.01535351  0.01071469  0.00725646\n",
    "#    -0.02823549 -0.01658672 -0.031664    0.06521034 -0.05619556]\n",
    "#   [ 0.07416828 -0.02284142  0.02354072  0.06744127 -0.09243318\n",
    "#     0.0346201   0.08162504  0.09570658 -0.06736961 -0.04148067\n",
    "#     0.03080386  0.07890059 -0.0014052   0.01580431  0.06599071\n",
    "#     0.01494177 -0.05903205 -0.00370815  0.00470866  0.04583171]]], shape=(2, 3, 20), dtype=float32)\n",
    "\n",
    "\n",
    "# tf.Tensor(\n",
    "# [[[-6.9845634e+00 -6.3026972e+00 -5.6099157e+00 -4.8417873e+00\n",
    "#    -4.2424603e+00 -3.5559251e+00 -2.8541598e+00 -2.1407962e+00\n",
    "#    -1.4662814e+00 -7.6298809e-01 -5.9269905e-02  6.4128447e-01\n",
    "#     1.3473196e+00  2.0630426e+00  2.7946987e+00  3.4707155e+00\n",
    "#     4.1796207e+00  4.8410749e+00  5.5175695e+00  6.2084188e+00]\n",
    "#   [-6.9689574e+00 -6.2603641e+00 -5.5599370e+00 -4.8412828e+00\n",
    "#    -4.1032391e+00 -3.4169731e+00 -2.6794748e+00 -1.9689865e+00\n",
    "#    -1.3328519e+00 -6.7532539e-01  5.3219795e-03  6.6818047e-01\n",
    "#     1.3690338e+00  2.0675526e+00  2.7643347e+00  3.4434633e+00\n",
    "#     4.1283512e+00  4.8057919e+00  5.5320759e+00  6.1975203e+00]\n",
    "#   [-6.8573036e+00 -6.1755118e+00 -5.4705248e+00 -4.7430887e+00\n",
    "#    -4.0950909e+00 -3.3844838e+00 -2.6496916e+00 -1.9075465e+00\n",
    "#    -1.2475171e+00 -5.7489538e-01  1.3377237e-01  8.6714792e-01\n",
    "#     1.5595932e+00  2.2606735e+00  2.9873600e+00  3.6880064e+00\n",
    "#     4.3520727e+00  5.0433674e+00  5.7388716e+00  6.4551973e+00]]\n",
    "\n",
    "#    [-6.9845634e+00 -6.3026972e+00 -5.6099157e+00 -4.8417873e+00\n",
    "#    -4.2424603e+00 -3.5559251e+00 -2.8541598e+00 -2.1407962e+00\n",
    "#    -1.4662814e+00 -7.6298809e-01 -5.9269905e-02  6.4128447e-01\n",
    "#     1.3473196e+00  2.0630426e+00  2.7946987e+00  3.4707155e+00\n",
    "#     4.1796207e+00  4.8410749e+00  5.5175695e+00  6.2084188e+00]\n",
    "#   [-6.9689574e+00 -6.2603641e+00 -5.5599370e+00 -4.8412828e+00\n",
    "#    -4.1032391e+00 -3.4169731e+00 -2.6794748e+00 -1.9689865e+00\n",
    "#    -1.3328519e+00 -6.7532539e-01  5.3219795e-03  6.6818047e-01\n",
    "#     1.3690338e+00  2.0675526e+00  2.7643347e+00  3.4434633e+00\n",
    "#     4.1283512e+00  4.8057919e+00  5.5320759e+00  6.1975203e+00]\n",
    "#   [-6.8573036e+00 -6.1755118e+00 -5.4705248e+00 -4.7430887e+00\n",
    "#    -4.0950909e+00 -3.3844838e+00 -2.6496916e+00 -1.9075465e+00\n",
    "#    -1.2475171e+00 -5.7489538e-01  1.3377237e-01  8.6714792e-01\n",
    "#     1.5595932e+00  2.2606735e+00  2.9873600e+00  3.6880064e+00\n",
    "#     4.3520727e+00  5.0433674e+00  5.7388716e+00  6.4551973e+00]]], shape=(2, 3, 20), dtype=float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.05309175, -0.02268984, -0.00073227,  0.14473043, -0.19736235,\n",
       "         -0.01326796,  0.01716295,  0.04003201, -0.03761906,  0.02019005,\n",
       "          0.02103117,  0.01475964,  0.02561173,  0.04465405,  0.07558934,\n",
       "         -0.03455871,  0.03127078, -0.06442269, -0.03358709, -0.00460080],\n",
       "        [-0.05309175, -0.02268984, -0.00073227,  0.14473043, -0.19736235,\n",
       "         -0.01326796,  0.01716295,  0.04003201, -0.03761906,  0.02019005,\n",
       "          0.02103117,  0.01475964,  0.02561173,  0.04465405,  0.07558934,\n",
       "         -0.03455871,  0.03127078, -0.06442269, -0.03358709, -0.00460080]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# thetas from Model \n",
    "\n",
    "theta_tilde_r = torch.tensor([\n",
    "    [-0.05309175, -0.02268984, -0.00073227,  0.14473043, -0.19736235,\n",
    "     -0.01326796,  0.01716295,  0.04003201, -0.03761906,  0.02019005,\n",
    "      0.02103117,  0.01475964,  0.02561173,  0.04465405,  0.07558934,\n",
    "     -0.03455871,  0.03127078, -0.06442269, -0.03358709, -0.0046008],\n",
    "\n",
    "    [-0.05309175, -0.02268984, -0.00073227,  0.14473043, -0.19736235,\n",
    "     -0.01326796,  0.01716295,  0.04003201, -0.03761906,  0.02019005,\n",
    "      0.02103117,  0.01475964,  0.02561173,  0.04465405,  0.07558934,\n",
    "     -0.03455871,  0.03127078, -0.06442269, -0.03358709, -0.0046008]\n",
    "])\n",
    "\n",
    "theta_r = torch.tensor([\n",
    "    [-6.9845634e+00, -6.3026972e+00, -5.6099157e+00, -4.8417873e+00,\n",
    "     -4.2424603e+00, -3.5559251e+00, -2.8541598e+00, -2.1407962e+00,\n",
    "     -1.4662814e+00, -7.6298809e-01, -5.9269905e-02,  6.4128447e-01,\n",
    "      1.3473196e+00,  2.0630426e+00,  2.7946987e+00,  3.4707155e+00,\n",
    "      4.1796207e+00,  4.8410749e+00,  5.5175695e+00,  6.2084188e+00],\n",
    "\n",
    "    [-6.9845634e+00, -6.3026972e+00, -5.6099157e+00, -4.8417873e+00,\n",
    "     -4.2424603e+00, -3.5559251e+00, -2.8541598e+00, -2.1407962e+00,\n",
    "     -1.4662814e+00, -7.6298809e-01, -5.9269905e-02,  6.4128447e-01,\n",
    "      1.3473196e+00,  2.0630426e+00,  2.7946987e+00,  3.4707155e+00,\n",
    "      4.1796207e+00,  4.8410749e+00,  5.5175695e+00,  6.2084188e+00]\n",
    "])\n",
    "\n",
    "theta_tilde_r   # same data see the to samples are the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.98456335, -6.30269718, -5.60991573, -4.84178734, -4.24246025,\n",
       "         -3.55592513, -2.85415936, -2.14079618, -1.46628141, -0.76298857,\n",
       "         -0.05927038,  0.64128399,  1.34731865,  2.06304264,  2.79469872,\n",
       "          3.47071552,  4.17961979,  4.84107494,  5.51756954,  6.20841885],\n",
       "        [-6.98456335, -6.30269718, -5.60991573, -4.84178734, -4.24246025,\n",
       "         -3.55592513, -2.85415936, -2.14079618, -1.46628141, -0.76298857,\n",
       "         -0.05927038,  0.64128399,  1.34731865,  2.06304264,  2.79469872,\n",
       "          3.47071552,  4.17961979,  4.84107494,  5.51756954,  6.20841885]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas_python=transform_intercepts_continous(theta_tilde_r)\n",
    "thetas_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00, -4.76837158e-07,  0.00000000e+00,\n",
       "          0.00000000e+00,  4.76837158e-07,  4.76837158e-07,  4.76837158e-07,\n",
       "          9.53674316e-07,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          9.53674316e-07,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00, -4.76837158e-07,  0.00000000e+00,\n",
       "          0.00000000e+00,  4.76837158e-07,  4.76837158e-07,  4.76837158e-07,\n",
       "          9.53674316e-07,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          9.53674316e-07,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_r-thetas_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- works Same output as R CODE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h_dag_extra calls hdag  and hdag dash which call bernstein poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_START=0.0001\n",
    "R_START=1-L_START"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernstein basis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernstein_basis(tensor, M):\n",
    "    \"\"\"\n",
    "    Compute the Bernstein basis polynomials for a given input tensor.\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Input tensor of shape (n_samples).\n",
    "        M (int): Degree of the Bernstein polynomial.\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape (B, Nodes, M+1) with the Bernstein basis.\n",
    "    \"\"\"\n",
    "    tensor = torch.as_tensor(tensor)\n",
    "    dtype = tensor.dtype\n",
    "    M = torch.tensor(M, dtype=dtype, device=tensor.device)\n",
    "\n",
    "    # Expand dims to allow broadcasting\n",
    "    tensor_expanded = tensor.unsqueeze(-1)  # shape (B, Nodes, 1)\n",
    "\n",
    "    # Clip values to avoid log(0)\n",
    "    eps = torch.finfo(dtype).eps\n",
    "    tensor_expanded = torch.clamp(tensor_expanded, min=eps, max=1 - eps)\n",
    "\n",
    "    k_values = torch.arange(M + 1, dtype=dtype, device=tensor.device)  # shape (M+1,)\n",
    "    \n",
    "    # Log binomial coefficient: log(M choose k)\n",
    "    log_binomial_coeff = (\n",
    "        torch.lgamma(M + 1) \n",
    "        - torch.lgamma(k_values + 1) \n",
    "        - torch.lgamma(M - k_values + 1)\n",
    "    )\n",
    "\n",
    "    # Log powers\n",
    "    log_powers = (\n",
    "        k_values * torch.log(tensor_expanded)\n",
    "        + (M - k_values) * torch.log(1 - tensor_expanded)\n",
    "    )\n",
    "\n",
    "    # Bernstein basis in log space\n",
    "    log_bernstein = log_binomial_coeff + log_powers  # shape (B, Nodes, M+1)\n",
    "\n",
    "    return torch.exp(log_bernstein)\n",
    "\n",
    "def test_bernstein_basis_1d():\n",
    "    samples = torch.tensor([0.2, 0.4, 0.4], dtype=torch.float32)\n",
    "    M = 4\n",
    "    output = bernstein_basis(samples, M)\n",
    "\n",
    "    # Should return shape (N, M+1)\n",
    "    assert output.shape == (len(samples), M + 1), f\"Expected shape {(len(samples), M + 1)}, got {output.shape}\"\n",
    "\n",
    "    # All values should be >= 0\n",
    "    assert torch.all(output >= 0), \"All Bernstein basis values should be non-negative\"\n",
    "\n",
    "    # Each row should sum to ~1\n",
    "    row_sums = output.sum(dim=1)\n",
    "    assert torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-6), f\"Sums were not 1: {row_sums}\"\n",
    "\n",
    "test_bernstein_basis_1d()\n",
    "\n",
    "def h_dag(targets: torch.Tensor, thetas: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        targets: shape (n,)\n",
    "        thetas: shape (n, b)\n",
    "    Returns:\n",
    "        Tensor: shape (n,)\n",
    "    \"\"\"\n",
    "    _, b = thetas.shape\n",
    "    B = bernstein_basis(targets, b - 1)  # shape (n, b)\n",
    "    return torch.mean(B * thetas, dim=1)\n",
    "\n",
    "def h_dag_dash(targets: torch.Tensor, thetas: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        targets: shape (n,)\n",
    "        thetas: shape (n, b)\n",
    "    Returns:\n",
    "        Tensor: shape (n,)\n",
    "    \"\"\"\n",
    "    _, b = thetas.shape\n",
    "    dtheta = thetas[:, 1:] - thetas[:, :-1]         # shape (n, b-1)\n",
    "    B_dash = bernstein_basis(targets, b - 2)        # shape (n, b-1)\n",
    "    return torch.sum(B_dash * dtheta, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  h_extarpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train$df_orig\n",
    "# tf.Tensor(\n",
    "# [[ 0.38709584 -0.18872575  3.9523134 ]\n",
    "#  [ 0.7616431  -0.11664478 -1.4364368 ]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "# min max for tensors q5 nd q95\n",
    "# tf.Tensor([ 0.4058232  -0.18512172 -1.1669992 ], shape=(3), dtype=float32)\n",
    "# tf.Tensor([ 0.74291575 -0.12024883  3.6828759 ], shape=(3), dtype=float32)\n",
    "\n",
    "\n",
    "targets=torch.tensor([ 0.38709584, 0.7616431 ])\n",
    "min=0.4058232\n",
    "max=0.74291575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_dag_extra: tensor([-0.38710955,  0.34880140])\n",
      "differnece r python : tensor([-4.41074371e-06,  2.05636024e-06])\n"
     ]
    }
   ],
   "source": [
    "def h_extrapolated(thetas: torch.Tensor, targets: torch.Tensor, k_min: float, k_max: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        thetas: shape (n, b)\n",
    "        targets: shape (n,)\n",
    "        k_min: float, lower bound of scaling (not tracked in graph)\n",
    "        k_max: float, upper bound of scaling (not tracked in graph)\n",
    "    Returns:\n",
    "        Tensor of shape (n,)\n",
    "    \"\"\"\n",
    "    # Constants (not part of the graph)\n",
    "    L_START = 0.0\n",
    "    R_START = 1.0\n",
    "\n",
    "    # Detach constants from graph\n",
    "    L_tensor = torch.tensor(L_START, dtype=targets.dtype, device=targets.device)\n",
    "    R_tensor = torch.tensor(R_START, dtype=targets.dtype, device=targets.device)\n",
    "\n",
    "    # Scale targets\n",
    "    t_i = (targets - k_min) / (k_max - k_min)  # shape (n,)\n",
    "    t_i_exp = t_i.unsqueeze(-1)  # shape (n, 1)\n",
    "\n",
    "    # Extrapolation at left (t_i < 0)\n",
    "    b0 = h_dag(L_tensor.expand_as(targets), thetas).unsqueeze(-1)     # (n, 1)\n",
    "    slope0 = h_dag_dash(L_tensor.expand_as(targets), thetas).unsqueeze(-1)  # (n, 1)\n",
    "    h_left = slope0 * (t_i_exp - L_tensor) + b0\n",
    "\n",
    "    # Start with placeholder\n",
    "    h = h_left.clone()\n",
    "\n",
    "    # Mask for left extrapolation\n",
    "    mask0 = t_i_exp < L_tensor\n",
    "    h = torch.where(mask0, h_left, t_i_exp)  # placeholder fill\n",
    "\n",
    "    # Extrapolation at right (t_i > 1)\n",
    "    b1 = h_dag(R_tensor.expand_as(targets), thetas).unsqueeze(-1)\n",
    "    slope1 = h_dag_dash(R_tensor.expand_as(targets), thetas).unsqueeze(-1)\n",
    "    h_right = slope1 * (t_i_exp - R_tensor) + b1\n",
    "\n",
    "    mask1 = t_i_exp > R_tensor\n",
    "    h = torch.where(mask1, h_right, h)\n",
    "\n",
    "    # In-domain: t_i ∈ [0,1]\n",
    "    mask_mid = (t_i_exp >= L_tensor) & (t_i_exp <= R_tensor)\n",
    "    h_center = h_dag(t_i, thetas).unsqueeze(-1)\n",
    "    h = torch.where(mask_mid, h_center, h)\n",
    "\n",
    "    return h.squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "#output h_dag extra\n",
    "# h_dag_extra(t_i[,cont_dims, drop=FALSE],\n",
    "#                     theta[,cont_dims,\n",
    "#                     1:len_theta,drop=FALSE],\n",
    "#                     k_min[cont_dims],\n",
    "#                     k_max[cont_dims])\n",
    "\n",
    "# tf.Tensor(\n",
    "# [[-0.38711396 -0.38781664  0.36255726]\n",
    "#  [ 0.34880346  0.34685454 -0.38074815]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "r_hdagextra=torch.tensor([-0.38711396,0.34880346])\n",
    "python_hdagextra=h_extrapolated(thetas_python, targets, min, max)\n",
    "\n",
    "print(f'h_dag_extra: {python_hdagextra}')\n",
    "print(f'differnece r python : {r_hdagextra-python_hdagextra}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h_dag_dash_extrapolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_dag_dash_extra: tensor([0.68188578, 0.69082350])\n",
      "differnece r python : tensor([-5.96046448e-08,  0.00000000e+00])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def h_dash_extrapolated(thetas: torch.Tensor, targets: torch.Tensor, k_min: float, k_max: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extrapolated version of h_dag_dash for out-of-domain values.\n",
    "    \n",
    "    Args:\n",
    "        t_targetsi: shape (n,)\n",
    "        thetas: shape (n, b)\n",
    "        k_min: float (not tracked by autograd)\n",
    "        k_max: float (not tracked by autograd)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: shape (n,)\n",
    "    \"\"\"\n",
    "    # Constants\n",
    "    L_START = 0.0001\n",
    "    R_START = 1.0-L_START\n",
    "\n",
    "    # Detach constants from graph\n",
    "    L_tensor = torch.tensor(L_START, dtype=targets.dtype, device=targets.device)\n",
    "    R_tensor = torch.tensor(R_START, dtype=targets.dtype, device=targets.device)\n",
    "\n",
    "    # Scale input\n",
    "    t_scaled = (targets - k_min) / (k_max - k_min)\n",
    "    t_exp = t_scaled.unsqueeze(-1)  # shape (n, 1)\n",
    "\n",
    "    # Left extrapolation: constant slope at L_START\n",
    "    slope0 = h_dag_dash(L_tensor.expand_as(targets), thetas).unsqueeze(-1)  # (n, 1)\n",
    "    mask0 = t_exp < L_tensor\n",
    "    h_dash = torch.where(mask0, slope0, t_exp)  # placeholder init\n",
    "\n",
    "    # Right extrapolation: constant slope at R_START\n",
    "    slope1 = h_dag_dash(R_tensor.expand_as(targets), thetas).unsqueeze(-1)  # (n, 1)\n",
    "    mask1 = t_exp > R_tensor\n",
    "    h_dash = torch.where(mask1, slope1, h_dash)\n",
    "\n",
    "    # In-domain interpolation\n",
    "    mask_mid = (t_exp >= L_tensor) & (t_exp <= R_tensor)\n",
    "    h_center = h_dag_dash(t_scaled, thetas).unsqueeze(-1)  # (n, 1)\n",
    "    h_dash = torch.where(mask_mid, h_center, h_dash)\n",
    "\n",
    "    return h_dash.squeeze(-1)  # shape (n,)\n",
    "\n",
    "\n",
    "# h_dag_dash_extra(t_i[,cont_dims, drop=FALSE],\n",
    "#                      theta[,cont_dims,1:len_theta,drop=FALSE],\n",
    "#                      k_min[cont_dims],\n",
    "#                      k_max[cont_dims])\n",
    "# tf.Tensor(\n",
    "# [[0.6818857  0.7085784  0.7162883 ]\n",
    "#  [0.6908235  0.66555375 0.6818333 ]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "r_hdag_dash_extra=torch.tensor([0.6818857 ,0.6908235])\n",
    "python_hdag_dash_extra=h_dash_extrapolated(thetas_python, targets, min, max)\n",
    "\n",
    "print(f'h_dag_dash_extra: {python_hdag_dash_extra}')\n",
    "print(f'differnece r python : {r_hdag_dash_extra-python_hdag_dash_extra}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Works to E-5 until here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wrap all in loss fct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skeleton for loss analog to R CODE  all the \n",
    "\n",
    "def contram_nll(outputs, targets, min_max):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        outputs: dict with keys 'int_out' and 'shift_out'\n",
    "        targets: shape (n,)\n",
    "        min_max: tuple of two floats or tensors (min, max)\n",
    "    Returns:\n",
    "        scalar NLL\n",
    "    \"\"\"\n",
    "    # Ensure min and max are not part of graph\n",
    "    min_val = torch.tensor(min_max[0], dtype=targets.dtype, device=targets.device)\n",
    "    max_val = torch.tensor(min_max[1], dtype=targets.dtype, device=targets.device)\n",
    "\n",
    "    thetas_tilde = outputs['int_out']  # shape (n, b)\n",
    "    thetas = transform_intercepts_continous(thetas_tilde)\n",
    "\n",
    "    Shifts = outputs['shift_out']  # shape (n,)\n",
    "    \n",
    "\n",
    "    # Compute h\n",
    "    h_I = h_extrapolated(thetas, targets, min_val, max_val)  # shape (n,)\n",
    "    h = h_I + torch.sum(Shifts)  # shape (n,)\n",
    "\n",
    "    # Latent logistic density log-prob\n",
    "    log_latent_density = -h - 2 * torch.nn.functional.softplus(-h)  # shape (n,)\n",
    "\n",
    "    # Derivative term (log |h'| - log(scale))\n",
    "    h_dash = h_dash_extrapolated(thetas, targets, min_val, max_val)  # shape (n,)\n",
    "    log_hdash = torch.log(torch.abs(h_dash)) - torch.log(max_val - min_val)  # shape (n,)\n",
    "\n",
    "    # Final NLL\n",
    "    nll = -torch.mean(log_latent_density + log_hdash)\n",
    "\n",
    "    return nll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test los step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train$df_orig\n",
    "# tf.Tensor(\n",
    "# [[ 0.38709584 -0.18872575  3.9523134 ]\n",
    "#  [ 0.7616431  -0.11664478 -1.4364368 ]], shape=(2, 3), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1320659/1406563847.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  min_val = torch.tensor(min_max[0], dtype=targets.dtype, device=targets.device)\n",
      "/tmp/ipykernel_1320659/1406563847.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_val = torch.tensor(min_max[1], dtype=targets.dtype, device=targets.device)\n"
     ]
    }
   ],
   "source": [
    "int_out=theta_tilde_r\n",
    "shifts_out=torch.tensor([[0,  0 ]])\n",
    "outputs={'int_out':int_out,'shift_out':shifts_out}\n",
    "\n",
    "\n",
    "targets=torch.tensor([ 0.38709584, 0.7616431 ])\n",
    "min=0.4058232\n",
    "max=0.74291575\n",
    "min_max=torch.Tensor([min, max])\n",
    "\n",
    "\n",
    "# contram_nll(outputs, targets, min_max)\n",
    "min_val = torch.tensor(min_max[0], dtype=targets.dtype, device=targets.device)\n",
    "max_val = torch.tensor(min_max[1], dtype=targets.dtype, device=targets.device)\n",
    "\n",
    "Shifts = outputs['shift_out']\n",
    "\n",
    "thetas_tilde = outputs['int_out'] \n",
    "thetas = transform_intercepts_continous(thetas_tilde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_dag_extra: tensor([-0.38710955,  0.34880140])\n",
      "differnece r python : tensor([-4.41074371e-06,  2.05636024e-06])\n"
     ]
    }
   ],
   "source": [
    "r_hdagextra=torch.tensor([-0.38711396,0.34880346])\n",
    "h_I=h_extrapolated(thetas_python, targets, min, max)\n",
    "\n",
    "print(f'h_dag_extra: {h_I}')\n",
    "print(f'differnece r python : {r_hdagextra-h_I}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.38710955,  0.34880140])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = h_I + torch.sum(Shifts)\n",
    "h   # same becasue shifts are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_latent_density: tensor([-1.42352617, -1.41655695])\n",
      "differnece r python : tensor([8.34465027e-07, 3.57627869e-07])\n"
     ]
    }
   ],
   "source": [
    "# tf.Tensor(\n",
    "# [[-1.423527  -1.4178157 -1.4313766]\n",
    "#  [-1.4165573 -1.4279113 -1.411321 ]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "\n",
    "r_log_latent_density=torch.tensor([-1.423527 ,-1.4165573])\n",
    "\n",
    "\n",
    "log_latent_density = -h - 2 * torch.nn.functional.softplus(-h)  # shape (n,)\n",
    "log_latent_density\n",
    "\n",
    "\n",
    "print(f'log_latent_density: {log_latent_density}')\n",
    "print(f'differnece r python : {log_latent_density-r_log_latent_density}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_dash: tensor([0.68188578, 0.69082350])\n",
      "differnece r python : tensor([-0.00577646, -0.01432210])\n"
     ]
    }
   ],
   "source": [
    "# tf.Tensor(\n",
    "# [[0.68766224 0.63910955 0.6786249 ]\n",
    "#  [0.7051456  0.7142741  0.70591867]], shape=(2, 3), dtype=float32)\n",
    "r_h_dash=torch.tensor([0.68766224,0.7051456])\n",
    "r_hdag_dash_extra=torch.tensor([0.6818857 ,0.6908235])\n",
    "\n",
    "\n",
    "\n",
    "h_dash = h_dash_extrapolated(thetas, targets, min_val, max_val)  # shape (n,)\n",
    "\n",
    "print(f'h_dash: {h_dash}')\n",
    "print(f'differnece r python : {h_dash-r_h_dash}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_dash: tensor([0.70450473, 0.71752691])\n",
      "differnece r python : tensor([-1.19209290e-07,  0.00000000e+00])\n"
     ]
    }
   ],
   "source": [
    "# tf.Tensor(\n",
    "# [[ 0.7045046  2.390831  -1.9126254]\n",
    "#  [ 0.7175269  2.3281896 -1.9619229]], shape=(2, 3), dtype=float32)\n",
    "r_log_hdash=torch.tensor([0.7045046 ,0.7175269])\n",
    "\n",
    "\n",
    "log_hdash = torch.log(torch.abs(h_dash)) - torch.log(max_val - min_val)  # shape (n,)\n",
    "log_hdash\n",
    "\n",
    "print(f'h_dash: {log_hdash}')\n",
    "print(f'differnece r python : {r_log_hdash-log_hdash}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLL = NLL - tf$reduce_mean(log_latent_density + log_hdash)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
