{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R -> to_Theta, Python transform_intercepts_continpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def transform_intercepts_continous(theta_tilde:torch.Tensor) -> torch.Tensor:\n",
    "    \n",
    "    \"\"\"\n",
    "    Transforms the unordered theta_tilde to ordered theta values for the bernstein polynomial\n",
    "    E.G: \n",
    "    theta_1 = theta_tilde_1\n",
    "    theta_2 = theta_tilde_1 + exp(theta_tilde_2)\n",
    "    ..\n",
    "    :param theta_tilde: The unordered theta_tilde values\n",
    "    :return: The ordered theta values\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the shift based on the last dimension size\n",
    "    last_dim_size = theta_tilde.shape[-1]\n",
    "    shift = torch.log(torch.tensor(2.0)) * last_dim_size / 2\n",
    "\n",
    "    # Get the width values by applying softplus from the second position onward\n",
    "    widths = torch.nn.functional.softplus(theta_tilde[..., 1:])\n",
    "\n",
    "    # Concatenate the first value (raw) with the softplus-transformed widths\n",
    "    widths = torch.cat([theta_tilde[..., [0]], widths], dim=-1)\n",
    "\n",
    "    # Return the cumulative sum minus the shift\n",
    "    return torch.cumsum(widths, dim=-1) - shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same tensor as in R code 2 samples\n",
    "\n",
    "# train  data 2 samples 3 vars\n",
    "\n",
    "# > t_i\n",
    "# tf.Tensor(\n",
    "# [[ 0.28106958 -0.33505845  2.945311  ]\n",
    "#  [ 0.19878516 -0.23470472  0.7153738 ]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "# h_params\n",
    "# tf.Tensor(\n",
    "# [[[ 0.          0.          0.01346988 -0.03040301 -0.0009475\n",
    "#     0.03353934  0.03301353 -0.04421922  0.02690415  0.0534528\n",
    "#    -0.07279044  0.06620763 -0.00113248  0.09650994  0.02934606\n",
    "#     0.00773158 -0.10510534 -0.01168713 -0.03555111  0.019867\n",
    "#    -0.09727976 -0.0474655 ]\n",
    "#   [ 0.         -0.00938768  0.14461783 -0.00913084 -0.02329957\n",
    "#    -0.01995979 -0.07978858  0.08354648  0.07621053  0.03968899\n",
    "#     0.04010997 -0.07187632 -0.02174324 -0.02492543  0.04793847\n",
    "#     0.03261854 -0.01283982 -0.01570366 -0.01853073  0.04250432\n",
    "#     0.0073978   0.03714988]\n",
    "#   [-0.00955794  0.00252406  0.00787929 -0.05027757 -0.01628618\n",
    "#    -0.01388468  0.03728567 -0.02157371  0.00681018 -0.01150865\n",
    "#    -0.08478781 -0.03085445  0.03231757  0.05626081 -0.0752314\n",
    "#    -0.01583911  0.06897556 -0.06667089 -0.04729394 -0.02368151\n",
    "#     0.03042696 -0.00568471]]\n",
    "\n",
    "#  [[ 0.          0.          0.01346988 -0.03040301 -0.0009475\n",
    "#     0.03353934  0.03301353 -0.04421922  0.02690415  0.0534528\n",
    "#    -0.07279044  0.06620763 -0.00113248  0.09650994  0.02934606\n",
    "#     0.00773158 -0.10510534 -0.01168713 -0.03555111  0.019867\n",
    "#    -0.09727976 -0.0474655 ]\n",
    "#   [ 0.         -0.00663939  0.14461783 -0.00913084 -0.02329957\n",
    "#    -0.01995979 -0.07978858  0.08354648  0.07621053  0.03968899\n",
    "#     0.04010997 -0.07187632 -0.02174324 -0.02492543  0.04793847\n",
    "#     0.03261854 -0.01283982 -0.01570366 -0.01853073  0.04250432\n",
    "#     0.0073978   0.03714988]\n",
    "#   [-0.00955794  0.00178513  0.00787929 -0.05027757 -0.01628618\n",
    "#    -0.01388468  0.03728567 -0.02157371  0.00681018 -0.01150865\n",
    "#    -0.08478781 -0.03085445  0.03231757  0.05626081 -0.0752314\n",
    "#    -0.01583911  0.06897556 -0.06667089 -0.04729394 -0.02368151\n",
    "#     0.03042696 -0.00568471]]], shape=(2, 3, 22), dtype=float32)\n",
    "\n",
    "# kmin:\n",
    "# tf.Tensor([ 0.12124275 -0.80636215 -4.9503837 ], shape=(3), dtype=float32)\n",
    "\n",
    "# kmax:\n",
    "# tf.Tensor([0.79330695 0.41867393 4.934168  ], shape=(3), dtype=float32)\n",
    "\n",
    "\n",
    "#theat_thilde  2 samples 3 vars 20 features\n",
    "# tf.Tensor(\n",
    "# [[[ 0.01346988 -0.03040301 -0.0009475   0.03353934  0.03301353\n",
    "#    -0.04421922  0.02690415  0.0534528  -0.07279044  0.06620763\n",
    "#    -0.00113248  0.09650994  0.02934606  0.00773158 -0.10510534\n",
    "#    -0.01168713 -0.03555111  0.019867   -0.09727976 -0.0474655 ]\n",
    "#   [ 0.14461783 -0.00913084 -0.02329957 -0.01995979 -0.07978858\n",
    "#     0.08354648  0.07621053  0.03968899  0.04010997 -0.07187632\n",
    "#    -0.02174324 -0.02492543  0.04793847  0.03261854 -0.01283982\n",
    "#    -0.01570366 -0.01853073  0.04250432  0.0073978   0.03714988]\n",
    "#   [ 0.00787929 -0.05027757 -0.01628618 -0.01388468  0.03728567\n",
    "#    -0.02157371  0.00681018 -0.01150865 -0.08478781 -0.03085445\n",
    "#     0.03231757  0.05626081 -0.0752314  -0.01583911  0.06897556\n",
    "#    -0.06667089 -0.04729394 -0.02368151  0.03042696 -0.00568471]]\n",
    "\n",
    "#  [[ 0.01346988 -0.03040301 -0.0009475   0.03353934  0.03301353\n",
    "#    -0.04421922  0.02690415  0.0534528  -0.07279044  0.06620763\n",
    "#    -0.00113248  0.09650994  0.02934606  0.00773158 -0.10510534\n",
    "#    -0.01168713 -0.03555111  0.019867   -0.09727976 -0.0474655 ]\n",
    "#   [ 0.14461783 -0.00913084 -0.02329957 -0.01995979 -0.07978858\n",
    "#     0.08354648  0.07621053  0.03968899  0.04010997 -0.07187632\n",
    "#    -0.02174324 -0.02492543  0.04793847  0.03261854 -0.01283982\n",
    "#    -0.01570366 -0.01853073  0.04250432  0.0073978   0.03714988]\n",
    "#   [ 0.00787929 -0.05027757 -0.01628618 -0.01388468  0.03728567\n",
    "#    -0.02157371  0.00681018 -0.01150865 -0.08478781 -0.03085445\n",
    "#     0.03231757  0.05626081 -0.0752314  -0.01583911  0.06897556\n",
    "#    -0.06667089 -0.04729394 -0.02368151  0.03042696 -0.00568471]]], shape=(2, 3, 20), dtype=float32)\n",
    "\n",
    "# only the first since this funciton accepts only 1  target\n",
    "\n",
    "\n",
    "#output h_dag extra\n",
    "\n",
    "# > h_I\n",
    "# tf.Tensor(\n",
    "# [[-0.18897358 -0.08517991  0.1750973 ]\n",
    "#  [-0.2702095  -0.0309604   0.02766471]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "\n",
    "### h(0)   and dh(0) \n",
    "\n",
    "# > h_dag(R_START, theta)\n",
    "# tf.Tensor(\n",
    "# [[0.31085795 0.32204473 0.30668354]\n",
    "#  [0.31085795 0.32204473 0.30668354]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "# > h_dag_dash(R_START, theta)\n",
    "# tf.Tensor(\n",
    "# [[0.66965276 0.7118679  0.69034123]\n",
    "#  [0.66965276 0.7118679  0.69034123]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "#### right h(1)   and dh(1) \n",
    "\n",
    "# > h_dag(L_START, theta)\n",
    "# tf.Tensor(\n",
    "# [[-0.34583557 -0.33927712 -0.346116  ]\n",
    "#  [-0.34583557 -0.33927712 -0.346116  ]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "# > h_dag_dash(L_START, theta)\n",
    "# tf.Tensor(\n",
    "# [[0.6780876  0.68857914 0.66835433]\n",
    "#  [0.6780876  0.68857914 0.66835433]], shape=(2, 3), dtype=float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.01346988, -0.03040301, -0.00094750,  0.03353934,  0.03301353,\n",
       "         -0.04421922,  0.02690415,  0.05345280, -0.07279044,  0.06620763,\n",
       "         -0.00113248,  0.09650994,  0.02934606,  0.00773158, -0.10510534,\n",
       "         -0.01168713, -0.03555111,  0.01986700, -0.09727976, -0.04746550],\n",
       "        [ 0.01346988, -0.03040301, -0.00094750,  0.03353934,  0.03301353,\n",
       "         -0.04421922,  0.02690415,  0.05345280, -0.07279044,  0.06620763,\n",
       "         -0.00113248,  0.09650994,  0.02934606,  0.00773158, -0.10510534,\n",
       "         -0.01168713, -0.03555111,  0.01986700, -0.09727976, -0.04746550]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# thetas from Model \n",
    "\n",
    "r_vgl=torch.Tensor([[ 0.01346988, -0.03040301, -0.0009475,   0.03353934,  0.03301353, \n",
    "              -0.04421922,  0.02690415,  0.0534528,  -0.07279044,  0.06620763,\n",
    "   -0.00113248,  0.09650994,  0.02934606,  0.00773158, -0.10510534,\n",
    "   -0.01168713, -0.03555111,  0.019867,   -0.09727976, -0.0474655 ],\n",
    "                    \n",
    "    [ 0.01346988, -0.03040301, -0.0009475,   0.03353934,  0.03301353,\n",
    "   -0.04421922,  0.02690415,  0.0534528,  -0.07279044,  0.06620763,\n",
    "   -0.00113248,  0.09650994,  0.02934606,  0.00773158, -0.10510534,\n",
    "   -0.01168713, -0.03555111,  0.019867,   -0.09727976, -0.0474655 ]])\n",
    "r_vgl   # same data see the to samples are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.91800213, -6.23994064, -5.54726696, -4.83720970, -4.12741947,\n",
       "         -3.45613742, -2.74944782, -2.02921724, -1.37180281, -0.64500427,\n",
       "          0.04757690,  0.79014301,  1.49807072,  2.19509125,  2.83706570,\n",
       "          3.52438641,  4.19991684,  4.90304661,  5.54873657,  6.21843243],\n",
       "        [-6.91800213, -6.23994064, -5.54726696, -4.83720970, -4.12741947,\n",
       "         -3.45613742, -2.74944782, -2.02921724, -1.37180281, -0.64500427,\n",
       "          0.04757690,  0.79014301,  1.49807072,  2.19509125,  2.83706570,\n",
       "          3.52438641,  4.19991684,  4.90304661,  5.54873657,  6.21843243]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas=transform_intercepts_continous(r_vgl)\n",
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# theta after to_theta()\n",
    "# tf.Tensor(\n",
    "# [[[-6.918002   -6.2399406  -5.547267   -4.8372097  -4.1274195\n",
    "#    -3.4561374  -2.7494478  -2.0292172  -1.3718033  -0.6450043\n",
    "#     0.0475769   0.790143    1.4980707   2.1950912   2.8370657\n",
    "#     3.5243864   4.199916    4.9030457   5.5487356   6.2184315 ]\n",
    "#   [-6.786854   -6.098262   -5.4166965  -4.7334795  -4.079431\n",
    "#    -3.3436384  -2.61166    -1.8984714  -1.1850681  -0.5272136\n",
    "#     0.15512085  0.83588314  1.5532866   2.2628756   2.949623\n",
    "#     3.6349497   4.3188744   5.0334997   5.7303524   6.4422474 ]\n",
    "#   [-6.9235926  -6.255268   -5.570231   -4.8840017  -4.172038\n",
    "#    -3.4896195  -2.7930613  -2.1056519  -1.454      -0.7761612\n",
    "#    -0.06672478  0.65494823  1.3111868   1.9964457   2.7246752\n",
    "#     3.3850422   4.054822    4.7361984   5.4446745   6.134983  ]]\n",
    "\n",
    "#  [[-6.918002   -6.2399406  -5.547267   -4.8372097  -4.1274195\n",
    "#    -3.4561374  -2.7494478  -2.0292172  -1.3718033  -0.6450043\n",
    "#     0.0475769   0.790143    1.4980707   2.1950912   2.8370657\n",
    "#     3.5243864   4.199916    4.9030457   5.5487356   6.2184315 ]\n",
    "#   [-6.786854   -6.098262   -5.4166965  -4.7334795  -4.079431\n",
    "#    -3.3436384  -2.61166    -1.8984714  -1.1850681  -0.5272136\n",
    "#     0.15512085  0.83588314  1.5532866   2.2628756   2.949623\n",
    "#     3.6349497   4.3188744   5.0334997   5.7303524   6.4422474 ]\n",
    "#   [-6.9235926  -6.255268   -5.570231   -4.8840017  -4.172038\n",
    "#    -3.4896195  -2.7930613  -2.1056519  -1.454      -0.7761612\n",
    "#    -0.06672478  0.65494823  1.3111868   1.9964457   2.7246752\n",
    "#     3.3850422   4.054822    4.7361984   5.4446745   6.134983  ]]], shape=(2, 3, 20), dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- works Same output as R CODE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## r -> h_dag extra python  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same tensor as in R code 2 samples\n",
    "\n",
    "# train  data 2 samples 3 vars\n",
    "# tf.Tensor(\n",
    "# [[ 0.28106958 -0.33505845  2.945311  ]\n",
    "#  [ 0.19878516 -0.23470472  0.7153738 ]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "# h_params\n",
    "# tf.Tensor(\n",
    "# [[[ 0.          0.          0.01346988 -0.03040301 -0.0009475\n",
    "#     0.03353934  0.03301353 -0.04421922  0.02690415  0.0534528\n",
    "#    -0.07279044  0.06620763 -0.00113248  0.09650994  0.02934606\n",
    "#     0.00773158 -0.10510534 -0.01168713 -0.03555111  0.019867\n",
    "#    -0.09727976 -0.0474655 ]\n",
    "#   [ 0.         -0.00938768  0.14461783 -0.00913084 -0.02329957\n",
    "#    -0.01995979 -0.07978858  0.08354648  0.07621053  0.03968899\n",
    "#     0.04010997 -0.07187632 -0.02174324 -0.02492543  0.04793847\n",
    "#     0.03261854 -0.01283982 -0.01570366 -0.01853073  0.04250432\n",
    "#     0.0073978   0.03714988]\n",
    "#   [-0.00955794  0.00252406  0.00787929 -0.05027757 -0.01628618\n",
    "#    -0.01388468  0.03728567 -0.02157371  0.00681018 -0.01150865\n",
    "#    -0.08478781 -0.03085445  0.03231757  0.05626081 -0.0752314\n",
    "#    -0.01583911  0.06897556 -0.06667089 -0.04729394 -0.02368151\n",
    "#     0.03042696 -0.00568471]]\n",
    "\n",
    "#  [[ 0.          0.          0.01346988 -0.03040301 -0.0009475\n",
    "#     0.03353934  0.03301353 -0.04421922  0.02690415  0.0534528\n",
    "#    -0.07279044  0.06620763 -0.00113248  0.09650994  0.02934606\n",
    "#     0.00773158 -0.10510534 -0.01168713 -0.03555111  0.019867\n",
    "#    -0.09727976 -0.0474655 ]\n",
    "#   [ 0.         -0.00663939  0.14461783 -0.00913084 -0.02329957\n",
    "#    -0.01995979 -0.07978858  0.08354648  0.07621053  0.03968899\n",
    "#     0.04010997 -0.07187632 -0.02174324 -0.02492543  0.04793847\n",
    "#     0.03261854 -0.01283982 -0.01570366 -0.01853073  0.04250432\n",
    "#     0.0073978   0.03714988]\n",
    "#   [-0.00955794  0.00178513  0.00787929 -0.05027757 -0.01628618\n",
    "#    -0.01388468  0.03728567 -0.02157371  0.00681018 -0.01150865\n",
    "#    -0.08478781 -0.03085445  0.03231757  0.05626081 -0.0752314\n",
    "#    -0.01583911  0.06897556 -0.06667089 -0.04729394 -0.02368151\n",
    "#     0.03042696 -0.00568471]]], shape=(2, 3, 22), dtype=float32)\n",
    "\n",
    "# kmin:\n",
    "# tf.Tensor([ 0.12124275 -0.80636215 -4.9503837 ], shape=(3), dtype=float32)\n",
    "\n",
    "# kmax:\n",
    "# tf.Tensor([0.79330695 0.41867393 4.934168  ], shape=(3), dtype=float32)\n",
    "\n",
    "\n",
    "#theat_thilde  2 samples 3 vars 20 features\n",
    "# tf.Tensor(\n",
    "# [[[ 0.01346988 -0.03040301 -0.0009475   0.03353934  0.03301353\n",
    "#    -0.04421922  0.02690415  0.0534528  -0.07279044  0.06620763\n",
    "#    -0.00113248  0.09650994  0.02934606  0.00773158 -0.10510534\n",
    "#    -0.01168713 -0.03555111  0.019867   -0.09727976 -0.0474655 ]\n",
    "#   [ 0.14461783 -0.00913084 -0.02329957 -0.01995979 -0.07978858\n",
    "#     0.08354648  0.07621053  0.03968899  0.04010997 -0.07187632\n",
    "#    -0.02174324 -0.02492543  0.04793847  0.03261854 -0.01283982\n",
    "#    -0.01570366 -0.01853073  0.04250432  0.0073978   0.03714988]\n",
    "#   [ 0.00787929 -0.05027757 -0.01628618 -0.01388468  0.03728567\n",
    "#    -0.02157371  0.00681018 -0.01150865 -0.08478781 -0.03085445\n",
    "#     0.03231757  0.05626081 -0.0752314  -0.01583911  0.06897556\n",
    "#    -0.06667089 -0.04729394 -0.02368151  0.03042696 -0.00568471]]\n",
    "\n",
    "#  [[ 0.01346988 -0.03040301 -0.0009475   0.03353934  0.03301353\n",
    "#    -0.04421922  0.02690415  0.0534528  -0.07279044  0.06620763\n",
    "#    -0.00113248  0.09650994  0.02934606  0.00773158 -0.10510534\n",
    "#    -0.01168713 -0.03555111  0.019867   -0.09727976 -0.0474655 ]\n",
    "#   [ 0.14461783 -0.00913084 -0.02329957 -0.01995979 -0.07978858\n",
    "#     0.08354648  0.07621053  0.03968899  0.04010997 -0.07187632\n",
    "#    -0.02174324 -0.02492543  0.04793847  0.03261854 -0.01283982\n",
    "#    -0.01570366 -0.01853073  0.04250432  0.0073978   0.03714988]\n",
    "#   [ 0.00787929 -0.05027757 -0.01628618 -0.01388468  0.03728567\n",
    "#    -0.02157371  0.00681018 -0.01150865 -0.08478781 -0.03085445\n",
    "#     0.03231757  0.05626081 -0.0752314  -0.01583911  0.06897556\n",
    "#    -0.06667089 -0.04729394 -0.02368151  0.03042696 -0.00568471]]], shape=(2, 3, 20), dtype=float32)\n",
    "\n",
    "# only the first since this funciton accepts only 1  target\n",
    "\n",
    "\n",
    "#output h_dag extra\n",
    "\n",
    "# > h_I\n",
    "# tf.Tensor(\n",
    "# [[-0.18897358 -0.08517991  0.1750973 ]\n",
    "#  [-0.2702095  -0.0309604   0.02766471]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "\n",
    "### h(0)   and dh(0) \n",
    "\n",
    "# > h_dag(R_START, theta)\n",
    "# tf.Tensor(\n",
    "# [[0.31085795 0.32204473 0.30668354]\n",
    "#  [0.31085795 0.32204473 0.30668354]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "# > h_dag_dash(R_START, theta)\n",
    "# tf.Tensor(\n",
    "# [[0.66965276 0.7118679  0.69034123]\n",
    "#  [0.66965276 0.7118679  0.69034123]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "#### right h(1)   and dh(1) \n",
    "\n",
    "# > h_dag(L_START, theta)\n",
    "# tf.Tensor(\n",
    "# [[-0.34583557 -0.33927712 -0.346116  ]\n",
    "#  [-0.34583557 -0.33927712 -0.346116  ]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "# > h_dag_dash(L_START, theta)\n",
    "# tf.Tensor(\n",
    "# [[0.6780876  0.68857914 0.66835433]\n",
    "#  [0.6780876  0.68857914 0.66835433]], shape=(2, 3), dtype=float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h_dag_extra calls hdag  and hdag dash which call bernstein poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_START=0.0001\n",
    "R_START=1-L_START\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernstein basis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernstein_basis(tensor, M):\n",
    "    \"\"\"\n",
    "    Compute the Bernstein basis polynomials for a given input tensor.\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Input tensor of shape (n_samples).\n",
    "        M (int): Degree of the Bernstein polynomial.\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape (B, Nodes, M+1) with the Bernstein basis.\n",
    "    \"\"\"\n",
    "    tensor = torch.as_tensor(tensor)\n",
    "    dtype = tensor.dtype\n",
    "    M = torch.tensor(M, dtype=dtype, device=tensor.device)\n",
    "\n",
    "    # Expand dims to allow broadcasting\n",
    "    tensor_expanded = tensor.unsqueeze(-1)  # shape (B, Nodes, 1)\n",
    "\n",
    "    # Clip values to avoid log(0)\n",
    "    eps = torch.finfo(dtype).eps\n",
    "    tensor_expanded = torch.clamp(tensor_expanded, min=eps, max=1 - eps)\n",
    "\n",
    "    k_values = torch.arange(M + 1, dtype=dtype, device=tensor.device)  # shape (M+1,)\n",
    "    \n",
    "    # Log binomial coefficient: log(M choose k)\n",
    "    log_binomial_coeff = (\n",
    "        torch.lgamma(M + 1) \n",
    "        - torch.lgamma(k_values + 1) \n",
    "        - torch.lgamma(M - k_values + 1)\n",
    "    )\n",
    "\n",
    "    # Log powers\n",
    "    log_powers = (\n",
    "        k_values * torch.log(tensor_expanded)\n",
    "        + (M - k_values) * torch.log(1 - tensor_expanded)\n",
    "    )\n",
    "\n",
    "    # Bernstein basis in log space\n",
    "    log_bernstein = log_binomial_coeff + log_powers  # shape (B, Nodes, M+1)\n",
    "\n",
    "    return torch.exp(log_bernstein)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bernstein_basis_1d():\n",
    "    samples = torch.tensor([0.2, 0.4, 0.4], dtype=torch.float32)\n",
    "    M = 4\n",
    "    output = bernstein_basis(samples, M)\n",
    "\n",
    "    # Should return shape (N, M+1)\n",
    "    assert output.shape == (len(samples), M + 1), f\"Expected shape {(len(samples), M + 1)}, got {output.shape}\"\n",
    "\n",
    "    # All values should be >= 0\n",
    "    assert torch.all(output >= 0), \"All Bernstein basis values should be non-negative\"\n",
    "\n",
    "    # Each row should sum to ~1\n",
    "    row_sums = output.sum(dim=1)\n",
    "    assert torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-6), f\"Sums were not 1: {row_sums}\"\n",
    "\n",
    "test_bernstein_basis_1d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hdag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_dag: tensor([-0.34583560, -0.34583560])\n",
      "h_dag: tensor([0.31085801, 0.31085801])\n"
     ]
    }
   ],
   "source": [
    "def h_dag(targets: torch.Tensor, thetas: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        targets: shape (n,)\n",
    "        thetas: shape (n, b)\n",
    "    Returns:\n",
    "        Tensor: shape (n,)\n",
    "    \"\"\"\n",
    "    _, b = thetas.shape\n",
    "    B = bernstein_basis(targets, b - 1)  # shape (n, b)\n",
    "    return torch.mean(B * thetas, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "print(f'h_dag: {h_dag(L_START, thetas)}')\n",
    "\n",
    "# > h_dag(L_START, theta)\n",
    "# tf.Tensor(\n",
    "# [[-0.34583557 -0.33927712 -0.346116  ]\n",
    "#  [-0.34583557 -0.33927712 -0.346116  ]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "print(f'h_dag: {h_dag(R_START, thetas)}')\n",
    "\n",
    "# > h_dag(R_START, theta)\n",
    "# tf.Tensor(\n",
    "# [[0.31085795 0.32204473 0.30668354]\n",
    "#  [0.31085795 0.32204473 0.30668354]], shape=(2, 3), dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  h_dag_dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_dag_dash: tensor([0.67808759, 0.67808759])\n",
      "h_dag_dash: tensor([0.66965276, 0.66965276])\n"
     ]
    }
   ],
   "source": [
    "def h_dag_dash(targets: torch.Tensor, thetas: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        targets: shape (n,)\n",
    "        thetas: shape (n, b)\n",
    "    Returns:\n",
    "        Tensor: shape (n,)\n",
    "    \"\"\"\n",
    "    _, b = thetas.shape\n",
    "    dtheta = thetas[:, 1:] - thetas[:, :-1]         # shape (n, b-1)\n",
    "    B_dash = bernstein_basis(targets, b - 2)        # shape (n, b-1)\n",
    "    return torch.sum(B_dash * dtheta, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'h_dag_dash: {h_dag_dash(L_START, thetas)}')\n",
    "\n",
    "# > h_dag_dash(L_START, theta)\n",
    "# tf.Tensor(\n",
    "# [[0.6780876  0.68857914 0.66835433]\n",
    "#  [0.6780876  0.68857914 0.66835433]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "\n",
    "\n",
    "print(f'h_dag_dash: {h_dag_dash(R_START, thetas)}')\n",
    "\n",
    "# > h_dag_dash(R_START, theta)\n",
    "# tf.Tensor(\n",
    "# [[0.66965276 0.7118679  0.69034123]\n",
    "#  [0.66965276 0.7118679  0.69034123]], shape=(2, 3), dtype=float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  h_extarpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmin:\n",
    "# tf.Tensor([ 0.12124275 -0.80636215 -4.9503837 ], shape=(3), dtype=float32)\n",
    "\n",
    "# kmax:\n",
    "# tf.Tensor([0.79330695 0.41867393 4.934168  ], shape=(3), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=torch.tensor([ 0.28106958,0.19878516 ])\n",
    "\n",
    "# tf.Tensor(\n",
    "# [[ 0.28106958 -0.33505845  2.945311  ]\n",
    "#  [ 0.19878516 -0.23470472  0.7153738 ]], shape=(2, 3), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_extrapolated: tensor([-0.18897405, -0.27021015])\n"
     ]
    }
   ],
   "source": [
    "def h_extrapolated(thetas: torch.Tensor, targets: torch.Tensor, k_min: float, k_max: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        thetas: shape (n, b)\n",
    "        targets: shape (n,)\n",
    "        k_min: float, lower bound of scaling (not tracked in graph)\n",
    "        k_max: float, upper bound of scaling (not tracked in graph)\n",
    "    Returns:\n",
    "        Tensor of shape (n,)\n",
    "    \"\"\"\n",
    "    # Constants (not part of the graph)\n",
    "    L_START = 0.0\n",
    "    R_START = 1.0\n",
    "\n",
    "    # Detach constants from graph\n",
    "    L_tensor = torch.tensor(L_START, dtype=targets.dtype, device=targets.device)\n",
    "    R_tensor = torch.tensor(R_START, dtype=targets.dtype, device=targets.device)\n",
    "\n",
    "    # Scale targets\n",
    "    t_i = (targets - k_min) / (k_max - k_min)  # shape (n,)\n",
    "    t_i_exp = t_i.unsqueeze(-1)  # shape (n, 1)\n",
    "\n",
    "    # Extrapolation at left (t_i < 0)\n",
    "    b0 = h_dag(L_tensor.expand_as(targets), thetas).unsqueeze(-1)     # (n, 1)\n",
    "    slope0 = h_dag_dash(L_tensor.expand_as(targets), thetas).unsqueeze(-1)  # (n, 1)\n",
    "    h_left = slope0 * (t_i_exp - L_tensor) + b0\n",
    "\n",
    "    # Start with placeholder\n",
    "    h = h_left.clone()\n",
    "\n",
    "    # Mask for left extrapolation\n",
    "    mask0 = t_i_exp < L_tensor\n",
    "    h = torch.where(mask0, h_left, t_i_exp)  # placeholder fill\n",
    "\n",
    "    # Extrapolation at right (t_i > 1)\n",
    "    b1 = h_dag(R_tensor.expand_as(targets), thetas).unsqueeze(-1)\n",
    "    slope1 = h_dag_dash(R_tensor.expand_as(targets), thetas).unsqueeze(-1)\n",
    "    h_right = slope1 * (t_i_exp - R_tensor) + b1\n",
    "\n",
    "    mask1 = t_i_exp > R_tensor\n",
    "    h = torch.where(mask1, h_right, h)\n",
    "\n",
    "    # In-domain: t_i ∈ [0,1]\n",
    "    mask_mid = (t_i_exp >= L_tensor) & (t_i_exp <= R_tensor)\n",
    "    h_center = h_dag(t_i, thetas).unsqueeze(-1)\n",
    "    h = torch.where(mask_mid, h_center, h)\n",
    "\n",
    "    return h.squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "#output h_dag extra\n",
    "\n",
    "# > h_I\n",
    "# tf.Tensor(\n",
    "# [[-0.18897358 -0.08517991  0.1750973 ]\n",
    "#  [-0.2702095  -0.0309604   0.02766471]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'h_extrapolated: {h_extrapolated(thetas, targets, 0.12124275, 0.79330695)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h_dag_dash_extrapolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_dag_dash_extra: tensor([0.69799966, 0.69808316])\n"
     ]
    }
   ],
   "source": [
    "def h_dash_extrapolated(thetas: torch.Tensor, targets: torch.Tensor, k_min: float, k_max: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extrapolated version of h_dag_dash for out-of-domain values.\n",
    "    \n",
    "    Args:\n",
    "        t_targetsi: shape (n,)\n",
    "        thetas: shape (n, b)\n",
    "        k_min: float (not tracked by autograd)\n",
    "        k_max: float (not tracked by autograd)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: shape (n,)\n",
    "    \"\"\"\n",
    "    # Constants\n",
    "    L_START = 0.0001\n",
    "    R_START = 1.0-L_START\n",
    "\n",
    "    # Detach constants from graph\n",
    "    L_tensor = torch.tensor(L_START, dtype=targets.dtype, device=targets.device)\n",
    "    R_tensor = torch.tensor(R_START, dtype=targets.dtype, device=targets.device)\n",
    "\n",
    "    # Scale input\n",
    "    t_scaled = (targets - k_min) / (k_max - k_min)\n",
    "    t_exp = t_scaled.unsqueeze(-1)  # shape (n, 1)\n",
    "\n",
    "    # Left extrapolation: constant slope at L_START\n",
    "    slope0 = h_dag_dash(L_tensor.expand_as(targets), thetas).unsqueeze(-1)  # (n, 1)\n",
    "    mask0 = t_exp < L_tensor\n",
    "    h_dash = torch.where(mask0, slope0, t_exp)  # placeholder init\n",
    "\n",
    "    # Right extrapolation: constant slope at R_START\n",
    "    slope1 = h_dag_dash(R_tensor.expand_as(targets), thetas).unsqueeze(-1)  # (n, 1)\n",
    "    mask1 = t_exp > R_tensor\n",
    "    h_dash = torch.where(mask1, slope1, h_dash)\n",
    "\n",
    "    # In-domain interpolation\n",
    "    mask_mid = (t_exp >= L_tensor) & (t_exp <= R_tensor)\n",
    "    h_center = h_dag_dash(t_scaled, thetas).unsqueeze(-1)  # (n, 1)\n",
    "    h_dash = torch.where(mask_mid, h_center, h_dash)\n",
    "\n",
    "    return h_dash.squeeze(-1)  # shape (n,)\n",
    "\n",
    "\n",
    "\n",
    "print(f'h_dag_dash_extra: {h_dash_extrapolated(thetas, targets, 0.12124275, 0.79330695)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wrap all in loss fct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skeleton for loss analog to R CODE\n",
    "\n",
    "def contram_nll(outputs, targets, min_max):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        outputs: dict with keys 'int_out' and 'shift_out'\n",
    "        targets: shape (n,)\n",
    "        min_max: tuple of two floats or tensors (min, max)\n",
    "    Returns:\n",
    "        scalar NLL\n",
    "    \"\"\"\n",
    "    # Ensure min and max are not part of graph\n",
    "    min_val = torch.tensor(min_max[0], dtype=targets.dtype, device=targets.device)\n",
    "    max_val = torch.tensor(min_max[1], dtype=targets.dtype, device=targets.device)\n",
    "\n",
    "    thetas_tilde = outputs['int_out']  # shape (n, b)\n",
    "    thetas = transform_intercepts_continous(thetas_tilde)\n",
    "\n",
    "    Shifts = outputs['shift_out']  # shape (n,)\n",
    "    \n",
    "\n",
    "    # Compute h\n",
    "    h_I = h_extrapolated(thetas, targets, min_val, max_val)  # shape (n,)\n",
    "    h = h_I + torch.sum(Shifts)  # shape (n,)\n",
    "\n",
    "    # Latent logistic density log-prob\n",
    "    log_latent_density = -h - 2 * torch.nn.functional.softplus(-h)  # shape (n,)\n",
    "\n",
    "    # Derivative term (log |h'| - log(scale))\n",
    "    h_dash = h_dash_extrapolated(thetas, targets, min_val, max_val)  # shape (n,)\n",
    "    log_hdash = torch.log(torch.abs(h_dash)) - torch.log(max_val - min_val)  # shape (n,)\n",
    "\n",
    "    # Final NLL\n",
    "    nll = -torch.mean(log_latent_density + log_hdash)\n",
    "\n",
    "    return nll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets\n",
    "# tf.Tensor(\n",
    "# [[ 0.28106958 -0.33505845  2.945311  ]\n",
    "#  [ 0.19878516 -0.23470472  0.7153738 ]], shape=(2, 3), dtype=float32)\n",
    "\n",
    "#outputs from model\n",
    "#   [-0.00955794  0.00252406  0.00787929 -0.05027757 -0.01628618\n",
    "#    -0.01388468  0.03728567 -0.02157371  0.00681018 -0.01150865\n",
    "#    -0.08478781 -0.03085445  0.03231757  0.05626081 -0.0752314\n",
    "#    -0.01583911  0.06897556 -0.06667089 -0.04729394 -0.02368151\n",
    "#     0.03042696 -0.00568471]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_params\n",
    "# tf.Tensor(\n",
    "# [[[ 0.          0.          0.01346988 -0.03040301 -0.0009475\n",
    "#     0.03353934  0.03301353 -0.04421922  0.02690415  0.0534528\n",
    "#    -0.07279044  0.06620763 -0.00113248  0.09650994  0.02934606\n",
    "#     0.00773158 -0.10510534 -0.01168713 -0.03555111  0.019867\n",
    "#    -0.09727976 -0.0474655 ]\n",
    "#   [ 0.         -0.00938768  0.14461783 -0.00913084 -0.02329957\n",
    "#    -0.01995979 -0.07978858  0.08354648  0.07621053  0.03968899\n",
    "#     0.04010997 -0.07187632 -0.02174324 -0.02492543  0.04793847\n",
    "#     0.03261854 -0.01283982 -0.01570366 -0.01853073  0.04250432\n",
    "#     0.0073978   0.03714988]\n",
    "#   [-0.00955794  0.00252406  0.00787929 -0.05027757 -0.01628618\n",
    "#    -0.01388468  0.03728567 -0.02157371  0.00681018 -0.01150865\n",
    "#    -0.08478781 -0.03085445  0.03231757  0.05626081 -0.0752314\n",
    "#    -0.01583911  0.06897556 -0.06667089 -0.04729394 -0.02368151\n",
    "#     0.03042696 -0.00568471]]\n",
    "\n",
    "#  [[ 0.          0.          0.01346988 -0.03040301 -0.0009475\n",
    "#     0.03353934  0.03301353 -0.04421922  0.02690415  0.0534528\n",
    "#    -0.07279044  0.06620763 -0.00113248  0.09650994  0.02934606\n",
    "#     0.00773158 -0.10510534 -0.01168713 -0.03555111  0.019867\n",
    "#    -0.09727976 -0.0474655 ]\n",
    "#   [ 0.         -0.00663939  0.14461783 -0.00913084 -0.02329957\n",
    "#    -0.01995979 -0.07978858  0.08354648  0.07621053  0.03968899\n",
    "#     0.04010997 -0.07187632 -0.02174324 -0.02492543  0.04793847\n",
    "#     0.03261854 -0.01283982 -0.01570366 -0.01853073  0.04250432\n",
    "#     0.0073978   0.03714988]\n",
    "#   [-0.00955794  0.00178513  0.00787929 -0.05027757 -0.01628618\n",
    "#    -0.01388468  0.03728567 -0.02157371  0.00681018 -0.01150865\n",
    "#    -0.08478781 -0.03085445  0.03231757  0.05626081 -0.0752314\n",
    "#    -0.01583911  0.06897556 -0.06667089 -0.04729394 -0.02368151\n",
    "#     0.03042696 -0.00568471]]], shape=(2, 3, 22), dtype=float32)\n",
    "\n",
    "# kmin:\n",
    "# tf.Tensor([ 0.12124275 -0.80636215 -4.9503837 ], shape=(3), dtype=float32)\n",
    "\n",
    "# kmax:\n",
    "# tf.Tensor([0.79330695 0.41867393 4.934168  ], shape=(3), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86963/3061380864.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  min_val = torch.tensor(min_max[0], dtype=targets.dtype, device=targets.device)\n",
      "/tmp/ipykernel_86963/3061380864.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_val = torch.tensor(min_max[1], dtype=targets.dtype, device=targets.device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(4.04906702)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_out=torch.tensor([[ 0.00787929, -0.05027757, -0.01628618,\n",
    "   -0.01388468,  0.03728567, -0.02157371,  0.00681018, -0.01150865,\n",
    "   -0.08478781, -0.03085445,  0.03231757,  0.05626081, -0.0752314,\n",
    "   -0.01583911,  0.06897556, -0.06667089, -0.04729394, -0.02368151,\n",
    "    0.03042696, -0.00568471],\n",
    "                      [  0.00787929, -0.05027757, -0.01628618,\n",
    "   -0.01388468,  0.03728567, -0.02157371,  0.00681018, -0.01150865,\n",
    "   -0.08478781, -0.03085445,  0.03231757,  0.05626081, -0.0752314,\n",
    "   -0.01583911,  0.06897556, -0.06667089, -0.04729394, -0.02368151,\n",
    "    0.03042696, -0.00568471]])\n",
    "\n",
    "\n",
    "shifts_out=torch.tensor([[-0.00955794,  0.00252406 ]])\n",
    "\n",
    "\n",
    "outputs={'int_out':int_out,'shift_out':shifts_out}\n",
    "\n",
    "\n",
    "min_max=torch.Tensor([-4.9503837, 4.934168])\n",
    "\n",
    "\n",
    "\n",
    "contram_nll(outputs, targets, min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86963/3061380864.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  min_val = torch.tensor(min_max[0], dtype=targets.dtype, device=targets.device)\n",
      "/tmp/ipykernel_86963/3061380864.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_val = torch.tensor(min_max[1], dtype=targets.dtype, device=targets.device)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 22\u001b[0m\n\u001b[1;32m     15\u001b[0m outputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint_out\u001b[39m\u001b[38;5;124m'\u001b[39m:int_out,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshift_out\u001b[39m\u001b[38;5;124m'\u001b[39m:shifts_out}\n\u001b[1;32m     18\u001b[0m min_max\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4.9503837\u001b[39m, \u001b[38;5;241m4.934168\u001b[39m])\n\u001b[0;32m---> 22\u001b[0m \u001b[43mcontram_nll\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_max\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 24\u001b[0m, in \u001b[0;36mcontram_nll\u001b[0;34m(outputs, targets, min_max)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Compute h\u001b[39;00m\n\u001b[1;32m     23\u001b[0m h_I \u001b[38;5;241m=\u001b[39m h_extrapolated(thetas, targets, min_val, max_val)  \u001b[38;5;66;03m# shape (n,)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m h \u001b[38;5;241m=\u001b[39m h_I \u001b[38;5;241m+\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mShifts\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape (n,)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Latent logistic density log-prob\u001b[39;00m\n\u001b[1;32m     27\u001b[0m log_latent_density \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mh \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftplus(\u001b[38;5;241m-\u001b[39mh)  \u001b[38;5;66;03m# shape (n,)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: sum(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "int_out=torch.tensor([[ 1.00787929, -0.05027757, -0.01628618,\n",
    "   -0.01388468,  0.03728567, -0.02157371,  0.00681018, -0.01150865,\n",
    "   -0.08478781, -0.03085445,  0.3231757,  0.95626081, -0.0752314,\n",
    "   -0.01583911,  0.07897556, -0.06667089, -0.04729394, -0.92368151,\n",
    "    0.03042696, -0.0088471],[  0.00787929, -0.05027757, -0.01628618,\n",
    "   -0.01388468,  0.03728567, -0.02157371,  0.00681018, -0.01150865,\n",
    "   -0.08478781, -0.03085445,  0.03231757,  0.05626081, -0.0752314,\n",
    "   -0.01583911,  0.06897556, -0.06667089, -0.04729394, -0.02368151,\n",
    "    0.03042696, -0.00568471]])\n",
    "\n",
    "\n",
    "shifts_out=None\n",
    "\n",
    "\n",
    "outputs={'int_out':int_out,'shift_out':shifts_out}\n",
    "\n",
    "\n",
    "min_max=torch.Tensor([-4.9503837, 4.934168])\n",
    "\n",
    "\n",
    "\n",
    "contram_nll(outputs, targets, min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO test fucnitno with r values before mean reduction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
