{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0447acb9",
   "metadata": {},
   "source": [
    "in this file a potenitla api is developed for the tramdag "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad3fcb",
   "metadata": {},
   "source": [
    "## TRAM Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "363bd9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'config.json'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"config.json\" #has to be created either manually or by running createw_config.ipynb first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2f8fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg=TramDagConfig.load(CONFIG_PATH=\"config.json\") # basically just loads a json file to a dictionary\n",
    "\n",
    "# cfg.compute_scaling(df_train, write=True) # computes min max levels from training data and writes to cfg\n",
    "# # checks all specifications and throws warnings\n",
    "# # returns a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6006efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.configuration import *\n",
    "\n",
    "\n",
    "class TramDagConfig:\n",
    "    def __init__(self, conf_dict: dict = None, CONF_DICT_PATH: str = None):\n",
    "        \"\"\"\n",
    "        Initialize TramDagConfig.\n",
    "\n",
    "        Args:\n",
    "            conf_dict: optional dict with configuration. If None, starts empty.\n",
    "            CONF_DICT_PATH: optional path to config file.\n",
    "        \"\"\"\n",
    "        self.conf_dict = conf_dict or {}\n",
    "        self.CONF_DICT_PATH = CONF_DICT_PATH\n",
    "        # TODO write each configuration as an attribute? Or keep as dict?\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, CONF_DICT_PATH: str):\n",
    "        \"\"\"\n",
    "        Alternative constructor: load config directly from a file.\n",
    "        \"\"\"\n",
    "        conf = load_configuration_dict(CONF_DICT_PATH)\n",
    "        return cls(conf, CONF_DICT_PATH=CONF_DICT_PATH)\n",
    "\n",
    "    def save(self, CONF_DICT_PATH: str = None):\n",
    "        \"\"\"\n",
    "        Save config to file. If path is not provided, fall back to stored path.\n",
    "        \"\"\"\n",
    "        path = CONF_DICT_PATH or self.CONF_DICT_PATH\n",
    "        if path is None:\n",
    "            raise ValueError(\"No CONF_DICT_PATH provided to save config.\")\n",
    "        write_configuration_dict(self.conf_dict, path)\n",
    "\n",
    "    def compute_scaling(self, df: pd.DataFrame, write: bool = True):\n",
    "        \"\"\"\n",
    "        Derive scaling information (min, max, levels) from data USE training data.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Make sure to provide only training data to compute_scaling!\")\n",
    "        # calculate 5% and 95% quantiles for min and max values\n",
    "        quantiles = df.quantile([0.05, 0.95])\n",
    "        min_vals = quantiles.loc[0.05]\n",
    "        max_vals = quantiles.loc[0.95]\n",
    "\n",
    "        # calculate levels for categorical variables\n",
    "        levels_dict = create_levels_dict(df, self.conf_dict['data_type'])\n",
    "\n",
    "        # TODO remove outer dependency of these functions (re-loading conf dict)\n",
    "        adj_matrix = read_adj_matrix_from_configuration(self.CONF_DICT_PATH)\n",
    "        nn_names_matrix = read_nn_names_matrix_from_configuration(self.CONF_DICT_PATH)\n",
    "\n",
    "        node_dict = create_node_dict(\n",
    "            adj_matrix,\n",
    "            nn_names_matrix,\n",
    "            self.conf_dict['data_type'],\n",
    "            min_vals=min_vals,\n",
    "            max_vals=max_vals,\n",
    "            levels_dict=levels_dict\n",
    "        )\n",
    "        conf_dict = load_configuration_dict(self.CONF_DICT_PATH)\n",
    "        conf_dict['nodes'] = node_dict\n",
    "        self.conf_dict = conf_dict  # keep it in memory too\n",
    "\n",
    "        if write and self.CONF_DICT_PATH is not None:\n",
    "            try:\n",
    "                write_configuration_dict(conf_dict, self.CONF_DICT_PATH)\n",
    "                print(f'[INFO] Configuration with updated scaling saved to {self.CONF_DICT_PATH}')\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to save configuration: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af11e93",
   "metadata": {},
   "source": [
    "### test it --> works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3adc38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Make sure to provide only training data to compute_scaling!\n",
      "[INFO] Configuration with updated scaling saved to /home/bule/TramDag/dev_experiment_logs/exp_6_2/configuration.json\n"
     ]
    }
   ],
   "source": [
    "train_df=pd.read_csv('/home/bule/TramDag/dev_experiment_logs/exp_6_2/exp_6_2_train.csv')\n",
    "val_df=pd.read_csv('/home/bule/TramDag/dev_experiment_logs/exp_6_2/exp_6_2_val.csv')\n",
    "\n",
    "cfg = TramDagConfig.load(\"/home/bule/TramDag/dev_experiment_logs/exp_6_2/configuration.json\")\n",
    "\n",
    "\n",
    "cfg.compute_scaling(train_df) # computes min max levels from training data and writes to cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08f2d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO add veryfier such that nothing is missing for later training such as experiment name "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04f8d0",
   "metadata": {},
   "source": [
    "# TramDagDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6d9bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from utils.tram_data import GenericDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TramDagDataset(Dataset):\n",
    "    DEFAULTS = {\n",
    "        \"batch_size\": 32,\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 4,\n",
    "        \"pin_memory\": False,\n",
    "        \"return_intercept_shift\": True,\n",
    "        \"debug\": False,\n",
    "        \"transform\": None,\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Empty init. Use classmethods like .from_dataframe().\"\"\"\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df, cfg, **kwargs):\n",
    "        self = cls()\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(f\"[ERROR] df must be a pandas DataFrame, but got {type(df)}\")\n",
    "\n",
    "        # merge defaults with overrides\n",
    "        settings = dict(cls.DEFAULTS)\n",
    "        settings.update(kwargs)\n",
    "\n",
    "        # infer variable name automatically\n",
    "        callers_locals = inspect.currentframe().f_back.f_locals\n",
    "        inferred = None\n",
    "        for var_name, var_val in callers_locals.items():\n",
    "            if var_val is df:\n",
    "                inferred = var_name\n",
    "                break\n",
    "        df_name = inferred or \"dataframe\"\n",
    "\n",
    "        if settings[\"shuffle\"]:\n",
    "            if any(x in df_name.lower() for x in [\"val\", \"validation\", \"test\"]):\n",
    "                print(f\"[WARNING] DataFrame '{df_name}' looks like a validation/test set â†’ shuffle=True. Are you sure?\")\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.df = df.copy()\n",
    "        self._apply_settings(settings)\n",
    "        self._build_dataloaders()\n",
    "        return self\n",
    "\n",
    "    def _apply_settings(self, settings: dict):\n",
    "        \"\"\"Apply settings from defaults + overrides.\"\"\"\n",
    "        self.batch_size = settings[\"batch_size\"]\n",
    "        self.shuffle = settings[\"shuffle\"]\n",
    "        self.num_workers = settings[\"num_workers\"]\n",
    "        self.pin_memory = settings[\"pin_memory\"]\n",
    "        self.return_intercept_shift = settings[\"return_intercept_shift\"]\n",
    "        self.debug = settings[\"debug\"]\n",
    "        self.transform = settings[\"transform\"]\n",
    "\n",
    "        # nodes dict\n",
    "        self.nodes_dict = self.cfg.conf_dict[\"nodes\"]\n",
    "\n",
    "        # validate dict attributes for all configurable params\n",
    "        for name, val in {\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"shuffle\": self.shuffle,\n",
    "            \"num_workers\": self.num_workers,\n",
    "            \"pin_memory\": self.pin_memory,\n",
    "            \"return_intercept_shift\": self.return_intercept_shift,\n",
    "            \"debug\": self.debug,\n",
    "            \"transform\": self.transform,\n",
    "        }.items():\n",
    "            self._check_keys(name, val)\n",
    "\n",
    "    def _build_dataloaders(self):\n",
    "        \"\"\"Build node-specific dataloaders from df + settings.\"\"\"\n",
    "        self.loaders = {}\n",
    "        for node in self.nodes_dict:\n",
    "            ds = GenericDataset(\n",
    "                self.df,\n",
    "                target_col=node,\n",
    "                target_nodes=self.nodes_dict,\n",
    "                transform=self.transform if not isinstance(self.transform, dict) else self.transform[node],\n",
    "                return_intercept_shift=self.return_intercept_shift if not isinstance(self.return_intercept_shift, dict) else self.return_intercept_shift[node],\n",
    "                debug=self.debug if not isinstance(self.debug, dict) else self.debug[node],\n",
    "            )\n",
    "\n",
    "            batch_size = self.batch_size[node] if isinstance(self.batch_size, dict) else self.batch_size\n",
    "            shuffle_flag = self.shuffle[node] if isinstance(self.shuffle, dict) else bool(self.shuffle)\n",
    "            num_workers = self.num_workers[node] if isinstance(self.num_workers, dict) else self.num_workers\n",
    "            pin_memory = self.pin_memory[node] if isinstance(self.pin_memory, dict) else self.pin_memory\n",
    "\n",
    "            self.loaders[node] = DataLoader(\n",
    "                ds,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle_flag,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=pin_memory,\n",
    "            )\n",
    "\n",
    "    def _check_keys(self, attr_name, attr_value):\n",
    "        \"\"\"Check if dict keys match cfg.conf_dict['nodes'].keys().\"\"\"\n",
    "        if isinstance(attr_value, dict):\n",
    "            expected_keys = set(self.nodes_dict.keys())\n",
    "            given_keys = set(attr_value.keys())\n",
    "            if expected_keys != given_keys:\n",
    "                raise ValueError(\n",
    "                    f\"[ERROR] the provided attribute '{attr_name}' keys are not same as in cfg.conf_dict['nodes'].keys().\\n\"\n",
    "                    f\"Expected: {expected_keys}, but got: {given_keys}\\n\"\n",
    "                    f\"Please provide values for all variables.\"\n",
    "                )\n",
    "\n",
    "    def summary(self):\n",
    "        print(\"\\n[TramDagDataset Summary]\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # ---- DataFrame section ----\n",
    "        print(\"\\n[DataFrame]\")\n",
    "        print(\"Shape:\", self.df.shape)\n",
    "        print(\"\\nHead:\")\n",
    "        print(self.df.head())\n",
    "\n",
    "        print(\"\\nDtypes:\")\n",
    "        print(self.df.dtypes)\n",
    "\n",
    "        print(\"\\nDescribe:\")\n",
    "        print(self.df.describe(include=\"all\"))\n",
    "\n",
    "        # ---- Settings per node ----\n",
    "        print(\"\\n[Node Settings]\")\n",
    "        for node in self.nodes_dict.keys():\n",
    "            batch_size = self.batch_size[node] if isinstance(self.batch_size, dict) else self.batch_size\n",
    "            shuffle_flag = self.shuffle[node] if isinstance(self.shuffle, dict) else bool(self.shuffle)\n",
    "            num_workers = self.num_workers[node] if isinstance(self.num_workers, dict) else self.num_workers\n",
    "            pin_memory = self.pin_memory[node] if isinstance(self.pin_memory, dict) else self.pin_memory\n",
    "            rshift = self.return_intercept_shift[node] if isinstance(self.return_intercept_shift, dict) else self.return_intercept_shift\n",
    "            debug_flag = self.debug[node] if isinstance(self.debug, dict) else self.debug\n",
    "            transform = self.transform[node] if isinstance(self.transform, dict) else self.transform\n",
    "\n",
    "            print(\n",
    "                f\" Node '{node}': \"\n",
    "                f\"batch_size={batch_size}, \"\n",
    "                f\"shuffle={shuffle_flag}, \"\n",
    "                f\"num_workers={num_workers}, \"\n",
    "                f\"pin_memory={pin_memory}, \"\n",
    "                f\"return_intercept_shift={rshift}, \"\n",
    "                f\"debug={debug_flag}, \"\n",
    "                f\"transform={transform}\"\n",
    "            )\n",
    "        print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.df.iloc[idx].to_dict()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947bda8",
   "metadata": {},
   "source": [
    "## testit -> works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3923c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_train_data=TramDagDataset.from_dataframe(train_df,cfg)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "463a47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_val_data=TramDagDataset.from_dataframe(val_df,cfg,shuffle=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1a9528",
   "metadata": {},
   "source": [
    "# TramDagModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f5ef99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Building model for node 'x1' with settings: {'set_initial_weights': True, 'debug': False}\n",
      "\n",
      "[INFO] Building model for node 'x2' with settings: {'set_initial_weights': True, 'debug': False}\n",
      "\n",
      "[INFO] Building model for node 'x3' with settings: {'set_initial_weights': True, 'debug': False}\n",
      "\n",
      "[TramDagModel Summary]\n",
      "============================================================\n",
      " Node 'x1': TramModel\n",
      "   - set_initial_weights: True\n",
      "   - debug: False\n",
      " Node 'x2': TramModel\n",
      "   - set_initial_weights: True\n",
      "   - debug: False\n",
      " Node 'x3': TramModel\n",
      "   - set_initial_weights: True\n",
      "   - debug: False\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "td_model = TramDagModel.from_config(cfg, set_initial_weights=True) \n",
    "# builds the models according to the config\n",
    "# returns a dict of TramModels for each node {'x1':model1,'x2':model2,...}\n",
    "\n",
    "td_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c39dd683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tram_model_helpers import train_val_loop, get_fully_specified_tram_model\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "\n",
    "\n",
    "class TramDagModel:\n",
    "    # ---- defaults used at construction time ----\n",
    "    DEFAULTS_CONFIG = {\n",
    "        \"set_initial_weights\": True,\n",
    "        \"debug\":False,\n",
    "        \n",
    "    }\n",
    "\n",
    "    # ---- defaults used at fit() time ----\n",
    "    DEFAULTS_FIT = {\n",
    "        \"epochs\": 100,\n",
    "        \"train_list\": None,\n",
    "        \"callbacks\": None,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"device\": \"auto\",\n",
    "        \"optimizers\": None,\n",
    "        \"schedulers\": None,\n",
    "        \"use_scheduler\": False,\n",
    "        \"save_linear_shifts\": True,\n",
    "        \"debug\":False,\n",
    "        \"verbose\": 1,\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Empty init. Use classmethods like .from_config().\"\"\"\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg, **kwargs):\n",
    "        \"\"\"\n",
    "        Build one TramModel per node based on configuration and kwargs.\n",
    "        Kwargs can be scalars (applied to all nodes) or dicts {node: value}.\n",
    "        \"\"\"\n",
    "        self = cls()\n",
    "        self.cfg = cfg\n",
    "        self.nodes_dict = self.cfg.conf_dict[\"nodes\"]\n",
    "\n",
    "        # merge defaults with user overrides\n",
    "        settings = dict(cls.DEFAULTS_CONFIG)\n",
    "        settings.update(kwargs)\n",
    "\n",
    "        # initialize settings storage\n",
    "        self.settings = {k: {} for k in settings.keys()}\n",
    "\n",
    "        # validate dict-typed args\n",
    "        for k, v in settings.items():\n",
    "            if isinstance(v, dict):\n",
    "                expected = set(self.nodes_dict.keys())\n",
    "                given = set(v.keys())\n",
    "                if expected != given:\n",
    "                    raise ValueError(\n",
    "                        f\"[ERROR] the provided argument '{k}' keys are not same as in cfg.conf_dict['nodes'].keys().\\n\"\n",
    "                        f\"Expected: {expected}, but got: {given}\\n\"\n",
    "                        f\"Please provide values for all variables.\"\n",
    "                    )\n",
    "\n",
    "        # build one model per node\n",
    "        self.models = {}\n",
    "        for node in self.nodes_dict.keys():\n",
    "            per_node_kwargs = {}\n",
    "            for k, v in settings.items():\n",
    "                resolved = v[node] if isinstance(v, dict) else v\n",
    "                per_node_kwargs[k] = resolved\n",
    "                self.settings[k][node] = resolved\n",
    "            print(f\"\\n[INFO] Building model for node '{node}' with settings: {per_node_kwargs}\")\n",
    "            self.models[node] = get_fully_specified_tram_model(\n",
    "                node=node,\n",
    "                configuration_dict=self.cfg.conf_dict,\n",
    "                **per_node_kwargs\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def fit(self, td_train_data, td_val_data, **kwargs):\n",
    "        \"\"\"\n",
    "        Fit TRAM models for specified nodes.\n",
    "        All kwargs can be scalar (applied to all nodes) or dict {node: value}.\n",
    "        \"\"\"\n",
    "        # merge defaults with overrides\n",
    "        settings = dict(self.DEFAULTS_FIT)\n",
    "        settings.update(kwargs)\n",
    "\n",
    "        device = torch.device(\n",
    "            \"cuda\" if (settings[\"device\"] == \"auto\" and torch.cuda.is_available()) else settings[\"device\"]\n",
    "        )\n",
    "        train_list = settings[\"train_list\"] or list(self.models.keys())\n",
    "\n",
    "        results = {}\n",
    "        for node in train_list:\n",
    "            model = self.models[node]\n",
    "\n",
    "            # resolve epochs\n",
    "            node_epochs = settings[\"epochs\"][node] if isinstance(settings[\"epochs\"], dict) else settings[\"epochs\"]\n",
    "\n",
    "            # resolve optimizer\n",
    "            if settings[\"optimizers\"] and node in settings[\"optimizers\"]:\n",
    "                optimizer = settings[\"optimizers\"][node]\n",
    "            else:\n",
    "                optimizer = Adam(model.parameters(), lr=settings[\"learning_rate\"])\n",
    "\n",
    "            # resolve scheduler\n",
    "            if settings[\"schedulers\"] and node in settings[\"schedulers\"]:\n",
    "                scheduler = settings[\"schedulers\"][node]\n",
    "            else:\n",
    "                scheduler = None\n",
    "\n",
    "            # grab loaders\n",
    "            train_loader = td_train_data.loaders[node]\n",
    "            val_loader = td_val_data.loaders[node]\n",
    "\n",
    "            NODE_DIR = os.path.join(\"models\", node)\n",
    "            os.makedirs(NODE_DIR, exist_ok=True)\n",
    "\n",
    "            if settings[\"verbose\"]:\n",
    "                print(f\"\\n[INFO] Training node '{node}' for {node_epochs} epochs on {device}\")\n",
    "\n",
    "            history = train_val_loop(\n",
    "                node=node,\n",
    "                target_nodes=self.nodes_dict,\n",
    "                NODE_DIR=NODE_DIR,\n",
    "                tram_model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                epochs=node_epochs,\n",
    "                optimizer=optimizer,\n",
    "                use_scheduler=(scheduler is not None),\n",
    "                scheduler=scheduler,\n",
    "                save_linear_shifts=settings[\"save_linear_shifts\"],\n",
    "                verbose=settings[\"verbose\"],\n",
    "                device=device,\n",
    "                debug=False\n",
    "            )\n",
    "            results[node] = history\n",
    "\n",
    "        return results\n",
    "\n",
    "    def summary(self):\n",
    "        print(\"\\n[TramDagModel Summary]\")\n",
    "        print(\"=\" * 60)\n",
    "        for node, model in self.models.items():\n",
    "            print(f\" Node '{node}': {model.__class__.__name__}\")\n",
    "            for k, v in self.settings.items():\n",
    "                if node in v:\n",
    "                    print(f\"   - {k}: {v[node]}\")\n",
    "        print(\"=\" * 60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5629f348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Training node 'x1' for 1 epochs on cuda\n",
      "No existing model found. Starting fresh...\n",
      "Saved new best model.\n",
      "Epoch 1/1  Train NLL: -0.4294  Val NLL: -0.5253  [Train: 26.06s  Val: 1.85s  Total: 27.92s]\n",
      "\n",
      "[INFO] Training node 'x2' for 1 epochs on cuda\n",
      "No existing model found. Starting fresh...\n",
      "Saved new best model.\n",
      "Epoch 1/1  Train NLL: 0.9413  Val NLL: 0.5135  [Train: 30.16s  Val: 1.81s  Total: 31.98s]\n",
      "\n",
      "[INFO] Training node 'x3' for 1 epochs on cuda\n",
      "No existing model found. Starting fresh...\n",
      "Saved new best model.\n",
      "Epoch 1/1  Train NLL: 1.2151  Val NLL: 1.2097  [Train: 18.55s  Val: 1.19s  Total: 19.74s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x1': None, 'x2': None, 'x3': None}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_model.fit( td_train_data, td_val_data, epochs=1,debug=True)\n",
    "\n",
    "# the fit functin trains all models in the train_list independently for the specified epochs \n",
    "\n",
    "#td_fit object contains the history and the best models for each node as well as the cfg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb2d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model.history() # show_training_history(node_list,EXPERIMENT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b05610",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model.show_hdag_for_source_nodes()#show_hdag_for_source_nodes(configuration_dict,EXPERIMENT_DIR,device=device,xmin_plot=0,xmax_plot=1) # TODO for other nodes funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd443525",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model.inspect_trafo_standart_logistic()\n",
    "#inspect_trafo_standart_logistic(configuration_dict,EXPERIMENT_DIR,train_df,val_df,device,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacaaa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model.get_latent() # returns or saves as attribute to td_model , this calls the \n",
    "#all_latents_df = create_latent_df_for_full_dag(configuration_dict, EXPERIMENT_DIR, train_df, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model.sample()\n",
    "\n",
    "sampled_by_node, latents_by_node=sample_full_dag(configuration_dict,\n",
    "                EXPERIMENT_DIR,\n",
    "                device,\n",
    "                do_interventions={},\n",
    "                predefined_latent_samples_df=None,#all_latents_df,\n",
    "                number_of_samples= 10_000,\n",
    "                batch_size = 32,\n",
    "                delete_all_previously_sampled=True,\n",
    "                verbose=True,\n",
    "                debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
