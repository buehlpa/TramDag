{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0447acb9",
   "metadata": {},
   "source": [
    "in this file a potenitla api is developed for the tramdag "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad3fcb",
   "metadata": {},
   "source": [
    "## TRAM Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363bd9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"config.json\" #has to be created either manually or by running createw_config.ipynb first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg=TramDagConfig.load(CONFIG_PATH=\"config.json\") # basically just loads a json file to a dictionary\n",
    "\n",
    "# cfg.compute_scaling(df_train, write=True) # computes min max levels from training data and writes to cfg\n",
    "# # checks all specifications and throws warnings\n",
    "# # returns a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6006efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.configuration import *\n",
    "\n",
    "class TramDagConfig:\n",
    "    def __init__(self, conf_dict: dict = None, CONF_DICT_PATH: str = None):\n",
    "        \"\"\"\n",
    "        Initialize TramDagConfig.\n",
    "\n",
    "        Args:\n",
    "            conf_dict: optional dict with configuration. If None, starts empty.\n",
    "            CONF_DICT_PATH: optional path to config file.\n",
    "        \"\"\"\n",
    "        self.conf_dict = conf_dict or {}\n",
    "        self.CONF_DICT_PATH = CONF_DICT_PATH\n",
    "        # TODO write each configuration as an attribute? Or keep as dict?\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, CONF_DICT_PATH: str):\n",
    "        \"\"\"\n",
    "        Alternative constructor: load config directly from a file.\n",
    "        \"\"\"\n",
    "        conf = load_configuration_dict(CONF_DICT_PATH)\n",
    "        return cls(conf, CONF_DICT_PATH=CONF_DICT_PATH)\n",
    "\n",
    "    def save(self, CONF_DICT_PATH: str = None):\n",
    "        \"\"\"\n",
    "        Save config to file. If path is not provided, fall back to stored path.\n",
    "        \"\"\"\n",
    "        path = CONF_DICT_PATH or self.CONF_DICT_PATH\n",
    "        if path is None:\n",
    "            raise ValueError(\"No CONF_DICT_PATH provided to save config.\")\n",
    "        write_configuration_dict(self.conf_dict, path)\n",
    "\n",
    "    def compute_scaling(self, df: pd.DataFrame, write: bool = True):\n",
    "        \"\"\"\n",
    "        Derive scaling information (min, max, levels) from data USE training data.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Make sure to provide only training data to compute_scaling!\")\n",
    "        # calculate 5% and 95% quantiles for min and max values\n",
    "        quantiles = df.quantile([0.05, 0.95])\n",
    "        min_vals = quantiles.loc[0.05]\n",
    "        max_vals = quantiles.loc[0.95]\n",
    "\n",
    "        # calculate levels for categorical variables\n",
    "        levels_dict = create_levels_dict(df, self.conf_dict['data_type'])\n",
    "\n",
    "        # TODO remove outer dependency of these functions (re-loading conf dict)\n",
    "        adj_matrix = read_adj_matrix_from_configuration(self.CONF_DICT_PATH)\n",
    "        nn_names_matrix = read_nn_names_matrix_from_configuration(self.CONF_DICT_PATH)\n",
    "\n",
    "        node_dict = create_node_dict(\n",
    "            adj_matrix,\n",
    "            nn_names_matrix,\n",
    "            self.conf_dict['data_type'],\n",
    "            min_vals=min_vals,\n",
    "            max_vals=max_vals,\n",
    "            levels_dict=levels_dict\n",
    "        )\n",
    "        conf_dict = load_configuration_dict(self.CONF_DICT_PATH)\n",
    "        conf_dict['nodes'] = node_dict\n",
    "        self.conf_dict = conf_dict  # keep it in memory too\n",
    "\n",
    "        if write and self.CONF_DICT_PATH is not None:\n",
    "            try:\n",
    "                write_configuration_dict(conf_dict, self.CONF_DICT_PATH)\n",
    "                print(f'[INFO] Configuration with updated scaling saved to {self.CONF_DICT_PATH}')\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to save configuration: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af11e93",
   "metadata": {},
   "source": [
    "### test it --> works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3adc38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Make sure to provide only training data to compute_scaling!\n",
      "[INFO] Configuration with updated scaling saved to /home/bule/TramDag/dev_experiment_logs/exp_6_2/configuration.json\n"
     ]
    }
   ],
   "source": [
    "train_df=pd.read_csv('/home/bule/TramDag/dev_experiment_logs/exp_6_2/exp_6_2_train.csv')\n",
    "cfg = TramDagConfig.load(\"/home/bule/TramDag/dev_experiment_logs/exp_6_2/configuration.json\")\n",
    "\n",
    "\n",
    "cfg.compute_scaling(train_df) # computes min max levels from training data and writes to cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04f8d0",
   "metadata": {},
   "source": [
    "# TramDagDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed7786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tram_data import *\n",
    "from utils.tram_data_helpers import *\n",
    "\n",
    "\n",
    "class TramDagDataset:\n",
    "    def __init__(self, df: pd.DataFrame, cfg: TramDagConfig, split=\"train\"):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.split = split\n",
    "        self.datasets = {}\n",
    "\n",
    "        for node, meta in cfg.conf_dict[\"nodes\"].items():\n",
    "            self.datasets[node] = GenericDataset(\n",
    "                df=df,\n",
    "                target_col=node,\n",
    "                target_nodes=cfg.conf_dict[\"nodes\"],\n",
    "                return_intercept_shift=True,\n",
    "                return_y=True\n",
    "            )\n",
    "\n",
    "    def get_dataloader(self, node: str, batch_size=128, shuffle=True):\n",
    "        ds = self.datasets[node]\n",
    "        return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b95f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader( node,\n",
    "                    target_nodes,\n",
    "                    train_df=None,\n",
    "                    val_df=None,\n",
    "                    batch_size=32,\n",
    "                    return_intercept_shift=False,\n",
    "                    debug=False,\n",
    "                    transform=None,\n",
    "                    ):\n",
    "    \n",
    "        if transform is None:\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((128, 128)),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "        train_loader, val_loader = None, None\n",
    "\n",
    "        if train_df is not None:\n",
    "            train_ds = GenericDataset(train_df,target_col=node,target_nodes=target_nodes,transform=transform,return_intercept_shift=return_intercept_shift,debug=debug)\n",
    "            train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,num_workers=4, pin_memory=True)\n",
    "        else:\n",
    "            print(\"[INFO] train_df is None → skipping train dataloader.\")\n",
    "\n",
    "        if val_df is not None:\n",
    "            val_ds = GenericDataset(val_df,target_col=node,target_nodes=target_nodes,transform=transform,return_intercept_shift=return_intercept_shift,debug=debug)\n",
    "            val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        else:\n",
    "            print(\"[INFO] val_df is None → skipping val dataloader.\")\n",
    "\n",
    "        if train_loader is None and val_loader is None:\n",
    "            raise ValueError(\"[ERROR] Both train_df and val_df are None → no dataloaders created.\")\n",
    "\n",
    "        return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3923c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_train_data=TramDagDataset.from_dataframe(df_train,cfg) # also extracts min max vals from training data and the levels \n",
    "td_val_data  =TramDagDataset.from_dataframe(df_val,cfg) \n",
    "\n",
    "# additonal arguments could be batch_size, num_workers, pin_memory etc \n",
    "# returs a dictionary with {'x1':dataloader1,'x2':dataloader2,...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f5ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model = TramDagModel.from_config(cfg) \n",
    "# builds the models according to the config\n",
    "# returns a dict of TramModels for each node {'x1':model1,'x2':model2,...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39dd683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TramDagModel:\n",
    "    def __init__(self, cfg: TramDagConfig, device=\"cuda\"):\n",
    "        self.cfg = cfg\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "        self.models = {}\n",
    "\n",
    "        for node in cfg.conf_dict[\"nodes\"]:\n",
    "            self.models[node] = get_fully_specified_tram_model(node, cfg.conf_dict[\"nodes\"]).to(self.device)\n",
    "\n",
    "    def fit(self, train_dataset: TramDagDataset, val_dataset: TramDagDataset, epochs=20, lr=1e-3):\n",
    "        optimizers = {node: torch.optim.Adam(self.models[node].parameters(), lr=lr)\n",
    "                      for node in self.models}\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for node, model in self.models.items():\n",
    "                model.train()\n",
    "                train_loader = train_dataset.get_dataloader(node)\n",
    "                for (int_input, shift_list), y in train_loader:\n",
    "                    int_input = int_input.to(self.device)\n",
    "                    shift_list = [s.to(self.device) for s in shift_list] if shift_list else None\n",
    "                    y = y.to(self.device)\n",
    "\n",
    "                    outputs = model(int_input, shift_list)\n",
    "                    if self.cfg.conf_dict[\"nodes\"][node][\"data_type\"] == \"cont\":\n",
    "                        min_max = (self.cfg.conf_dict[\"nodes\"][node][\"min\"],\n",
    "                                   self.cfg.conf_dict[\"nodes\"][node][\"max\"])\n",
    "                        loss = contram_nll(outputs, y, min_max)\n",
    "                    else:\n",
    "                        loss = ontram_nll(outputs, y)\n",
    "\n",
    "                    optimizers[node].zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizers[node].step()\n",
    "            print(f\"Epoch {epoch+1}/{epochs} finished.\")\n",
    "\n",
    "    def save(self, path: str):\n",
    "        torch.save({\n",
    "            \"cfg\": self.cfg.conf_dict,\n",
    "            \"state_dicts\": {node: model.state_dict() for node, model in self.models.items()}\n",
    "        }, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str, device=\"cuda\"):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        cfg = TramDagConfig(checkpoint[\"cfg\"])\n",
    "        obj = cls(cfg, device=device)\n",
    "        for node, model in obj.models.items():\n",
    "            model.load_state_dict(checkpoint[\"state_dicts\"][node])\n",
    "        return obj\n",
    "\n",
    "    def predict(self, df: pd.DataFrame, node: str, batch_size=128):\n",
    "        dataset = TramDagDataset(df, self.cfg, split=\"test\")\n",
    "        loader = dataset.get_dataloader(node, batch_size=batch_size, shuffle=False)\n",
    "        model = self.models[node].eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for (int_input, shift_list), _ in loader:\n",
    "                int_input = int_input.to(self.device)\n",
    "                shift_list = [s.to(self.device) for s in shift_list] if shift_list else None\n",
    "                out = model(int_input, shift_list)\n",
    "                preds.append(out[\"int_out\"].cpu())\n",
    "        return torch.cat(preds, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5629f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a trainer object which can train all models in the dag independently\n",
    "td_model.fit( td_train_data, td_val_data, epochs=100 train_list=['x1','x2','x3'], callbacks=[], learning_rate,  device=\"auto\")\n",
    "\n",
    "# the fit functin trains all models in the train_list independently for the specified epochs \n",
    "\n",
    "#td_fit object contains the history and the best models for each node as well as the cfg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb2d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model.history() # show_training_history(node_list,EXPERIMENT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b05610",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model.show_hdag_for_source_nodes()#show_hdag_for_source_nodes(configuration_dict,EXPERIMENT_DIR,device=device,xmin_plot=0,xmax_plot=1) # TODO for other nodes funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd443525",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model.inspect_trafo_standart_logistic()\n",
    "#inspect_trafo_standart_logistic(configuration_dict,EXPERIMENT_DIR,train_df,val_df,device,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacaaa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model.get_latent() # returns or saves as attribute to td_model , this calls the \n",
    "#all_latents_df = create_latent_df_for_full_dag(configuration_dict, EXPERIMENT_DIR, train_df, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model.sample()\n",
    "\n",
    "sampled_by_node, latents_by_node=sample_full_dag(configuration_dict,\n",
    "                EXPERIMENT_DIR,\n",
    "                device,\n",
    "                do_interventions={},\n",
    "                predefined_latent_samples_df=None,#all_latents_df,\n",
    "                number_of_samples= 10_000,\n",
    "                batch_size = 32,\n",
    "                delete_all_previously_sampled=True,\n",
    "                verbose=True,\n",
    "                debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
