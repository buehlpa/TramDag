{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0447acb9",
   "metadata": {},
   "source": [
    "in this file a potenitla api is developed for the tramdag "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad3fcb",
   "metadata": {},
   "source": [
    "## TRAM Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6006efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.configuration import *\n",
    "\n",
    "class TramDagConfig:\n",
    "    def __init__(self, conf_dict: dict = None, CONF_DICT_PATH: str = None):\n",
    "        \"\"\"\n",
    "        Initialize TramDagConfig.\n",
    "\n",
    "        Args:\n",
    "            conf_dict: optional dict with configuration. If None, starts empty.\n",
    "            CONF_DICT_PATH: optional path to config file.\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO add verbose and debug , vebose print only infos, debug prints info + debug statements, warnings, errors are always printed\n",
    "        #TODO add veryfier such that nothing is missing for later training such as experiment name \n",
    "        \n",
    "        self.conf_dict = conf_dict or {}\n",
    "        self.CONF_DICT_PATH = CONF_DICT_PATH\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, CONF_DICT_PATH: str):\n",
    "        \"\"\"\n",
    "        Alternative constructor: load config directly from a file.\n",
    "        \"\"\"\n",
    "        conf = load_configuration_dict(CONF_DICT_PATH)\n",
    "        return cls(conf, CONF_DICT_PATH=CONF_DICT_PATH)\n",
    "\n",
    "    def save(self, CONF_DICT_PATH: str = None):\n",
    "        \"\"\"\n",
    "        Save config to file. If path is not provided, fall back to stored path.\n",
    "        \"\"\"\n",
    "        path = CONF_DICT_PATH or self.CONF_DICT_PATH\n",
    "        if path is None:\n",
    "            raise ValueError(\"No CONF_DICT_PATH provided to save config.\")\n",
    "        write_configuration_dict(self.conf_dict, path)\n",
    "\n",
    "    def compute_scaling(self, df: pd.DataFrame, write: bool = True):\n",
    "        \"\"\"\n",
    "        Derive scaling information (min, max, levels) from data USE training data.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Make sure to provide only training data to compute_scaling!\")\n",
    "        # calculate 5% and 95% quantiles for min and max values\n",
    "        quantiles = df.quantile([0.05, 0.95])\n",
    "        min_vals = quantiles.loc[0.05]\n",
    "        max_vals = quantiles.loc[0.95]\n",
    "\n",
    "        # calculate levels for categorical variables\n",
    "        levels_dict = create_levels_dict(df, self.conf_dict['data_type'])\n",
    "\n",
    "        # TODO remove outer dependency of these functions (re-loading conf dict)\n",
    "        adj_matrix = read_adj_matrix_from_configuration(self.CONF_DICT_PATH)\n",
    "        nn_names_matrix = read_nn_names_matrix_from_configuration(self.CONF_DICT_PATH)\n",
    "\n",
    "        node_dict = create_node_dict(\n",
    "            adj_matrix,\n",
    "            nn_names_matrix,\n",
    "            self.conf_dict['data_type'],\n",
    "            min_vals=min_vals,\n",
    "            max_vals=max_vals,\n",
    "            levels_dict=levels_dict\n",
    "        )\n",
    "        conf_dict = load_configuration_dict(self.CONF_DICT_PATH)\n",
    "        conf_dict['nodes'] = node_dict\n",
    "        self.conf_dict = conf_dict  # keep it in memory too\n",
    "\n",
    "        if write and self.CONF_DICT_PATH is not None:\n",
    "            try:\n",
    "                write_configuration_dict(conf_dict, self.CONF_DICT_PATH)\n",
    "                print(f'[INFO] Configuration with updated scaling saved to {self.CONF_DICT_PATH}')\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to save configuration: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04f8d0",
   "metadata": {},
   "source": [
    "# TramDagDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6d9bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from utils.tram_data import GenericDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TramDagDataset(Dataset):\n",
    "    \n",
    "    #TODO add docstring\n",
    "    #TODO add verbose and debug , vebose print only infos, debug prints info + debug statements, warnings, errors are always printed\n",
    "    #TODO add veryfier such that nothing is missing for later training such as experiment name \n",
    "    \n",
    "    DEFAULTS = {\n",
    "        \"batch_size\": 32,\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 4,\n",
    "        \"pin_memory\": False,\n",
    "        \"return_intercept_shift\": True,\n",
    "        \"debug\": False,\n",
    "        \"transform\": None,\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Empty init. Use classmethods like .from_dataframe().\"\"\"\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df, cfg, **kwargs):\n",
    "        self = cls()\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(f\"[ERROR] df must be a pandas DataFrame, but got {type(df)}\")\n",
    "\n",
    "        # merge defaults with overrides\n",
    "        settings = dict(cls.DEFAULTS)\n",
    "        settings.update(kwargs)\n",
    "\n",
    "        # infer variable name automatically\n",
    "        callers_locals = inspect.currentframe().f_back.f_locals\n",
    "        inferred = None\n",
    "        for var_name, var_val in callers_locals.items():\n",
    "            if var_val is df:\n",
    "                inferred = var_name\n",
    "                break\n",
    "        df_name = inferred or \"dataframe\"\n",
    "\n",
    "        if settings[\"shuffle\"]:\n",
    "            if any(x in df_name.lower() for x in [\"val\", \"validation\", \"test\"]):\n",
    "                print(f\"[WARNING] DataFrame '{df_name}' looks like a validation/test set â†’ shuffle=True. Are you sure?\")\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.df = df.copy()\n",
    "        self._apply_settings(settings)\n",
    "        self._build_dataloaders()\n",
    "        return self\n",
    "\n",
    "    def _apply_settings(self, settings: dict):\n",
    "        \"\"\"Apply settings from defaults + overrides.\"\"\"\n",
    "        self.batch_size = settings[\"batch_size\"]\n",
    "        self.shuffle = settings[\"shuffle\"]\n",
    "        self.num_workers = settings[\"num_workers\"]\n",
    "        self.pin_memory = settings[\"pin_memory\"]\n",
    "        self.return_intercept_shift = settings[\"return_intercept_shift\"]\n",
    "        self.debug = settings[\"debug\"]\n",
    "        self.transform = settings[\"transform\"]\n",
    "\n",
    "        # nodes dict\n",
    "        self.nodes_dict = self.cfg.conf_dict[\"nodes\"]\n",
    "\n",
    "        # validate dict attributes for all configurable params\n",
    "        for name, val in {\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"shuffle\": self.shuffle,\n",
    "            \"num_workers\": self.num_workers,\n",
    "            \"pin_memory\": self.pin_memory,\n",
    "            \"return_intercept_shift\": self.return_intercept_shift,\n",
    "            \"debug\": self.debug,\n",
    "            \"transform\": self.transform,\n",
    "        }.items():\n",
    "            self._check_keys(name, val)\n",
    "\n",
    "    def _build_dataloaders(self):\n",
    "        \"\"\"Build node-specific dataloaders from df + settings.\"\"\"\n",
    "        self.loaders = {}\n",
    "        for node in self.nodes_dict:\n",
    "            ds = GenericDataset(\n",
    "                self.df,\n",
    "                target_col=node,\n",
    "                target_nodes=self.nodes_dict,\n",
    "                transform=self.transform if not isinstance(self.transform, dict) else self.transform[node],\n",
    "                return_intercept_shift=self.return_intercept_shift if not isinstance(self.return_intercept_shift, dict) else self.return_intercept_shift[node],\n",
    "                debug=self.debug if not isinstance(self.debug, dict) else self.debug[node],\n",
    "            )\n",
    "\n",
    "            batch_size = self.batch_size[node] if isinstance(self.batch_size, dict) else self.batch_size\n",
    "            shuffle_flag = self.shuffle[node] if isinstance(self.shuffle, dict) else bool(self.shuffle)\n",
    "            num_workers = self.num_workers[node] if isinstance(self.num_workers, dict) else self.num_workers\n",
    "            pin_memory = self.pin_memory[node] if isinstance(self.pin_memory, dict) else self.pin_memory\n",
    "\n",
    "            self.loaders[node] = DataLoader(\n",
    "                ds,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle_flag,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=pin_memory,\n",
    "            )\n",
    "\n",
    "    def _check_keys(self, attr_name, attr_value):\n",
    "        \"\"\"Check if dict keys match cfg.conf_dict['nodes'].keys().\"\"\"\n",
    "        if isinstance(attr_value, dict):\n",
    "            expected_keys = set(self.nodes_dict.keys())\n",
    "            given_keys = set(attr_value.keys())\n",
    "            if expected_keys != given_keys:\n",
    "                raise ValueError(\n",
    "                    f\"[ERROR] the provided attribute '{attr_name}' keys are not same as in cfg.conf_dict['nodes'].keys().\\n\"\n",
    "                    f\"Expected: {expected_keys}, but got: {given_keys}\\n\"\n",
    "                    f\"Please provide values for all variables.\"\n",
    "                )\n",
    "\n",
    "    def summary(self):\n",
    "        print(\"\\n[TramDagDataset Summary]\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # ---- DataFrame section ----\n",
    "        print(\"\\n[DataFrame]\")\n",
    "        print(\"Shape:\", self.df.shape)\n",
    "        print(\"\\nHead:\")\n",
    "        print(self.df.head())\n",
    "\n",
    "        print(\"\\nDtypes:\")\n",
    "        print(self.df.dtypes)\n",
    "\n",
    "        print(\"\\nDescribe:\")\n",
    "        print(self.df.describe(include=\"all\"))\n",
    "\n",
    "        # ---- Settings per node ----\n",
    "        print(\"\\n[Node Settings]\")\n",
    "        for node in self.nodes_dict.keys():\n",
    "            batch_size = self.batch_size[node] if isinstance(self.batch_size, dict) else self.batch_size\n",
    "            shuffle_flag = self.shuffle[node] if isinstance(self.shuffle, dict) else bool(self.shuffle)\n",
    "            num_workers = self.num_workers[node] if isinstance(self.num_workers, dict) else self.num_workers\n",
    "            pin_memory = self.pin_memory[node] if isinstance(self.pin_memory, dict) else self.pin_memory\n",
    "            rshift = self.return_intercept_shift[node] if isinstance(self.return_intercept_shift, dict) else self.return_intercept_shift\n",
    "            debug_flag = self.debug[node] if isinstance(self.debug, dict) else self.debug\n",
    "            transform = self.transform[node] if isinstance(self.transform, dict) else self.transform\n",
    "\n",
    "            print(\n",
    "                f\" Node '{node}': \"\n",
    "                f\"batch_size={batch_size}, \"\n",
    "                f\"shuffle={shuffle_flag}, \"\n",
    "                f\"num_workers={num_workers}, \"\n",
    "                f\"pin_memory={pin_memory}, \"\n",
    "                f\"return_intercept_shift={rshift}, \"\n",
    "                f\"debug={debug_flag}, \"\n",
    "                f\"transform={transform}\"\n",
    "            )\n",
    "        print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.df.iloc[idx].to_dict()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1a9528",
   "metadata": {},
   "source": [
    "# TramDagModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c39dd683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tram_model_helpers import train_val_loop, get_fully_specified_tram_model \n",
    "from utils.tram_data_helpers import create_latent_df_for_full_dag, sample_full_dag\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "class TramDagModel:\n",
    "    \n",
    "    #TODO add docstring\n",
    "    #TODO add verbose and debug , vebose print only infos, debug prints info + debug statements, warnings, errors are always printed\n",
    "    # ---- defaults used at construction time ----\n",
    "    DEFAULTS_CONFIG = {\n",
    "        \"set_initial_weights\": True,\n",
    "        \"debug\":False,\n",
    "        \n",
    "    }\n",
    "\n",
    "    # ---- defaults used at fit() time ----\n",
    "    DEFAULTS_FIT = {\n",
    "        \"epochs\": 100,\n",
    "        \"train_list\": None,\n",
    "        \"callbacks\": None,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"device\": \"auto\",\n",
    "        \"optimizers\": None,\n",
    "        \"schedulers\": None,\n",
    "        \"use_scheduler\": False,\n",
    "        \"save_linear_shifts\": True,\n",
    "        \"debug\":False,\n",
    "        \"verbose\": 1,\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Empty init. Use classmethods like .from_config().\"\"\"\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg, **kwargs):\n",
    "        \"\"\"\n",
    "        Build one TramModel per node based on configuration and kwargs.\n",
    "        Kwargs can be scalars (applied to all nodes) or dicts {node: value}.\n",
    "        \"\"\"\n",
    "        self = cls()\n",
    "        self.cfg = cfg\n",
    "        self.nodes_dict = self.cfg.conf_dict[\"nodes\"] \n",
    "\n",
    "        # merge defaults with user overrides\n",
    "        settings = dict(cls.DEFAULTS_CONFIG)\n",
    "        settings.update(kwargs)\n",
    "\n",
    "        # initialize settings storage\n",
    "        self.settings = {k: {} for k in settings.keys()}\n",
    "\n",
    "        # validate dict-typed args\n",
    "        for k, v in settings.items():\n",
    "            if isinstance(v, dict):\n",
    "                expected = set(self.nodes_dict.keys())\n",
    "                given = set(v.keys())\n",
    "                if expected != given:\n",
    "                    raise ValueError(\n",
    "                        f\"[ERROR] the provided argument '{k}' keys are not same as in cfg.conf_dict['nodes'].keys().\\n\"\n",
    "                        f\"Expected: {expected}, but got: {given}\\n\"\n",
    "                        f\"Please provide values for all variables.\"\n",
    "                    )\n",
    "\n",
    "        # build one model per node\n",
    "        self.models = {}\n",
    "        for node in self.nodes_dict.keys():\n",
    "            per_node_kwargs = {}\n",
    "            for k, v in settings.items():\n",
    "                resolved = v[node] if isinstance(v, dict) else v\n",
    "                per_node_kwargs[k] = resolved\n",
    "                self.settings[k][node] = resolved\n",
    "            print(f\"\\n[INFO] Building model for node '{node}' with settings: {per_node_kwargs}\")\n",
    "            self.models[node] = get_fully_specified_tram_model(\n",
    "                node=node,\n",
    "                configuration_dict=self.cfg.conf_dict,\n",
    "                **per_node_kwargs\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def fit(self, td_train_data, td_val_data, **kwargs):\n",
    "        \"\"\"\n",
    "        Fit TRAM models for specified nodes.\n",
    "        All kwargs can be scalar (applied to all nodes) or dict {node: value}.\n",
    "        \"\"\"\n",
    "        # merge defaults with overrides\n",
    "        settings = dict(self.DEFAULTS_FIT)\n",
    "        settings.update(kwargs)\n",
    "\n",
    "        device = torch.device(\n",
    "            \"cuda\" if (settings[\"device\"] == \"auto\" and torch.cuda.is_available()) else settings[\"device\"]\n",
    "        )\n",
    "        train_list = settings[\"train_list\"] or list(self.models.keys())\n",
    "\n",
    "        def _resolve(key, node):\n",
    "            val = settings[key]\n",
    "            return val[node] if isinstance(val, dict) else val\n",
    "\n",
    "        # store resolved settings for this fit\n",
    "        self.fit_settings = {k: {} for k in settings.keys()}\n",
    "\n",
    "        results = {}\n",
    "        for node in train_list:\n",
    "            model = self.models[node]\n",
    "\n",
    "            # resolve per-node settings\n",
    "            node_epochs = _resolve(\"epochs\", node)\n",
    "            node_lr = _resolve(\"learning_rate\", node)\n",
    "            node_debug = _resolve(\"debug\", node)\n",
    "            node_save_linear_shifts = _resolve(\"save_linear_shifts\", node)\n",
    "            node_verbose = _resolve(\"verbose\", node)\n",
    "\n",
    "            # record them\n",
    "            self.fit_settings[\"epochs\"][node] = node_epochs\n",
    "            self.fit_settings[\"learning_rate\"][node] = node_lr\n",
    "            self.fit_settings[\"debug\"][node] = node_debug\n",
    "            self.fit_settings[\"save_linear_shifts\"][node] = node_save_linear_shifts\n",
    "            self.fit_settings[\"verbose\"][node] = node_verbose\n",
    "\n",
    "            # resolve optimizer\n",
    "            if settings[\"optimizers\"] and node in settings[\"optimizers\"]:\n",
    "                optimizer = settings[\"optimizers\"][node]\n",
    "            else:\n",
    "                optimizer = Adam(model.parameters(), lr=node_lr)\n",
    "            self.fit_settings[\"optimizers\"][node] = optimizer\n",
    "\n",
    "            # resolve scheduler\n",
    "            if settings[\"schedulers\"] and node in settings[\"schedulers\"]:\n",
    "                scheduler = settings[\"schedulers\"][node]\n",
    "            else:\n",
    "                scheduler = None\n",
    "            self.fit_settings[\"schedulers\"][node] = scheduler\n",
    "\n",
    "            # grab loaders\n",
    "            train_loader = td_train_data.loaders[node]\n",
    "            val_loader = td_val_data.loaders[node]\n",
    "\n",
    "            try:\n",
    "                EXPERIMENT_DIR = self.cfg.conf_dict[\"PATHS\"][\"EXPERIMENT_DIR\"]\n",
    "                NODE_DIR = os.path.join(EXPERIMENT_DIR, f\"{node}\")\n",
    "                # print(f\"[INFO] NODE_DIR : {NODE_DIR}\")\n",
    "            except Exception:\n",
    "                NODE_DIR = os.path.join(\"models\", node)\n",
    "                print(\"[WARNING] No log directory specified in config, saving to default location.\")\n",
    "            os.makedirs(NODE_DIR, exist_ok=True)\n",
    "            self.fit_settings[\"NODE_DIR\"] = {node: NODE_DIR}\n",
    "\n",
    "            if node_verbose:\n",
    "                print(f\"\\n[INFO] Training node '{node}' for {node_epochs} epochs on {device}\")\n",
    "\n",
    "            history = train_val_loop(\n",
    "                node=node,\n",
    "                target_nodes=self.nodes_dict,\n",
    "                NODE_DIR=NODE_DIR,\n",
    "                tram_model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                epochs=node_epochs,\n",
    "                optimizer=optimizer,\n",
    "                use_scheduler=(scheduler is not None),\n",
    "                scheduler=scheduler,\n",
    "                save_linear_shifts=node_save_linear_shifts,\n",
    "                verbose=node_verbose,\n",
    "                device=device,\n",
    "                debug=node_debug,\n",
    "            )\n",
    "            results[node] = history\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    def get_latent(self, df, verbose=False):\n",
    "            \"\"\"\n",
    "            Compute latent representations for the full DAG.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            df : pd.DataFrame\n",
    "                Input dataframe with columns for each node.\n",
    "            verbose : bool, optional\n",
    "                If True, prints [INFO] statements during processing.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            pd.DataFrame\n",
    "                DataFrame with latent variables for each node. Columns are\n",
    "                [node, f\"{node}_U\"] for each continuous target.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                EXPERIMENT_DIR = self.cfg.conf_dict[\"PATHS\"][\"EXPERIMENT_DIR\"]\n",
    "            except KeyError:\n",
    "                raise ValueError(\n",
    "                    \"[ERROR] Missing 'EXPERIMENT_DIR' in cfg.conf_dict['PATHS']. \"\n",
    "                    \"Latent extraction requires trained model checkpoints.\"\n",
    "                )\n",
    "\n",
    "            all_latents_df = create_latent_df_for_full_dag(\n",
    "                configuration_dict=self.cfg.conf_dict,\n",
    "                EXPERIMENT_DIR=EXPERIMENT_DIR,\n",
    "                df=df,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "\n",
    "            return all_latents_df\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        do_interventions: dict = None,\n",
    "        predefined_latent_samples_df: pd.DataFrame = None,\n",
    "        number_of_samples: int = 10_000,\n",
    "        batch_size: int = 32,\n",
    "        delete_all_previously_sampled: bool = True,\n",
    "        verbose: bool = False,\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample from the DAG using trained TRAM models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        do_interventions : dict, optional\n",
    "            Mapping of node names to fixed values. Example: {'x1': 1.0}.\n",
    "        predefined_latent_samples_df : pd.DataFrame, optional\n",
    "            DataFrame with predefined latent U's. Must contain columns \"{node}_U\".\n",
    "        number_of_samples : int, default=10_000\n",
    "            Number of samples to draw if no predefined latents are given.\n",
    "        batch_size : int, default=32\n",
    "            Batch size for DataLoader evaluation during sampling.\n",
    "        delete_all_previously_sampled : bool, default=True\n",
    "            Whether to remove existing sampled.pt/latents.pt files before resampling.\n",
    "        verbose : bool, default=True\n",
    "            Print high-level progress messages ([INFO]).\n",
    "        debug : bool, default=False\n",
    "            Print detailed debug messages ([DEBUG]) in addition to [INFO].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sampled_by_node : dict\n",
    "            Mapping {node: tensor of sampled values}.\n",
    "        latents_by_node : dict\n",
    "            Mapping {node: tensor of latent U's used}.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            EXPERIMENT_DIR = self.cfg.conf_dict[\"PATHS\"][\"EXPERIMENT_DIR\"]\n",
    "        except KeyError:\n",
    "            raise ValueError(\n",
    "                \"[ERROR] Missing 'EXPERIMENT_DIR' in cfg.conf_dict['PATHS']. \"\n",
    "                \"Sampling requires trained model checkpoints.\"\n",
    "            )\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        sampled_by_node, latents_by_node = sample_full_dag(\n",
    "            configuration_dict=self.cfg.conf_dict,\n",
    "            EXPERIMENT_DIR=EXPERIMENT_DIR,\n",
    "            device=device,\n",
    "            do_interventions=do_interventions or {},\n",
    "            predefined_latent_samples_df=predefined_latent_samples_df,\n",
    "            number_of_samples=number_of_samples,\n",
    "            batch_size=batch_size,\n",
    "            delete_all_previously_sampled=delete_all_previously_sampled,\n",
    "            verbose=verbose,\n",
    "            debug=debug,\n",
    "        )\n",
    "        return sampled_by_node, latents_by_node\n",
    "\n",
    "    def summary(self):\n",
    "        print(\"\\n[TramDagModel Summary]\")\n",
    "        print(\"=\" * 60)\n",
    "        for node, model in self.models.items():\n",
    "            print(f\" Node '{node}': {model.__class__.__name__}\")\n",
    "            for k, v in self.settings.items():\n",
    "                if node in v:\n",
    "                    print(f\"   - {k}: {v[node]}\")\n",
    "        print(\"=\" * 60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6ba94",
   "metadata": {},
   "source": [
    "# TESTING API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5edf3e",
   "metadata": {},
   "source": [
    "#### Data (pandas dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e754cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('/home/bule/TramDag/dev_experiment_logs/exp_6_2/exp_6_2_train.csv')\n",
    "val_df=pd.read_csv('/home/bule/TramDag/dev_experiment_logs/exp_6_2/exp_6_2_val.csv')\n",
    "\n",
    "# splits "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929f1e25",
   "metadata": {},
   "source": [
    "####  Configuration\n",
    "create  a config file e.g. with create_config.ipynb\n",
    "load the file and compute necessary values on the cfg based on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e51af8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Make sure to provide only training data to compute_scaling!\n",
      "[INFO] Configuration with updated scaling saved to /home/bule/TramDag/dev_experiment_logs/exp_6_2/configuration.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = TramDagConfig.load(\"/home/bule/TramDag/dev_experiment_logs/exp_6_2/configuration.json\")\n",
    "cfg.compute_scaling(train_df) # computes min max levels from training data and writes to cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6522e8ef",
   "metadata": {},
   "source": [
    "create a dedicated TramDagDataset from the pandas dataframe and the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef60b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_train_data=TramDagDataset.from_dataframe(train_df,cfg)  \n",
    "td_val_data=TramDagDataset.from_dataframe(val_df,cfg,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e16fb00",
   "metadata": {},
   "source": [
    "create a TramdagModel from the Config file (optionally use COLR POLR as startign values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e66a42b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Building model for node 'x1' with settings: {'set_initial_weights': False, 'debug': False}\n",
      "\n",
      "[INFO] Building model for node 'x2' with settings: {'set_initial_weights': False, 'debug': False}\n",
      "\n",
      "[INFO] Building model for node 'x3' with settings: {'set_initial_weights': False, 'debug': False}\n"
     ]
    }
   ],
   "source": [
    "td_model = TramDagModel.from_config(cfg, set_initial_weights=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8b9df2",
   "metadata": {},
   "source": [
    "fit the model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5629f348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Training node 'x1' for 1 epochs on cuda\n",
      "Existing model found. Loading weights and history...\n",
      "\n",
      "[INFO] Training node 'x2' for 1 epochs on cuda\n",
      "Existing model found. Loading weights and history...\n",
      "\n",
      "[INFO] Training node 'x3' for 1 epochs on cuda\n",
      "Existing model found. Loading weights and history...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x1': None, 'x2': None, 'x3': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_model.fit( td_train_data, td_val_data, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633d0b7",
   "metadata": {},
   "source": [
    "get latents from a dateset marked as _U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7da65de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping node 'x3' (ordinal targets not yet supported).\n",
      "[INFO] Final latent DataFrame shape: (80000, 4)\n"
     ]
    }
   ],
   "source": [
    "latents_df = td_model.get_latent(train_df, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc3a56",
   "metadata": {},
   "source": [
    "sample from the whole graph , intervene or use predefined latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fa74e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted directory: /home/bule/TramDag/dev_experiment_logs/exp_6_2/x1/sampling\n",
      "Deleted directory: /home/bule/TramDag/dev_experiment_logs/exp_6_2/x2/sampling\n",
      "Deleted directory: /home/bule/TramDag/dev_experiment_logs/exp_6_2/x3/sampling\n",
      "\n",
      "----*----------*-------------*--------Sample Node: x1 ------------*-----------------*-------------------*--\n",
      "[INFO] Sampling new latents for node x1 from standard logistic distribution\n",
      "[WARNING] target_col 'x1' not in DataFrame columns â€” is this intended to be used as a Sampler?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chandrupatla root finding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:20<00:00, 483.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----*----------*-------------*--------Sample Node: x2 ------------*-----------------*-------------------*--\n",
      "[INFO] Sampling new latents for node x2 from standard logistic distribution\n",
      "[WARNING] target_col 'x2' not in DataFrame columns â€” is this intended to be used as a Sampler?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Chandrupatla root finding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:20<00:00, 499.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----*----------*-------------*--------Sample Node: x3 ------------*-----------------*-------------------*--\n",
      "[INFO] Sampling new latents for node x3 from standard logistic distribution\n",
      "[WARNING] target_col 'x3' not in DataFrame columns â€” is this intended to be used as a Sampler?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'x1': tensor([0.3483, 0.0947, 0.7013,  ..., 0.7456, 0.4402, 0.7402]),\n",
       "  'x2': tensor([-0.7723, -0.4455, -0.6446,  ..., -0.5838,  0.0118, -0.0776]),\n",
       "  'x3': tensor([3, 1, 1,  ..., 1, 1, 1])},\n",
       " {'x1': tensor([-0.3249, -3.5280,  0.5531,  ...,  1.4165, -0.0556,  1.2757]),\n",
       "  'x2': tensor([-3.1587, -2.0288, -1.7538,  ..., -1.4134,  0.9523,  1.1017]),\n",
       "  'x3': tensor([0.1774, 0.6739, 6.4358,  ..., 2.4577, 0.5640, 2.2817])})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_model.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad785f25",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a31aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TramDagConfig.load(\"/home/bule/TramDag/dev_experiment_logs/exp_6_2/configuration.json\")\n",
    "td_model = TramDagModel.from_config(cfg, set_initial_weights=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# laod model \n",
    "td_model =TramDagModel.load_from_directory(\"/home/bule/TramDag/dev_experiment_logs/exp_6_2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b854fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model.load() # load exisit trained models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb2d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=td_model.history() # # mo show_training_history(node_list,EXPERIMENT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b05610",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model.show_hdag_for_source_nodes()#show_hdag_for_source_nodes(configuration_dict,EXPERIMENT_DIR,device=device,xmin_plot=0,xmax_plot=1) # TODO for other nodes funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd443525",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model.inspect_trafo_standart_logistic()\n",
    "#inspect_trafo_standart_logistic(configuration_dict,EXPERIMENT_DIR,train_df,val_df,device,verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
