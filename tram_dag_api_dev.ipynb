{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0447acb9",
   "metadata": {},
   "source": [
    "in this file a potenitla api is developed for the tramdag "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad3fcb",
   "metadata": {},
   "source": [
    "## TRAM Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6006efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.configuration import *\n",
    "\n",
    "class TramDagConfig:\n",
    "    def __init__(self, conf_dict: dict = None, CONF_DICT_PATH: str = None, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize TramDagConfig.\n",
    "\n",
    "        Args:\n",
    "            conf_dict: optional dict with configuration. If None, starts empty.\n",
    "            CONF_DICT_PATH: optional path to config file.\n",
    "        \"\"\"\n",
    "        #TODO add verbose and debug , vebose print only infos, debug prints info + debug statements, warnings, errors are always printed\n",
    "        #TODO add veryfier such that nothing is missing for later training such as experiment name \n",
    "        \n",
    "        self.debug = False\n",
    "        self.verbose = False\n",
    "        \n",
    "        for key, value in kwargs.items():\n",
    "            if key in ['conf_dict', 'CONF_DICT_PATH']:\n",
    "                raise ValueError(f\"Cannot override '{key}' via kwargs.\")\n",
    "            setattr(self, key, value)\n",
    "        \n",
    "        self.conf_dict = conf_dict or {}\n",
    "        self.CONF_DICT_PATH = CONF_DICT_PATH\n",
    "        \n",
    "        # verification \n",
    "        self._verify_completeness()\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, CONF_DICT_PATH: str,debug: bool = False):\n",
    "        \"\"\"\n",
    "        Alternative constructor: load config directly from a file.\n",
    "        \"\"\"\n",
    "        conf = load_configuration_dict(CONF_DICT_PATH)\n",
    "        return cls(conf, CONF_DICT_PATH=CONF_DICT_PATH,debug=debug)\n",
    "\n",
    "    def save(self, CONF_DICT_PATH: str = None):\n",
    "        \"\"\"\n",
    "        Save config to file. If path is not provided, fall back to stored path.\n",
    "        \"\"\"\n",
    "        path = CONF_DICT_PATH or self.CONF_DICT_PATH\n",
    "        if path is None:\n",
    "            raise ValueError(\"No CONF_DICT_PATH provided to save config.\")\n",
    "        write_configuration_dict(self.conf_dict, path)\n",
    "\n",
    "\n",
    "    def _verify_completeness(self):\n",
    "        \"\"\"\n",
    "        Verify that the configuration is complete and consistent:\n",
    "        - All mandatory keys exist\n",
    "        - Mandatory keys have valid values\n",
    "        - Optional keys (if present) are valid\n",
    "        \"\"\"\n",
    "        mandatory_keys = [\"experiment_name\",\"PATHS\", \"nodes\", \"data_type\", \"adj_matrix\",\"nodes\",\"model_names\"]\n",
    "        optional_keys = [\"date_of_creation\", \"seed\"]\n",
    "\n",
    "        # ---- 1. Check mandatory keys exist\n",
    "        missing = [k for k in mandatory_keys if k not in self.conf_dict]\n",
    "        if missing:\n",
    "            print(f\"[WARNING] Missing mandatory keys in configuration: {missing}\"\n",
    "                \"\\n Please add them to the configuration dict and reload.\")\n",
    "            \n",
    "        # --- 2. Check  if mandatory keys in nodesdict are present\n",
    "        mandatory_keys_nodes = ['data_type', 'node_type','parents','parents_datatype','transformation_terms_in_h()','transformation_term_nn_models_in_h()']\n",
    "        optional_keys_nodes = [\"levels\"]\n",
    "        for node, node_dict in self.conf_dict.get(\"nodes\", {}).items():\n",
    "            # check missing mandatory keys\n",
    "            missing_node_keys = [k for k in mandatory_keys_nodes if k not in node_dict]\n",
    "            if missing_node_keys:\n",
    "                print(f\"[WARNING] Node '{node}' is missing mandatory keys: {missing_node_keys}\")\n",
    "                \n",
    "\n",
    "        \n",
    "        if self._verify_levels_dict():\n",
    "            if self.debug:\n",
    "                print(\"[DEBUG] levels are present for all ordinal variables in configuration dict.\")\n",
    "            pass\n",
    "        else:\n",
    "            print(\"[WARNING] levels are missing for some ordinal variables in configuration dict. THIS will FAIL in model training later!\\n\"\n",
    "                \" Please provide levels manually to config and reload or compute levels from data using the method compute_levels().\\n\"\n",
    "                \" e.g. cfg.compute_levels(train_df) # computes levels from training data and writes to cfg\")\n",
    "\n",
    "        if self._verify_experiment_name():\n",
    "            if self.debug:\n",
    "                print(\"[DEBUG] experiment_name is valid in configuration dict.\")\n",
    "            pass\n",
    "        \n",
    "        if self._verify_adj_matrix():\n",
    "            if self.debug:\n",
    "                print(\"[DEBUG] adj_matrix is valid in configuration dict.\")\n",
    "            pass\n",
    "\n",
    "    def _verify_levels_dict(self):\n",
    "        \"\"\"\n",
    "        Verify that levels_dict is present for all categorical variables.\n",
    "        \"\"\"\n",
    "        data_type = self.conf_dict.get('data_type', {})\n",
    "        nodes = self.conf_dict.get('nodes', {})\n",
    "        for var, dtype in data_type.items():\n",
    "            if 'ordinal' in dtype:\n",
    "                if var not in nodes or 'levels' not in nodes[var]:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def _verify_experiment_name(self):\n",
    "        experiment_name = self.conf_dict.get(\"experiment_name\")\n",
    "        if experiment_name is None or str(experiment_name).strip() == \"\":\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "    def _verify_adj_matrix(self):\n",
    "        adj_matrix = self.conf_dict['adj_matrix']\n",
    "        if isinstance(adj_matrix, list):\n",
    "            adj_matrix = np.array(self.conf_dict['adj_matrix'])\n",
    "        if validate_adj_matrix(adj_matrix):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def compute_levels(self, df: pd.DataFrame, write: bool = True):\n",
    "        \"\"\"\n",
    "        Derive levels information from data and update configuration dict.\n",
    "        \"\"\"\n",
    "        levels_dict = create_levels_dict(df, self.conf_dict['data_type'])\n",
    "        \n",
    "        # update nodes dict with levels\n",
    "        for var, levels in levels_dict.items():\n",
    "            if var in self.conf_dict['nodes']:\n",
    "                self.conf_dict['nodes'][var]['levels'] = levels\n",
    "            else:\n",
    "                print(f\"[WARNING] Variable '{var}' not found in nodes dict. Cannot add levels.\")\n",
    "        \n",
    "        if write and self.CONF_DICT_PATH is not None:\n",
    "            try:\n",
    "                self.save(self.CONF_DICT_PATH)\n",
    "                if self.verbose or self.debug:\n",
    "                    print(f'[INFO] Configuration with updated levels saved to {self.CONF_DICT_PATH}')\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to save configuration: {e}')\n",
    "\n",
    "    def plot_dag(self, seed: int = 42, use_spring: bool = True):\n",
    "        \"\"\"\n",
    "        Plot the DAG with Source, Sink, and Intermediate nodes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : int, default=42\n",
    "            Random seed for layout stability.\n",
    "        use_spring : bool, default=True\n",
    "            If True use networkx.spring_layout; \n",
    "            if False try Graphviz “dot” (falls back to spring).\n",
    "        \"\"\"\n",
    "        adj_matrix = self.conf_dict.get(\"adj_matrix\")\n",
    "        data_type  = self.conf_dict.get(\"data_type\")\n",
    "\n",
    "        if adj_matrix is None or data_type is None:\n",
    "            raise ValueError(\"Configuration must include 'adj_matrix' and 'data_type'.\")\n",
    "\n",
    "        # convert list to numpy if needed\n",
    "        if isinstance(adj_matrix, list):\n",
    "            adj_matrix = np.array(adj_matrix)\n",
    "\n",
    "        if not validate_adj_matrix(adj_matrix):\n",
    "            raise ValueError(\"Invalid adjacency matrix.\")\n",
    "        if len(data_type) != adj_matrix.shape[0]:\n",
    "            raise ValueError(\"data_type must match adjacency matrix size.\")\n",
    "\n",
    "        node_labels = list(data_type.keys())\n",
    "        G, edge_labels = create_nx_graph(adj_matrix, node_labels)\n",
    "\n",
    "        # classify nodes\n",
    "        sources       = {n for n in G.nodes if G.in_degree(n) == 0}\n",
    "        sinks         = {n for n in G.nodes if G.out_degree(n) == 0}\n",
    "        intermediates = set(G.nodes) - sources - sinks\n",
    "\n",
    "        # assign node colors\n",
    "        node_colors = [\n",
    "            \"green\" if n in sources\n",
    "            else \"red\" if n in sinks\n",
    "            else \"lightblue\"\n",
    "            for n in G.nodes\n",
    "        ]\n",
    "\n",
    "        # choose layout\n",
    "        if use_spring:\n",
    "            pos = nx.spring_layout(G, seed=seed, k=1.5, iterations=100)\n",
    "        else:\n",
    "            try:\n",
    "                pos = nx.nx_agraph.graphviz_layout(G, prog=\"dot\")\n",
    "            except (ImportError, nx.NetworkXException):\n",
    "                pos = nx.spring_layout(G, seed=seed, k=1.5, iterations=100)\n",
    "\n",
    "        # draw nodes and edges\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        nx.draw(\n",
    "            G, pos,\n",
    "            with_labels=True,\n",
    "            node_color=node_colors,\n",
    "            edge_color=\"gray\",\n",
    "            node_size=2500,\n",
    "            arrowsize=20\n",
    "        )\n",
    "\n",
    "        # draw edge labels colored by prefix\n",
    "        for (u, v), lbl in edge_labels.items():\n",
    "            color = (\n",
    "                \"blue\"  if lbl.startswith(\"ci\")\n",
    "                else \"red\"   if lbl.startswith(\"ls\")\n",
    "                else \"green\" if lbl.startswith(\"cs\")\n",
    "                else \"black\"\n",
    "            )\n",
    "            nx.draw_networkx_edge_labels(\n",
    "                G, pos,\n",
    "                edge_labels={(u, v): lbl},\n",
    "                font_color=color,\n",
    "                font_size=12\n",
    "            )\n",
    "\n",
    "        # build legend\n",
    "        legend_items = [\n",
    "            Patch(facecolor=\"green\",     edgecolor=\"black\", label=\"Source\"),\n",
    "            Patch(facecolor=\"red\",       edgecolor=\"black\", label=\"Sink\"),\n",
    "            Patch(facecolor=\"lightblue\", edgecolor=\"black\", label=\"Intermediate\")\n",
    "        ]\n",
    "        plt.legend(handles=legend_items, loc=\"upper right\", frameon=True)\n",
    "\n",
    "        plt.title(\"TRAM DAG\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04f8d0",
   "metadata": {},
   "source": [
    "# TramDagDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6d9bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from utils.tram_data import GenericDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TramDagDataset(Dataset):\n",
    "    \n",
    "    #TODO add docstring\n",
    "    #TODO add verbose and debug , vebose print only infos, debug prints info + debug statements, warnings, errors are always printed\n",
    "    #TODO add veryfier such that nothing is missing for later training such as experiment name \n",
    "    \n",
    "    DEFAULTS = {\n",
    "        \"batch_size\": 32,\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 4,\n",
    "        \"pin_memory\": False,\n",
    "        \"return_intercept_shift\": True,\n",
    "        \"debug\": False,\n",
    "        \"transform\": None,\n",
    "        \"use_dataloader\": True,\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Empty init. Use classmethods like .from_dataframe().\"\"\"\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df, cfg, **kwargs):\n",
    "        self = cls()\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(f\"[ERROR] df must be a pandas DataFrame, but got {type(df)}\")\n",
    "\n",
    "        # merge defaults with overrides\n",
    "        settings = dict(cls.DEFAULTS)\n",
    "        settings.update(kwargs)\n",
    "\n",
    "        # infer variable name automatically\n",
    "        callers_locals = inspect.currentframe().f_back.f_locals\n",
    "        inferred = None\n",
    "        for var_name, var_val in callers_locals.items():\n",
    "            if var_val is df:\n",
    "                inferred = var_name\n",
    "                break\n",
    "        df_name = inferred or \"dataframe\"\n",
    "\n",
    "        if settings[\"shuffle\"]:\n",
    "            if any(x in df_name.lower() for x in [\"val\", \"validation\", \"test\"]):\n",
    "                print(f\"[WARNING] DataFrame '{df_name}' looks like a validation/test set → shuffle=True. Are you sure?\")\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.df = df.copy()\n",
    "        self._apply_settings(settings)\n",
    "        self._build_dataloaders()\n",
    "        return self\n",
    "\n",
    "    def compute_scaling(self, df: pd.DataFrame=None, write: bool = True):\n",
    "        \"\"\"\n",
    "        Derive scaling information (min, max, levels) from data USE training data.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(\"[DEBUG] Make sure to provide only training data to compute_scaling!\")     \n",
    "        if df is None:\n",
    "            df = self.df\n",
    "            if self.debug:\n",
    "                print(\"[DEBUG] No DataFrame provided, using internal df.\")\n",
    "        quantiles = df.quantile([0.05, 0.95])\n",
    "        min_vals = quantiles.loc[0.05]\n",
    "        max_vals = quantiles.loc[0.95]\n",
    "        minmax_dict = pd.concat([min_vals, max_vals], axis=1).T.to_dict('list')\n",
    "        return minmax_dict\n",
    "\n",
    "    def _apply_settings(self, settings: dict):\n",
    "        self.batch_size = settings[\"batch_size\"]\n",
    "        self.shuffle = settings[\"shuffle\"]\n",
    "        self.num_workers = settings[\"num_workers\"]\n",
    "        self.pin_memory = settings[\"pin_memory\"]\n",
    "        self.return_intercept_shift = settings[\"return_intercept_shift\"]\n",
    "        self.debug = settings[\"debug\"]\n",
    "        self.transform = settings[\"transform\"]\n",
    "        self.use_dataloader = settings[\"use_dataloader\"]   \n",
    "        self.nodes_dict = self.cfg.conf_dict[\"nodes\"]\n",
    "\n",
    "        # validate dict attributes\n",
    "        for name, val in {\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"shuffle\": self.shuffle,\n",
    "            \"num_workers\": self.num_workers,\n",
    "            \"pin_memory\": self.pin_memory,\n",
    "            \"return_intercept_shift\": self.return_intercept_shift,\n",
    "            \"debug\": self.debug,\n",
    "            \"transform\": self.transform,\n",
    "        }.items():\n",
    "            self._check_keys(name, val)\n",
    "\n",
    "    def _build_dataloaders(self):\n",
    "        \"\"\"Build node-specific dataloaders or raw datasets depending on settings.\"\"\"\n",
    "        self.loaders = {}\n",
    "        for node in self.nodes_dict:\n",
    "            ds = GenericDataset(\n",
    "                self.df,\n",
    "                target_col=node,\n",
    "                target_nodes=self.nodes_dict,\n",
    "                transform=self.transform if not isinstance(self.transform, dict) else self.transform[node],\n",
    "                return_intercept_shift=self.return_intercept_shift if not isinstance(self.return_intercept_shift, dict) else self.return_intercept_shift[node],\n",
    "                debug=self.debug if not isinstance(self.debug, dict) else self.debug[node],\n",
    "            )\n",
    "\n",
    "            if self.use_dataloader:\n",
    "                batch_size = self.batch_size[node] if isinstance(self.batch_size, dict) else self.batch_size\n",
    "                shuffle_flag = self.shuffle[node] if isinstance(self.shuffle, dict) else bool(self.shuffle)\n",
    "                num_workers = self.num_workers[node] if isinstance(self.num_workers, dict) else self.num_workers\n",
    "                pin_memory = self.pin_memory[node] if isinstance(self.pin_memory, dict) else self.pin_memory\n",
    "\n",
    "                self.loaders[node] = DataLoader(\n",
    "                    ds,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=shuffle_flag,\n",
    "                    num_workers=num_workers,\n",
    "                    pin_memory=pin_memory,\n",
    "                )\n",
    "            else:\n",
    "                # just keep raw dataset\n",
    "                self.loaders[node] = ds\n",
    "\n",
    "\n",
    "    def _check_keys(self, attr_name, attr_value):\n",
    "        \"\"\"Check if dict keys match cfg.conf_dict['nodes'].keys().\"\"\"\n",
    "        if isinstance(attr_value, dict):\n",
    "            expected_keys = set(self.nodes_dict.keys())\n",
    "            given_keys = set(attr_value.keys())\n",
    "            if expected_keys != given_keys:\n",
    "                raise ValueError(\n",
    "                    f\"[ERROR] the provided attribute '{attr_name}' keys are not same as in cfg.conf_dict['nodes'].keys().\\n\"\n",
    "                    f\"Expected: {expected_keys}, but got: {given_keys}\\n\"\n",
    "                    f\"Please provide values for all variables.\"\n",
    "                )\n",
    "\n",
    "    def summary(self):\n",
    "        print(\"\\n[TramDagDataset Summary]\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # ---- DataFrame section ----\n",
    "        print(\"\\n[DataFrame]\")\n",
    "        print(\"Shape:\", self.df.shape)\n",
    "        print(\"\\nHead:\")\n",
    "        print(self.df.head())\n",
    "\n",
    "        print(\"\\nDtypes:\")\n",
    "        print(self.df.dtypes)\n",
    "\n",
    "        print(\"\\nDescribe:\")\n",
    "        print(self.df.describe(include=\"all\"))\n",
    "\n",
    "        # ---- Settings per node ----\n",
    "        print(\"\\n[Node Settings]\")\n",
    "        for node in self.nodes_dict.keys():\n",
    "            batch_size = self.batch_size[node] if isinstance(self.batch_size, dict) else self.batch_size\n",
    "            shuffle_flag = self.shuffle[node] if isinstance(self.shuffle, dict) else bool(self.shuffle)\n",
    "            num_workers = self.num_workers[node] if isinstance(self.num_workers, dict) else self.num_workers\n",
    "            pin_memory = self.pin_memory[node] if isinstance(self.pin_memory, dict) else self.pin_memory\n",
    "            rshift = self.return_intercept_shift[node] if isinstance(self.return_intercept_shift, dict) else self.return_intercept_shift\n",
    "            debug_flag = self.debug[node] if isinstance(self.debug, dict) else self.debug\n",
    "            transform = self.transform[node] if isinstance(self.transform, dict) else self.transform\n",
    "\n",
    "            print(\n",
    "                f\" Node '{node}': \"\n",
    "                f\"batch_size={batch_size}, \"\n",
    "                f\"shuffle={shuffle_flag}, \"\n",
    "                f\"num_workers={num_workers}, \"\n",
    "                f\"pin_memory={pin_memory}, \"\n",
    "                f\"return_intercept_shift={rshift}, \"\n",
    "                f\"debug={debug_flag}, \"\n",
    "                f\"transform={transform}\"\n",
    "            )\n",
    "        print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.df.iloc[idx].to_dict()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1a9528",
   "metadata": {},
   "source": [
    "# TramDagModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39dd683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tram_model_helpers import train_val_loop, get_fully_specified_tram_model \n",
    "from utils.tram_data_helpers import create_latent_df_for_full_dag, sample_full_dag\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "class TramDagModel:\n",
    "    \n",
    "    #TODO add docstring\n",
    "    #TODO add verbose and debug , vebose print only infos, debug prints info + debug statements, warnings, errors are always printed\n",
    "    # ---- defaults used at construction time ----\n",
    "    DEFAULTS_CONFIG = {\n",
    "        \"set_initial_weights\": True,\n",
    "        \"debug\":False,\n",
    "        \n",
    "    }\n",
    "\n",
    "    # ---- defaults used at fit() time ----\n",
    "    DEFAULTS_FIT = {\n",
    "        \"epochs\": 100,\n",
    "        \"train_list\": None,\n",
    "        \"callbacks\": None,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"device\": \"auto\",\n",
    "        \"optimizers\": None,\n",
    "        \"schedulers\": None,\n",
    "        \"use_scheduler\": False,\n",
    "        \"save_linear_shifts\": True,\n",
    "        \"debug\":False,\n",
    "        \"verbose\": True,\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Empty init. Use classmethods like .from_config().\"\"\"\n",
    "        self.debug = False\n",
    "        self.verbose = False\n",
    "        self.device = None\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg, **kwargs):\n",
    "        \"\"\"\n",
    "        Build one TramModel per node based on configuration and kwargs.\n",
    "        Kwargs can be scalars (applied to all nodes) or dicts {node: value}.\n",
    "        \"\"\"\n",
    "        self = cls()\n",
    "        self.cfg = cfg\n",
    "        self.nodes_dict = self.cfg.conf_dict[\"nodes\"] \n",
    "\n",
    "        # resolve device\n",
    "        device_arg = kwargs.get(\"device\", \"auto\")\n",
    "        if device_arg == \"auto\":\n",
    "            device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            device_str = device_arg\n",
    "        self.device = torch.device(device_str)\n",
    "            \n",
    "        # merge defaults with user overrides\n",
    "        settings = dict(cls.DEFAULTS_CONFIG)\n",
    "        settings.update(kwargs)\n",
    "\n",
    "        # set flags on the instance so they are accessible later\n",
    "        self.debug = settings.get(\"debug\", False)\n",
    "        self.verbose = settings.get(\"verbose\", False)\n",
    "\n",
    "        if self.verbose or self.debug:\n",
    "            print(f\"[INFO] TramDagModel using device: {self.device}\")\n",
    "            \n",
    "        # initialize settings storage\n",
    "        self.settings = {k: {} for k in settings.keys()}\n",
    "\n",
    "        # validate dict-typed args\n",
    "        for k, v in settings.items():\n",
    "            if isinstance(v, dict):\n",
    "                expected = set(self.nodes_dict.keys())\n",
    "                given = set(v.keys())\n",
    "                if expected != given:\n",
    "                    raise ValueError(\n",
    "                        f\"[ERROR] the provided argument '{k}' keys are not same as in cfg.conf_dict['nodes'].keys().\\n\"\n",
    "                        f\"Expected: {expected}, but got: {given}\\n\"\n",
    "                        f\"Please provide values for all variables.\"\n",
    "                    )\n",
    "\n",
    "        # build one model per node\n",
    "        self.models = {}\n",
    "        for node in self.nodes_dict.keys():\n",
    "            per_node_kwargs = {}\n",
    "            for k, v in settings.items():\n",
    "                if k == \"device\":   # skip device, not for get_fully_specified_tram_model\n",
    "                    continue\n",
    "                resolved = v[node] if isinstance(v, dict) else v\n",
    "                per_node_kwargs[k] = resolved\n",
    "                self.settings[k][node] = resolved\n",
    "            if self.debug or self.verbose:\n",
    "                print(f\"\\n[INFO] Building model for node '{node}' with settings: {per_node_kwargs}\")\n",
    "            self.models[node] = get_fully_specified_tram_model(\n",
    "                node=node,\n",
    "                configuration_dict=self.cfg.conf_dict,\n",
    "                **per_node_kwargs\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    @classmethod\n",
    "    def from_directory(cls, experiment_dir: str, device: str = \"auto\", debug: bool = False, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Reconstruct a TramDagModel from an experiment directory.\n",
    "\n",
    "        This loads:\n",
    "        - The configuration file (config.json).\n",
    "        - The minmax scaling file (min_max_scaling.json).\n",
    "        - Initializes all per-node models (like from_config).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        experiment_dir : str\n",
    "            Path to the experiment directory containing `config.json` and `min_max_scaling.json`.\n",
    "        device : str, optional\n",
    "            Device string (\"cpu\", \"cuda\", or \"auto\"). Default is \"auto\".\n",
    "        debug : bool, optional\n",
    "            Enable debug printing. Default = False.\n",
    "        verbose : bool, optional\n",
    "            Enable info printing. Default = True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        TramDagModel\n",
    "            A fully initialized TramDagModel with config and minmax loaded.\n",
    "        \"\"\"\n",
    "\n",
    "        # --- load config file ---\n",
    "        config_path = os.path.join(experiment_dir, \"config.json\")\n",
    "        if not os.path.exists(config_path):\n",
    "            raise FileNotFoundError(f\"[ERROR] Config file not found at {config_path}\")\n",
    "\n",
    "        with open(config_path, \"r\") as f:\n",
    "            cfg_dict = json.load(f)\n",
    "\n",
    "        # Create TramConfig wrapper (adjust if your cfg is a dict already)\n",
    "        cfg = TramDagConfig(cfg_dict)\n",
    "\n",
    "        # --- build model from config ---\n",
    "        self = cls.from_config(cfg, device=device, debug=debug, verbose=verbose)\n",
    "\n",
    "        # --- load minmax scaling ---\n",
    "        minmax_path = os.path.join(experiment_dir, \"min_max_scaling.json\")\n",
    "        if not os.path.exists(minmax_path):\n",
    "            raise FileNotFoundError(f\"[ERROR] MinMax file not found at {minmax_path}\")\n",
    "\n",
    "        with open(minmax_path, \"r\") as f:\n",
    "            self.minmax_dict = json.load(f)\n",
    "\n",
    "        if self.verbose or self.debug:\n",
    "            print(f\"[INFO] Loaded TramDagModel from {experiment_dir}\")\n",
    "            print(f\"[INFO] Config loaded from {config_path}\")\n",
    "            print(f\"[INFO] MinMax scaling loaded from {minmax_path}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def load_or_compute_minmax(self, use_existing=False, write=True,td_train_data=None):\n",
    "        EXPERIMENT_DIR = self.cfg.conf_dict[\"PATHS\"][\"EXPERIMENT_DIR\"]\n",
    "        minmax_path = os.path.join(EXPERIMENT_DIR, \"min_max_scaling.json\")\n",
    "\n",
    "        # laod exisitng if possible\n",
    "        if use_existing:\n",
    "            if not os.path.exists(minmax_path):\n",
    "                raise FileNotFoundError(f\"MinMax file not found: {minmax_path}\")\n",
    "            try:\n",
    "                with open(minmax_path, 'r') as f:\n",
    "                    self.minmax_dict = json.load(f)\n",
    "                if self.debug or self.verbose:\n",
    "                    print(f\"[INFO] Loaded existing minmax dict from {minmax_path}\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Could not load existing minmax dict: {e}\")\n",
    "\n",
    "        # \n",
    "        if self.debug or self.verbose:\n",
    "            print(\"[INFO] Computing new minmax dict from training data...\")\n",
    "        self.minmax_dict = td_train_data.compute_scaling()\n",
    "\n",
    "        if write:\n",
    "            os.makedirs(EXPERIMENT_DIR, exist_ok=True)\n",
    "            with open(minmax_path, 'w') as f:\n",
    "                json.dump(self.minmax_dict, f, indent=4)\n",
    "            if self.debug or self.verbose:\n",
    "                print(f\"[INFO] Saved new minmax dict to {minmax_path}\")\n",
    "\n",
    "\n",
    "    def fit(self, train_data, val_data=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Fit TRAM models for specified nodes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data : pd.DataFrame or TramDagDataset\n",
    "            Training data, either as a raw dataframe or as a TramDagDataset.\n",
    "        val_data : pd.DataFrame or TramDagDataset, optional\n",
    "            Validation data, either as a raw dataframe or as a TramDagDataset.\n",
    "        kwargs : dict\n",
    "            Overrides for DEFAULTS_FIT (epochs, learning_rate, device, etc.).\n",
    "        \"\"\"\n",
    "\n",
    "        # --- convert to TramDagDataset if needed ---\n",
    "        if isinstance(train_data, pd.DataFrame):\n",
    "            td_train_data = TramDagDataset.from_dataframe(train_data, self.cfg)\n",
    "        elif isinstance(train_data, TramDagDataset):\n",
    "            td_train_data = train_data\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f\"[ERROR] train_data must be pd.DataFrame or TramDagDataset, got {type(train_data)}\"\n",
    "            )\n",
    "\n",
    "        if isinstance(val_data, pd.DataFrame):\n",
    "            td_val_data = TramDagDataset.from_dataframe(val_data, self.cfg, shuffle=False)\n",
    "        elif isinstance(val_data, TramDagDataset):\n",
    "            td_val_data = val_data\n",
    "        elif val_data is None:\n",
    "            td_val_data = None\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f\"[ERROR] val_data must be pd.DataFrame, TramDagDataset, or None, got {type(val_data)}\"\n",
    "            )\n",
    "\n",
    "        # --- merge defaults with overrides ---\n",
    "        settings = dict(self.DEFAULTS_FIT)\n",
    "        settings.update(kwargs)\n",
    "\n",
    "        # --- resolve device ---\n",
    "        device_arg = settings.get(\"device\", \"auto\")\n",
    "        if device_arg == \"auto\":\n",
    "            device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            device_str = device_arg\n",
    "        self.device = torch.device(device_str)\n",
    "        device = self.device\n",
    "\n",
    "        # which nodes to train , default all nodes\n",
    "        train_list = settings[\"train_list\"] or list(self.models.keys())\n",
    "\n",
    "        def _resolve(key, node):\n",
    "            val = settings[key]\n",
    "            return val[node] if isinstance(val, dict) else val\n",
    "\n",
    "        # store resolved settings for this fit\n",
    "        self.fit_settings = {k: {} for k in settings.keys()}\n",
    "\n",
    "        # calculate scaling from training data and write to EXPERIMENT_DIR \n",
    "        self.load_or_compute_minmax(use_existing=False, write=True, td_train_data=td_train_data)\n",
    "\n",
    "        results = {}\n",
    "        for node in train_list:\n",
    "            model = self.models[node]\n",
    "\n",
    "            # resolve per-node settings\n",
    "            node_epochs = _resolve(\"epochs\", node)\n",
    "            node_lr = _resolve(\"learning_rate\", node)\n",
    "            node_debug = _resolve(\"debug\", node)\n",
    "            node_save_linear_shifts = _resolve(\"save_linear_shifts\", node)\n",
    "            node_verbose = _resolve(\"verbose\", node)\n",
    "\n",
    "            # record them\n",
    "            self.fit_settings[\"epochs\"][node] = node_epochs\n",
    "            self.fit_settings[\"learning_rate\"][node] = node_lr\n",
    "            self.fit_settings[\"debug\"][node] = node_debug\n",
    "            self.fit_settings[\"save_linear_shifts\"][node] = node_save_linear_shifts\n",
    "            self.fit_settings[\"verbose\"][node] = node_verbose\n",
    "\n",
    "            # resolve optimizer\n",
    "            if settings[\"optimizers\"] and node in settings[\"optimizers\"]:\n",
    "                optimizer = settings[\"optimizers\"][node]\n",
    "            else:\n",
    "                optimizer = Adam(model.parameters(), lr=node_lr)\n",
    "            self.fit_settings[\"optimizers\"][node] = optimizer\n",
    "\n",
    "            # resolve scheduler\n",
    "            if settings[\"schedulers\"] and node in settings[\"schedulers\"]:\n",
    "                scheduler = settings[\"schedulers\"][node]\n",
    "            else:\n",
    "                scheduler = None\n",
    "            self.fit_settings[\"schedulers\"][node] = scheduler\n",
    "\n",
    "            # grab loaders\n",
    "            train_loader = td_train_data.loaders[node]\n",
    "            val_loader = td_val_data.loaders[node] if td_val_data else None\n",
    "\n",
    "            # min max for scaling\n",
    "            min_vals = torch.tensor(self.minmax_dict[node][0], dtype=torch.float32, device=device)\n",
    "            max_vals = torch.tensor(self.minmax_dict[node][1], dtype=torch.float32, device=device)\n",
    "            min_max = torch.stack([min_vals, max_vals], dim=0)\n",
    "\n",
    "            try:\n",
    "                EXPERIMENT_DIR = self.cfg.conf_dict[\"PATHS\"][\"EXPERIMENT_DIR\"]\n",
    "                NODE_DIR = os.path.join(EXPERIMENT_DIR, f\"{node}\")\n",
    "            except Exception:\n",
    "                NODE_DIR = os.path.join(\"models\", node)\n",
    "                print(\"[WARNING] No log directory specified in config, saving to default location.\")\n",
    "\n",
    "            os.makedirs(NODE_DIR, exist_ok=True)\n",
    "            self.fit_settings[\"NODE_DIR\"] = {node: NODE_DIR}\n",
    "\n",
    "            if node_verbose:\n",
    "                print(f\"\\n[INFO] Training node '{node}' for {node_epochs} epochs on {device}\")\n",
    "\n",
    "            history = train_val_loop(\n",
    "                node=node,\n",
    "                target_nodes=self.nodes_dict,\n",
    "                NODE_DIR=NODE_DIR,\n",
    "                tram_model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                epochs=node_epochs,\n",
    "                optimizer=optimizer,\n",
    "                use_scheduler=(scheduler is not None),\n",
    "                scheduler=scheduler,\n",
    "                save_linear_shifts=node_save_linear_shifts,\n",
    "                verbose=node_verbose,\n",
    "                device=device,\n",
    "                debug=node_debug,\n",
    "                min_max=min_max\n",
    "            )\n",
    "\n",
    "            results[node] = history\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "    def get_latent(self, df, verbose=False):\n",
    "            \"\"\"\n",
    "            Compute latent representations for the full DAG.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            df : pd.DataFrame\n",
    "                Input dataframe with columns for each node.\n",
    "            verbose : bool, optional\n",
    "                If True, prints [INFO] statements during processing.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            pd.DataFrame\n",
    "                DataFrame with latent variables for each node. Columns are\n",
    "                [node, f\"{node}_U\"] for each continuous target.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                EXPERIMENT_DIR = self.cfg.conf_dict[\"PATHS\"][\"EXPERIMENT_DIR\"]\n",
    "            except KeyError:\n",
    "                raise ValueError(\n",
    "                    \"[ERROR] Missing 'EXPERIMENT_DIR' in cfg.conf_dict['PATHS']. \"\n",
    "                    \"Latent extraction requires trained model checkpoints.\"\n",
    "                )\n",
    "\n",
    "            all_latents_df = create_latent_df_for_full_dag(\n",
    "                configuration_dict=self.cfg.conf_dict,\n",
    "                EXPERIMENT_DIR=EXPERIMENT_DIR,\n",
    "                df=df,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "\n",
    "            return all_latents_df\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        do_interventions: dict = None,\n",
    "        predefined_latent_samples_df: pd.DataFrame = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample from the DAG using trained TRAM models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        do_interventions : dict, optional\n",
    "            Mapping of node names to fixed values. Example: {'x1': 1.0}.\n",
    "        predefined_latent_samples_df : pd.DataFrame, optional\n",
    "            DataFrame with predefined latent U's. Must contain columns \"{node}_U\".\n",
    "        kwargs : dict\n",
    "            Overrides for default settings (number_of_samples, batch_size, device, etc.).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sampled_by_node : dict\n",
    "            Mapping {node: tensor of sampled values}.\n",
    "        latents_by_node : dict\n",
    "            Mapping {node: tensor of latent U's used}.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            EXPERIMENT_DIR = self.cfg.conf_dict[\"PATHS\"][\"EXPERIMENT_DIR\"]\n",
    "        except KeyError:\n",
    "            raise ValueError(\n",
    "                \"[ERROR] Missing 'EXPERIMENT_DIR' in cfg.conf_dict['PATHS']. \"\n",
    "                \"Sampling requires trained model checkpoints.\"\n",
    "            )\n",
    "\n",
    "        # ---- defaults ----\n",
    "        settings = {\n",
    "            \"number_of_samples\": 10_000,\n",
    "            \"batch_size\": 32,\n",
    "            \"delete_all_previously_sampled\": True,\n",
    "            \"verbose\": False,\n",
    "            \"debug\": False,\n",
    "            \"device\": self.device.type if hasattr(self, \"device\") else \"auto\",\n",
    "        }\n",
    "        settings.update(kwargs)\n",
    "\n",
    "        \n",
    "        if not hasattr(self, \"minmax_dict\"):\n",
    "            raise RuntimeError(\n",
    "                \"[ERROR] minmax_dict not found. You must call .fit() or .load_or_compute_minmax() \"\n",
    "                \"before sampling, so scaling info is available.\"\n",
    "                )\n",
    "            \n",
    "        # ---- resolve device ----\n",
    "        device_arg = settings[\"device\"]\n",
    "        if device_arg == \"auto\":\n",
    "            device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            device_str = device_arg\n",
    "        self.device = torch.device(device_str)\n",
    "        device = self.device\n",
    "\n",
    "        # ---- perform sampling ----\n",
    "        sampled_by_node, latents_by_node = sample_full_dag(\n",
    "            configuration_dict=self.cfg.conf_dict,\n",
    "            EXPERIMENT_DIR=EXPERIMENT_DIR,\n",
    "            device=device,\n",
    "            do_interventions=do_interventions or {},\n",
    "            predefined_latent_samples_df=predefined_latent_samples_df,\n",
    "            number_of_samples=settings[\"number_of_samples\"],\n",
    "            batch_size=settings[\"batch_size\"],\n",
    "            delete_all_previously_sampled=settings[\"delete_all_previously_sampled\"],\n",
    "            verbose=settings[\"verbose\"],\n",
    "            debug=settings[\"debug\"],\n",
    "            minmax_dict=self.minmax_dict,\n",
    "        )\n",
    "\n",
    "        return sampled_by_node, latents_by_node\n",
    "\n",
    "    def summary(self):\n",
    "        print(\"\\n[TramDagModel Summary]\")\n",
    "        print(\"=\" * 60)\n",
    "        for node, model in self.models.items():\n",
    "            print(f\" Node '{node}': {model.__class__.__name__}\")\n",
    "            for k, v in self.settings.items():\n",
    "                if node in v:\n",
    "                    print(f\"   - {k}: {v[node]}\")\n",
    "        print(\"=\" * 60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e754cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('/home/bule/TramDag/dev_experiment_logs/exp_6_2/exp_6_2_train.csv')\n",
    "val_df=pd.read_csv('/home/bule/TramDag/dev_experiment_logs/exp_6_2/exp_6_2_val.csv')\n",
    "# splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e51af8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TramDagConfig.load(\"/home/bule/TramDag/dev_experiment_logs/exp_6_2/configuration.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6522e8ef",
   "metadata": {},
   "source": [
    "create a dedicated TramDagDataset from the pandas dataframe and the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef60b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# td_train_data=TramDagDataset.from_dataframe(train_df,cfg,device='auto',debug=True)  \n",
    "# td_val_data=TramDagDataset.from_dataframe(val_df,cfg,shuffle=False,device='auto',debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e16fb00",
   "metadata": {},
   "source": [
    "create a TramdagModel from the Config file (optionally use COLR POLR as startign values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e66a42b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model = TramDagModel.from_config(cfg, set_initial_weights=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8b9df2",
   "metadata": {},
   "source": [
    "fit the model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5629f348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Training node 'x1' for 519 epochs on cuda\n",
      "[INFO] Existing model found. Loading weights and history...\n",
      "Epoch 519/519  Train NLL: -0.5744  Val NLL: -0.5785  [Train: 25.03s  Val: 1.72s  Total: 26.75s]\n",
      "\n",
      "[INFO] Training node 'x2' for 519 epochs on cuda\n",
      "[INFO] Existing model found. Loading weights and history...\n",
      "Epoch 519/519  Train NLL: 0.3470  Val NLL: 0.3415  [Train: 27.21s  Val: 1.83s  Total: 29.05s]\n",
      "\n",
      "[INFO] Training node 'x3' for 519 epochs on cuda\n",
      "[INFO] Existing model found. Loading weights and history...\n",
      "Epoch 519/519  Train NLL: 1.2138  Val NLL: 1.2075  [Train: 18.02s  Val: 1.36s  Total: 19.38s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x1': None, 'x2': None, 'x3': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_model.fit(train_df, val_df,epochs=519)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633d0b7",
   "metadata": {},
   "source": [
    "get latents from a dateset marked as _U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7da65de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping node 'x3' (ordinal targets not yet supported).\n",
      "[INFO] Final latent DataFrame shape: (80000, 4)\n"
     ]
    }
   ],
   "source": [
    "latents_df = td_model.get_latent(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc3a56",
   "metadata": {},
   "source": [
    "sample from the whole graph , intervene or use predefined latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fa74e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted directory: /home/bule/TramDag/dev_experiment_logs/exp_6_2/x1/sampling\n",
      "Deleted directory: /home/bule/TramDag/dev_experiment_logs/exp_6_2/x2/sampling\n",
      "Deleted directory: /home/bule/TramDag/dev_experiment_logs/exp_6_2/x3/sampling\n",
      "\n",
      "----*----------*-------------*--------Sample Node: x1 ------------*-----------------*-------------------*--\n",
      "[INFO] Sampling new latents for node x1 from standard logistic distribution\n",
      "[WARNING] target_col 'x1' not in DataFrame columns — is this intended to be used as a Sampler?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chandrupatla root finding: 100%|██████████| 10000/10000 [00:20<00:00, 490.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----*----------*-------------*--------Sample Node: x2 ------------*-----------------*-------------------*--\n",
      "[INFO] Sampling new latents for node x2 from standard logistic distribution\n",
      "[WARNING] target_col 'x2' not in DataFrame columns — is this intended to be used as a Sampler?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Chandrupatla root finding: 100%|██████████| 10000/10000 [00:20<00:00, 497.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----*----------*-------------*--------Sample Node: x3 ------------*-----------------*-------------------*--\n",
      "[INFO] Sampling new latents for node x3 from standard logistic distribution\n",
      "[WARNING] target_col 'x3' not in DataFrame columns — is this intended to be used as a Sampler?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'x1': tensor([0.7811, 0.1069, 0.7993,  ..., 0.1500, 0.2716, 0.2310]),\n",
       "  'x2': tensor([-0.1590, -0.1666,  0.0845,  ...,  0.6845, -0.2752, -0.3163]),\n",
       "  'x3': tensor([1, 1, 1,  ..., 1, 1, 1])},\n",
       " {'x1': tensor([ 2.5734, -3.2737,  3.2499,  ..., -2.4392, -0.8847, -1.3168]),\n",
       "  'x2': tensor([ 0.7773, -0.6050,  2.0391,  ...,  3.7694, -0.8084, -1.0902]),\n",
       "  'x3': tensor([-1.6195,  2.7791, -0.9229,  ...,  3.8661,  1.9192,  0.5658])})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_model.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad785f25",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6a31aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TramDagConfig.load(\"/home/bule/TramDag/dev_experiment_logs/exp_6_2/configuration.json\")\n",
    "td_model = TramDagModel.from_config(cfg, set_initial_weights=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07ad5877",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'TramDagModel' has no attribute 'load_from_directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# laod model \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m td_model \u001b[38;5;241m=\u001b[39m\u001b[43mTramDagModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_directory\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/bule/TramDag/dev_experiment_logs/exp_6_2/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'TramDagModel' has no attribute 'load_from_directory'"
     ]
    }
   ],
   "source": [
    "# laod model \n",
    "td_model =TramDagModel.load_from_directory(\"/home/bule/TramDag/dev_experiment_logs/exp_6_2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b854fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model.load() # load exisit trained models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb2d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=td_model.history() # # mo show_training_history(node_list,EXPERIMENT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b05610",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model.show_hdag_for_source_nodes()#show_hdag_for_source_nodes(configuration_dict,EXPERIMENT_DIR,device=device,xmin_plot=0,xmax_plot=1) # TODO for other nodes funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd443525",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_model.inspect_trafo_standart_logistic()\n",
    "#inspect_trafo_standart_logistic(configuration_dict,EXPERIMENT_DIR,train_df,val_df,device,verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
