{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32793ca7",
   "metadata": {},
   "source": [
    "## tesing dataloader V5\n",
    "\n",
    "new ideas with ordinal c and yc \n",
    "has ordinal binary and continous outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from utils.graph import *\n",
    "from utils.loss_ordinal import *\n",
    "from utils.tram_model_helpers import *\n",
    "from utils.tram_models import *\n",
    "from utils.tram_data import *\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Train with GPU support.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU found, train with CPU support.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ada57",
   "metadata": {},
   "source": [
    "adjustet funcitnos for ordinal outcomes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8678f4f3",
   "metadata": {},
   "source": [
    "dev ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd61311",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"testing_v5_dataloader\"   ## <--- set experiment name\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "\n",
    "LOG_DIR=\"/home/bule/TramDag/dev_experiment_logs\"\n",
    "EXPERIMENT_DIR = os.path.join(LOG_DIR, experiment_name)\n",
    "DATA_PATH = EXPERIMENT_DIR # <----------- change to different source if needed\n",
    "CONF_DICT_PATH = os.path.join(EXPERIMENT_DIR, f\"configuration.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81881195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"x1\": np.random.normal(loc=0, scale=1, size=1000),\n",
    "    # \"x2\": np.random.uniform(low=0, high=10, size=1000),\n",
    "    \"ord_bin1\":  np.random.binomial(1, p=0.4, size=1000),\n",
    "    \"ord_multi\": np.random.choice([0, 1, 2, 3], size=1000, p=[0.2, 0.3, 0.3, 0.2]),\n",
    "    \"ord_bin2\":  np.random.binomial(1, p=0.4, size=1000),\n",
    "})\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "quantiles = train_df.quantile([0.05, 0.95])\n",
    "min_vals = quantiles.loc[0.05]\n",
    "max_vals = quantiles.loc[0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### New Functions and classes\n",
    "\n",
    "\n",
    "def create_levels_dict_v2(df:pd.DataFrame,data_type:dict):\n",
    "    # creates the levels dictionary for variables which should be modelled ordinaly \n",
    "    levels_dict={}\n",
    "    for variable,datatype in data_type.items():\n",
    "            if \"ordinal\" in datatype.lower():\n",
    "                unique_vals = set(df[variable].dropna().unique())\n",
    "                num_classes = len(unique_vals)\n",
    "\n",
    "                expected_vals = set(range(num_classes))\n",
    "                if unique_vals != expected_vals:\n",
    "                    raise ValueError(\n",
    "                        f\"Variable '{variable}' has values {sorted(unique_vals)}, \"\n",
    "                        f\"but expected values are {sorted(expected_vals)} (0 to {num_classes - 1}). \"\n",
    "                        \"Multiclass ordinal variables must be zero-indexed and contiguous.\"\n",
    "                )\n",
    "                levels_dict[variable]=len(np.unique(df[variable]))\n",
    "    return levels_dict   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_node_dict_v3(adj_matrix, nn_names_matrix, data_type, min_vals, max_vals,levels_dict=None):\n",
    "    \"\"\"\n",
    "    Creates a configuration dictionary for TRAMADAG based on an adjacency matrix,\n",
    "    a neural network names matrix, and a data type dictionary.\n",
    "    \"\"\"\n",
    "    if not validate_adj_matrix(adj_matrix):\n",
    "        raise ValueError(\"Invalid adjacency matrix. Please check the criteria.\")\n",
    "    \n",
    "    if len(data_type) != adj_matrix.shape[0]:\n",
    "        raise ValueError(\"Data type dictionary should have the same length as the adjacency matrix.\")\n",
    "    \n",
    "    target_nodes = {}\n",
    "    G, edge_labels = create_nx_graph(adj_matrix, node_labels=list(data_type.keys()))\n",
    "    \n",
    "    sources = [node for node in G.nodes if G.in_degree(node) == 0]\n",
    "    sinks = [node for node in G.nodes if G.out_degree(node) == 0]\n",
    "    \n",
    "    for i, node in enumerate(G.nodes):\n",
    "        parents = list(G.predecessors(node))\n",
    "        target_nodes[node] = {}\n",
    "        target_nodes[node]['Modelnr'] = i\n",
    "        target_nodes[node]['data_type'] = data_type[node]\n",
    "        \n",
    "        # write the levels of the ordinal outcome\n",
    "        if 'ordinal' in data_type[node]:\n",
    "            if levels_dict is None:\n",
    "                raise ValueError(\n",
    "                    \"levels_dict must be provided for ordinal nodes; \"\n",
    "                    \"e.g. levels_dict={'x3': 3}\"\n",
    "                )\n",
    "            if node not in levels_dict:\n",
    "                raise KeyError(\n",
    "                    f\"levels_dict is missing an entry for node '{node}'. \"\n",
    "                    f\"Expected something like levels_dict['{node}'] = <num_levels>\"\n",
    "                )\n",
    "            target_nodes[node]['levels'] = levels_dict[node]\n",
    "    \n",
    "        target_nodes[node]['node_type'] = \"source\" if node in sources else \"sink\" if node in sinks else \"internal\"\n",
    "        target_nodes[node]['parents'] = parents\n",
    "        target_nodes[node]['parents_datatype'] = {parent:data_type[parent] for parent in parents}\n",
    "        target_nodes[node]['transformation_terms_in_h()'] = {parent: edge_labels[(parent, node)] for parent in parents if (parent, node) in edge_labels}\n",
    "        target_nodes[node]['min'] = min_vals.iloc[i].tolist()   \n",
    "        target_nodes[node]['max'] = max_vals.iloc[i].tolist()\n",
    "\n",
    "        \n",
    "        transformation_term_nn_models = {}\n",
    "        for parent in parents:\n",
    "            parent_idx = list(data_type.keys()).index(parent)  \n",
    "            child_idx = list(data_type.keys()).index(node) \n",
    "            \n",
    "            if nn_names_matrix[parent_idx, child_idx] != \"0\":\n",
    "                transformation_term_nn_models[parent] = nn_names_matrix[parent_idx, child_idx]\n",
    "        target_nodes[node]['transformation_term_nn_models_in_h()'] = transformation_term_nn_models\n",
    "    return target_nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be112e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# datatype= \"ordinal_Xc_Yc\" # as X its continous aswell as Y continous\n",
    "# datatype= \"ordinal_Xc_Yo\"\n",
    "# datatype= \"ordinal_Xn_Yc\"\n",
    "# datatype= \"ordinal_Xn_Yo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0fe767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs_v2(x, transformation_terms, device='cuda'):\n",
    "    \"\"\"\n",
    "    Prepares model input by grouping features by transformation term base:\n",
    "      - ci11, ci12 → 'ci1' (intercept)\n",
    "      - cs11, cs12 → 'cs1' (shift)\n",
    "      - cs21 → 'cs2' (another shift group)\n",
    "      - cs, ls → treated as full group keys\n",
    "    Returns:\n",
    "      - int_inputs: Tensor of shape (B, n_features) for intercept model\n",
    "      - shift_list: List of tensors for each shift model, shape (B, group_features)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    transformation_terms=list(transformation_terms)\n",
    "    \n",
    "    ## if there is only a source so transforamtion terms is 0:\n",
    "    x = [xi.to(device, non_blocking=True) for xi in x]\n",
    "    if len(transformation_terms)== 0:\n",
    "        x = [xi.unsqueeze(1) for xi in x] \n",
    "        int_inputs= x[0]\n",
    "        return int_inputs, None\n",
    "\n",
    "    # Always ensure there's an intercept term\n",
    "    if not any('ci' in str(value) for value in transformation_terms):\n",
    "        transformation_terms.insert(0, 'si')\n",
    "\n",
    "    # Lists to collect intercept tensors and shift‐groups\n",
    "    int_tensors = []\n",
    "    shift_groups = []\n",
    "\n",
    "    # Helpers to track the “current” shift‐group for numbered suffixes\n",
    "    current_group = None\n",
    "    current_key = None\n",
    "\n",
    "    for tensor, term in zip(x, transformation_terms):\n",
    "        # 1) INTERCEPT terms (si*, ci*)\n",
    "        if term.startswith(('si','ci')):\n",
    "            int_tensors.append(tensor)\n",
    "\n",
    "        # 2) SHIFT terms (cs*, ls*)\n",
    "        elif term.startswith(('cs','ls')):\n",
    "            # numbered suffix → group by the first 3 chars (e.g. 'cs11'/'cs12' → 'cs1')\n",
    "            if len(term) > 2 and term[2].isdigit():\n",
    "                key = term[:3]\n",
    "                # start a new group if key changed\n",
    "                if current_group is None or current_key != key:\n",
    "                    current_group = []\n",
    "                    shift_groups.append(current_group)\n",
    "                    current_key = key\n",
    "                current_group.append(tensor)\n",
    "\n",
    "            # lone 'cs' or 'ls' → always its own group\n",
    "            else:\n",
    "                current_group = [tensor]\n",
    "                shift_groups.append(current_group)\n",
    "                current_key = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown transformation term: {term}\")\n",
    "\n",
    "    # Intercept: should be exactly one group\n",
    "    if len(int_tensors) == 0:\n",
    "        raise ValueError(\"No intercept tensors found!\")\n",
    "    int_inputs = torch.cat(\n",
    "        [t.to(device, non_blocking=True).view(t.shape[0], -1) for t in int_tensors],\n",
    "        dim=1\n",
    "    )\n",
    "\n",
    "    shift_list = [\n",
    "        torch.cat([t.to(device, non_blocking=True).view(t.shape[0], -1) for t in group], dim=1)\n",
    "        for group in shift_groups\n",
    "    ]\n",
    "\n",
    "    return int_inputs, shift_list if shift_list else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e6c21",
   "metadata": {},
   "source": [
    "## new version of dataloasdr v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e667265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericDataset_v5(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        target_col,\n",
    "        target_nodes=None,\n",
    "        parents_dataype_dict=None,\n",
    "        transformation_terms_in_h=None,\n",
    "        return_intercept_shift=True,\n",
    "        transform=None,\n",
    "        return_y=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        df: pd.DataFrame\n",
    "        target_col: str\n",
    "        target_nodes: dict mapping each node → metadata (including 'data_type')\n",
    "        parents_dataype_dict: dict var_name → \"cont\"|\"ord\"|\"other\"\n",
    "        transformation_terms_in_h: dict for intercept logic\n",
    "        return_intercept_shift: whether to return (int_input, shift_list) or raw features\n",
    "        transform: torchvision transform for images\n",
    "        \"\"\"\n",
    "        self.return_intercept_shift = return_intercept_shift\n",
    "        self.return_y=return_y\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.target_col = target_col\n",
    "        self.target_nodes = target_nodes or {}\n",
    "        self.parents_dataype_dict = parents_dataype_dict or {}\n",
    "        self.predictors = list(self.parents_dataype_dict.keys())\n",
    "        self.transform = transform\n",
    "        self.transformation_terms_preprocessing = list((transformation_terms_in_h or {}).values())\n",
    "        self.h_needs_simple_intercept = all('i' not in str(v) for v in self.transformation_terms_preprocessing)\n",
    "        self.target_data_type = self.target_nodes[self.target_col].get('data_type', '').lower()\n",
    "        self.target_num_classes = self.target_nodes[self.target_col].get('levels')\n",
    "\n",
    "        # figure out the intercept and shift terms for preprocessed \n",
    "        if return_intercept_shift:\n",
    "            self._set_intercept_shift_indexes()\n",
    "            \n",
    "        self._set_target_is_source()\n",
    "        self._set_ordinal_numal_classes()\n",
    "        \n",
    "        # checks\n",
    "        self._check_multiclass_predictors_of_df()\n",
    "        self._check_ordinal_levels()\n",
    "\n",
    "\n",
    "    def _set_target_is_source(self):\n",
    "        # set whether the target node is a source node\n",
    "        self.target_is_source = (self.target_nodes[self.target_col].get('node_type', '').lower() == \"source\")\n",
    "\n",
    "    def _set_ordinal_numal_classes(self):\n",
    "        # sets the number of classes for ordinal predictors\n",
    "        self.ordinal_num_classes = {\n",
    "            v: self.df[v].nunique()\n",
    "            for v in self.predictors\n",
    "            if \"ordinal\" in self.parents_dataype_dict[v].lower()\n",
    "            and \"xn\" in self.parents_dataype_dict[v].lower()\n",
    "        }\n",
    "\n",
    "    def _set_intercept_shift_indexes(self):\n",
    "        # always inject a simple intercept term if no 'ci' present\n",
    "        if not any('ci' in t for t in self.transformation_terms_preprocessing):\n",
    "            self.transformation_terms_preprocessing.insert(0, 'si')\n",
    "\n",
    "        self.intercept_indices = [\n",
    "            i for i, term in enumerate(self.transformation_terms_preprocessing)\n",
    "            if term.startswith(('si', 'ci'))\n",
    "        ]\n",
    "\n",
    "        self.shift_groups_indices = []\n",
    "        current_key = None\n",
    "        for i, term in enumerate(self.transformation_terms_preprocessing):\n",
    "            if term.startswith(('cs', 'ls')):\n",
    "                if len(term) > 2 and term[2].isdigit():\n",
    "                    grp = term[:3]\n",
    "                    if not self.shift_groups_indices or current_key != grp:\n",
    "                        self.shift_groups_indices.append([i])\n",
    "                        current_key = grp\n",
    "                    else:\n",
    "                        self.shift_groups_indices[-1].append(i)\n",
    "                else:\n",
    "                    self.shift_groups_indices.append([i])\n",
    "                    current_key = None\n",
    "\n",
    "    def _preprocess_inputs(self, x):\n",
    "        \"\"\"\n",
    "        x: List of tensors, each with a dummy batch‑dim at 0\n",
    "        Returns:\n",
    "            int_inputs: (B, sum(intercept_dims))\n",
    "            shift_list: list of (B, sum(dims_per_group)) or None\n",
    "        \"\"\"\n",
    "        # intercept\n",
    "        its = [x[i] for i in self.intercept_indices]\n",
    "        if not its:\n",
    "            raise ValueError(\"No intercept tensors found!\")\n",
    "        int_inputs = torch.cat([t.view(t.shape[0], -1) for t in its], dim=1)\n",
    "\n",
    "        # shift‐groups\n",
    "        shifts = []\n",
    "        for grp in self.shift_groups_indices:\n",
    "            parts = [x[i].view(x[i].shape[0], -1) for i in grp]\n",
    "            shifts.append(torch.cat(parts, dim=1))\n",
    "\n",
    "        return int_inputs, (shifts if shifts else None)\n",
    "\n",
    "    def _transform_y(self, row):\n",
    "        if self.target_data_type == \"continous\" or \"yc\" in self.target_data_type:\n",
    "            return torch.tensor(row[self.target_col], dtype=torch.float32)\n",
    "        elif self.target_num_classes:\n",
    "            yi = int(row[self.target_col])\n",
    "            return F.one_hot(\n",
    "                torch.tensor(yi, dtype=torch.long),\n",
    "                num_classes=self.target_num_classes\n",
    "            ).float().squeeze()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Cannot encode target '{self.target_col}': \"\n",
    "                f\"{self.target_data_type}/{self.target_num_classes}\"\n",
    "            )\n",
    "\n",
    "    def _check_multiclass_predictors_of_df(self):\n",
    "        for v in self.predictors:\n",
    "            dt = self.parents_dataype_dict[v].lower()\n",
    "            if \"ordinal\" in dt and \"xn\" in dt:\n",
    "                vals = set(self.df[v].dropna().unique())\n",
    "                if vals != set(range(len(vals))):\n",
    "                    raise ValueError(\n",
    "                        f\"Ordinal predictor '{v}' must be zero‑indexed; got {sorted(vals)}\"\n",
    "                    )\n",
    "\n",
    "    def _check_ordinal_levels(self):\n",
    "        ords = []\n",
    "        if \"ordinal\" in self.target_nodes[self.target_col]['data_type']:\n",
    "            ords.append(self.target_col)\n",
    "        ords += [\n",
    "            v for v in self.predictors\n",
    "            if \"ordinal\" in self.parents_dataype_dict[v].lower()\n",
    "            and \"xn\" in self.parents_dataype_dict[v].lower()\n",
    "        ]\n",
    "        for v in ords:\n",
    "            lvl = self.target_nodes[v].get('levels')\n",
    "            if lvl is None:\n",
    "                raise ValueError(f\"Ordinal '{v}' missing 'levels' metadata.\")\n",
    "            uniq = sorted(self.df[v].dropna().unique())\n",
    "            if uniq != list(range(lvl)):\n",
    "                raise ValueError(\n",
    "                    f\"Ordinal '{v}' values {uniq} != expected 0…{lvl-1}.\"\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        x_data = []\n",
    "\n",
    "        # simple intercept if needed\n",
    "        if self.h_needs_simple_intercept:\n",
    "            x_data.append(torch.tensor(1.0))\n",
    "\n",
    "        ################################ IF target is a source\n",
    "        if self.target_is_source:\n",
    "            y = self._transform_y(row)\n",
    "            if not self.return_intercept_shift:\n",
    "                if self.return_y:\n",
    "                    return tuple(x_data), y\n",
    "                else:\n",
    "                    return tuple(x_data)\n",
    "            \n",
    "            else:\n",
    "                # batchify & preprocess\n",
    "                batched = [x.unsqueeze(0) for x in x_data]\n",
    "                int_in, shifts = self._preprocess_inputs(batched)\n",
    "                # **squeeze off** our dummy batch‐dim:\n",
    "                int_in = int_in.squeeze(0)              # -> (n_intercept,)\n",
    "                shifts = [] if shifts is None else [s.squeeze(0) for s in shifts]\n",
    "                if self.return_y:\n",
    "                    return (int_in, shifts), y\n",
    "                else:\n",
    "                    return (int_in, shifts)\n",
    "\n",
    "        ################################ IF target has predictors\n",
    "        for var in self.predictors:\n",
    "            dt = self.parents_dataype_dict[var].lower()\n",
    "            if dt == \"continous\" or \"xc\" in dt:\n",
    "                x_data.append(torch.tensor(row[var], dtype=torch.float32))\n",
    "            elif \"ordinal\" in dt and \"xn\" in dt:\n",
    "                c = self.ordinal_num_classes[var]\n",
    "                o = int(row[var])\n",
    "                x_data.append(\n",
    "                    F.one_hot(torch.tensor(o, dtype=torch.long), num_classes=c).float().squeeze()\n",
    "                )\n",
    "            else:\n",
    "                img = Image.open(row[var]).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                x_data.append(img)\n",
    "\n",
    "        # only return the original data without separating them for int and shifts\n",
    "        if not self.return_intercept_shift:\n",
    "            if self.return_y:\n",
    "                y = self._transform_y(row)\n",
    "                return tuple(x_data), y\n",
    "            else:\n",
    "                return tuple(x_data)\n",
    "        \n",
    "        # returning already splitted int and shifts\n",
    "        else:    \n",
    "            batched = [x.unsqueeze(0) for x in x_data]\n",
    "            int_in, shifts = self._preprocess_inputs(batched)\n",
    "            int_in = int_in.squeeze(0)\n",
    "            shifts = [] if shifts is None else [s.squeeze(0) for s in shifts]\n",
    "            \n",
    "            if self.return_y:\n",
    "                y = self._transform_y(row)\n",
    "                return (int_in, shifts), y\n",
    "            else:\n",
    "                return (int_in, shifts)\n",
    "\n",
    "\n",
    "def get_dataloader_v5(node, target_nodes, train_df, val_df, batch_size=32,return_intercept_shift=False, verbose=False):\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    ordered_parents_dataype_dict, ordered_transformation_terms_in_h, _ = ordered_parents(node, target_nodes)\n",
    "    if verbose:\n",
    "        print(f\"Parents dtype: {ordered_parents_dataype_dict}\")\n",
    "    train_ds = GenericDataset_v5(train_df,target_col=node,target_nodes=target_nodes,parents_dataype_dict=ordered_parents_dataype_dict,transform=transform,transformation_terms_in_h=ordered_transformation_terms_in_h,return_intercept_shift=return_intercept_shift)\n",
    "    val_ds = GenericDataset_v5(val_df,target_col=node,target_nodes=target_nodes,parents_dataype_dict=ordered_parents_dataype_dict,transform=transform,transformation_terms_in_h=ordered_transformation_terms_in_h,return_intercept_shift=return_intercept_shift)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type={key:value for key, value in zip(train_df.columns, ['continous']+['ordinal_Xn_Yo']+['ordinal_Xn_Yo']+['ordinal_Xn_Yo'])}\n",
    "print(data_type)\n",
    "configuration_dict=new_conf_dict(experiment_name,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)\n",
    "\n",
    "adj_matrix=np.array([['0', '0', 'cs11', 'cs11'],\n",
    "                     ['0', '0', 'cs12', 'cs12'],\n",
    "                     ['0', '0', '0', 'cs'],\n",
    "                     ['0', '0', '0', '0']])\n",
    "\n",
    "return_intercept_shift=False\n",
    "\n",
    "\n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "levels_dict=create_levels_dict_v2(df,data_type)\n",
    "target_nodes=create_node_dict_v3(adj_matrix, nn_names_matrix, data_type, min_vals, max_vals,levels_dict)\n",
    "\n",
    "for node in target_nodes:\n",
    "    print(f'-----------------{node}-------------------')\n",
    "    train_loader, _ = get_dataloader_v5(node, target_nodes, train_df, val_df, batch_size=1,return_intercept_shift=return_intercept_shift, verbose=False)\n",
    "    \n",
    "    if return_intercept_shift:\n",
    "        (int_input, shift_list), y =next(iter(train_loader))\n",
    "        print(f'int_input {int_input}')\n",
    "        print(f'shift_list {shift_list}')\n",
    "\n",
    "        print(int_input.shape)\n",
    "    else:\n",
    "        x, y =next(iter(train_loader))\n",
    "\n",
    "        \n",
    "        _, ordered_transformation_terms_in_h, _=ordered_parents(node, target_nodes)\n",
    "        int_input, shift_list= preprocess_inputs_v2(x, ordered_transformation_terms_in_h.values(), device='cuda')\n",
    "        \n",
    "        print(f'int_input {int_input}')\n",
    "        print(f'shift_list {shift_list}')\n",
    "        print(int_input.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type={key:value for key, value in zip(train_df.columns, ['continous']+['ordinal_Xn_Yc']+['ordinal_Xc_Yo']+['ordinal_Xn_Yc'])}\n",
    "print(data_type)\n",
    "configuration_dict=new_conf_dict(experiment_name,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)\n",
    "\n",
    "adj_matrix=np.array([['0', 'cs', 'cs', 'ls'],\n",
    "                     ['0', '0', 'ls', 'ci'],\n",
    "                     ['0', '0', '0', 'cs'],\n",
    "                     ['0', '0', '0', '0']])\n",
    "\n",
    "return_intercept_shift=True\n",
    "      \n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "levels_dict=create_levels_dict_v2(df,data_type)\n",
    "target_nodes=create_node_dict_v3(adj_matrix, nn_names_matrix, data_type, min_vals, max_vals,levels_dict)\n",
    "\n",
    "for node in target_nodes:\n",
    "    print(f'-----------------{node}-------------------')\n",
    "    train_loader, _ = get_dataloader_v5(node, target_nodes, train_df, val_df, batch_size=1,return_intercept_shift=return_intercept_shift, verbose=False)\n",
    "    \n",
    "    if return_intercept_shift:\n",
    "        (int_input, shift_list), y =next(iter(train_loader))\n",
    "        print(f'int_input {int_input}')\n",
    "        print(f'shift_list {shift_list}')\n",
    "\n",
    "        print(int_input.shape)\n",
    "    else:\n",
    "        x, y =next(iter(train_loader))\n",
    "\n",
    "        \n",
    "        _, ordered_transformation_terms_in_h, _=ordered_parents(node, target_nodes)\n",
    "        int_input, shift_list= preprocess_inputs_v2(x, ordered_transformation_terms_in_h.values(), device='cuda')\n",
    "        \n",
    "        print(f'int_input {int_input}')\n",
    "        print(f'shift_list {shift_list}')\n",
    "        print(int_input.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc93ed48",
   "metadata": {},
   "source": [
    "## DATALAODER V6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7229707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericDataset_v6(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        target_col,\n",
    "        target_nodes=None,\n",
    "\n",
    "        transform=None,\n",
    "        return_intercept_shift=True,\n",
    "        return_y=True,\n",
    "        debug=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        df: pd.DataFrame\n",
    "        target_col: str\n",
    "        target_nodes: dict mapping each node → metadata (including 'data_type', 'levels', 'node_type')\n",
    "        parents_datatype_dict: dict var_name → \"cont\"|\"ord\"|\"other\"\n",
    "        transformation_terms_in_h: dict for intercept logic\n",
    "        return_intercept_shift: whether to return (int_input, shift_list) or raw features\n",
    "        transform: torchvision transform for images\n",
    "        return_y: whether to return target y\n",
    "        debug: bool to enable debug logging and attribute printouts\n",
    "        \"\"\"\n",
    "        # initialize debug \n",
    "        self._set_debug(debug)\n",
    "\n",
    "        # set attributes via dedicated setters\n",
    "        self._set_df(df)\n",
    "        self._set_target_col(target_col)\n",
    "        self._set_target_nodes(target_nodes)\n",
    "        self._set_ordered_parents_datatype_and_transformation_terms()\n",
    "\n",
    "        self._set_predictors()\n",
    "        self._set_transform(transform)\n",
    "\n",
    "        self._set_h_needs_simple_intercept()\n",
    "        self._set_target_data_type()\n",
    "        self._set_target_num_classes()\n",
    "        self.return_intercept_shift = return_intercept_shift\n",
    "        self.return_y = return_y\n",
    "\n",
    "        # intercept and shift\n",
    "        if self.return_intercept_shift:\n",
    "            self._set_intercept_shift_indexes()\n",
    "\n",
    "        # source/ordinal\n",
    "        self._set_target_is_source()\n",
    "        self._set_ordinal_numal_classes()\n",
    "\n",
    "        # checks\n",
    "        self._check_multiclass_predictors_of_df()\n",
    "        self._check_ordinal_levels()\n",
    "\n",
    "    # Setter methods\n",
    "    def _set_ordered_parents_datatype_and_transformation_terms(self):\n",
    "        \n",
    "        ordered_parents_datatype, ordered_transformation_terms_in_h, _ =ordered_parents(self.target_col, self.target_nodes)\n",
    "        if not isinstance(ordered_parents_datatype, dict):\n",
    "            raise TypeError(f\"parents_datatype_dict must be dict, got {type(ordered_parents_datatype)}\")\n",
    "        self.parents_datatype_dict = ordered_parents_datatype\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Set parents_datatype_dict: type={type(self.parents_datatype_dict)}, keys={list(self.parents_datatype_dict.keys())}\")\n",
    "\n",
    "        if ordered_transformation_terms_in_h is None:\n",
    "            ordered_transformation_terms_in_h = {}\n",
    "        if not isinstance(ordered_transformation_terms_in_h, dict):\n",
    "            raise TypeError(f\"transformation_terms_in_h must be dict, got {type(ordered_transformation_terms_in_h)}\")\n",
    "        self.transformation_terms_preprocessing = list(ordered_transformation_terms_in_h.values())\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Set transformation_terms_preprocessing: type={type(self.transformation_terms_preprocessing)}, value={self.transformation_terms_preprocessing}\")\n",
    "    \n",
    "    \n",
    "    def _set_debug(self, debug):\n",
    "        if not isinstance(debug, bool):\n",
    "            raise TypeError(f\"debug must be bool, got {type(debug)}\")\n",
    "        self.debug = debug\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Set debug: type={type(self.debug)}, value={self.debug}\")\n",
    "\n",
    "    def _set_df(self, df):\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(f\"df must be a pandas DataFrame, got {type(df)}\")\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Set df: type={type(self.df)}, shape={self.df.shape}\")\n",
    "\n",
    "\n",
    "    def _set_target_col(self, target_col):\n",
    "        if not isinstance(target_col, str):\n",
    "            raise TypeError(f\"target_col must be str, got {type(target_col)}\")\n",
    "        \n",
    "        if target_col not in self.df.columns:\n",
    "            print(\n",
    "                f\"[WARNING] target_col '{target_col}' not in DataFrame columns — is this intended for use as a Sampler?\")\n",
    "            if self.debug:\n",
    "                print(f\"[DEBUG] target_col '{target_col}' not found in DataFrame columns\")\n",
    "            # Still set it in case it's needed for Sampler or other logic\n",
    "            self.target_col = target_col\n",
    "            return\n",
    "\n",
    "        self.target_col = target_col\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Set target_col: type={type(self.target_col)}, value={self.target_col}\")\n",
    "\n",
    "\n",
    "    def _set_target_nodes(self, target_nodes):\n",
    "        if target_nodes is None:\n",
    "            target_nodes = {}\n",
    "        if not isinstance(target_nodes, dict):\n",
    "            raise TypeError(f\"target_nodes must be dict, got {type(target_nodes)}\")\n",
    "        self.target_nodes = target_nodes\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Set target_nodes: type={type(self.target_nodes)}, keys={list(self.target_nodes.keys())}\")\n",
    "\n",
    "\n",
    "    def _set_predictors(self):\n",
    "        self.predictors = list(self.parents_datatype_dict.keys())\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Set predictors: type={type(self.predictors)}, value={self.predictors}\")\n",
    "\n",
    "    def _set_transform(self, transform):\n",
    "        self.transform = transform\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Set transform: type={type(self.transform)}, value={self.transform}\")\n",
    "\n",
    "\n",
    "    def _set_h_needs_simple_intercept(self):\n",
    "        self.h_needs_simple_intercept = all('i' not in str(v) for v in self.transformation_terms_preprocessing)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Set h_needs_simple_intercept: type={type(self.h_needs_simple_intercept)}, value={self.h_needs_simple_intercept}\")\n",
    "\n",
    "    def _set_target_data_type(self):\n",
    "        dtype = self.target_nodes.get(self.target_col, {}).get('data_type', '')\n",
    "        self.target_data_type = dtype.lower()\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Set target_data_type: type={type(self.target_data_type)}, value={self.target_data_type}\")\n",
    "\n",
    "    def _set_target_num_classes(self):\n",
    "        levels = self.target_nodes.get(self.target_col, {}).get('levels')\n",
    "        if levels is not None and not isinstance(levels, int):\n",
    "            raise TypeError(f\"levels must be int, got {type(levels)}\")\n",
    "        self.target_num_classes = levels\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Set target_num_classes: type={type(self.target_num_classes)}, value={self.target_num_classes}\")\n",
    "            \n",
    "    def _set_target_is_source(self):\n",
    "        # determine if target node is a source\n",
    "        node_type = self.target_nodes.get(self.target_col, {}).get('node_type', '')\n",
    "        if not isinstance(node_type, str):\n",
    "            raise TypeError(f\"node_type metadata must be str, got {type(node_type)}\")\n",
    "        self.target_is_source = node_type.lower() == 'source'\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Set target_is_source: type={type(self.target_is_source)}, value={self.target_is_source}\")\n",
    "\n",
    "    def _set_ordinal_numal_classes(self):\n",
    "        # compute number of classes for each ordinal-xn predictor\n",
    "        mapping = {}\n",
    "        for v in self.predictors:\n",
    "            dt = self.parents_datatype_dict[v]\n",
    "            if not isinstance(dt, str):\n",
    "                raise TypeError(f\"datatype for predictor '{v}' must be str, got {type(dt)}\")\n",
    "            if 'ordinal' in dt.lower() and 'xn' in dt.lower():\n",
    "                if v not in self.df.columns:\n",
    "                    raise ValueError(f\"Predictor column '{v}' not in DataFrame\")\n",
    "                mapping[v] = self.df[v].nunique()\n",
    "        self.ordinal_num_classes = mapping\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Set ordinal_num_classes: type={type(self.ordinal_num_classes)}, value={self.ordinal_num_classes}\")\n",
    "\n",
    "    def get_sort_key(self, val):\n",
    "            \"\"\"Map each transformation value to a sorting index.\"\"\"\n",
    "            order = ['ci', 'ciXX', 'cs', 'csXX', 'ls']\n",
    "            \n",
    "            base = re.match(r'[a-zA-Z]+', val)\n",
    "            digits = re.findall(r'\\d+', val)\n",
    "            base = base.group(0) if base else ''\n",
    "            digits = int(digits[0]) if digits else -1\n",
    "\n",
    "            if base in ['ci', 'cs'] and digits != -1:\n",
    "                # For ciXX/csXX\n",
    "                return (order.index(base + 'XX'), digits)\n",
    "            elif base in order:\n",
    "                return (order.index(base), -1)\n",
    "            else:\n",
    "                return (len(order), digits)  # unknown terms go last\n",
    "\n",
    "    def ordered_parents(self,node, target_dict) -> dict:\n",
    "        \n",
    "        \"\"\"\n",
    "        Orders the transformation terms and their corresponding data types and nn models used for the models and the dataloader\n",
    "        \"\"\"\n",
    "        # Extract dictionaries\n",
    "        transformation_terms = target_dict[node]['transformation_terms_in_h()']\n",
    "        datatype_dict = target_dict[node]['parents_datatype']\n",
    "        nn_models_dict = target_dict[node]['transformation_term_nn_models_in_h()']\n",
    "\n",
    "        # Sort the items based on the transformation_terms\n",
    "        sorted_keys = sorted(transformation_terms.keys(), key=lambda k: self.get_sort_key(transformation_terms[k]))\n",
    "\n",
    "        # Create ordered dicts\n",
    "        ordered_transformation_terms_in_h = OrderedDict((k, transformation_terms[k]) for k in sorted_keys)\n",
    "        ordered_parents_datatype = OrderedDict((k, datatype_dict[k]) for k in sorted_keys)\n",
    "        ordered_transformation_term_nn_models_in_h = OrderedDict((k, nn_models_dict[k]) for k in sorted_keys)\n",
    "\n",
    "        return ordered_parents_datatype, ordered_transformation_terms_in_h, ordered_transformation_term_nn_models_in_h\n",
    "\n",
    "    # Core methods\n",
    "    def _set_intercept_shift_indexes(self):\n",
    "        if not any('ci' in t for t in self.transformation_terms_preprocessing):\n",
    "            self.transformation_terms_preprocessing.insert(0, 'si')\n",
    "            if self.debug:\n",
    "                print(\"[DEBUG] Inserted simple intercept term 'si'\")\n",
    "        self.intercept_indices = [i for i, term in enumerate(self.transformation_terms_preprocessing)\n",
    "                                    if term.startswith(('si', 'ci'))]\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Intercept indices: {self.intercept_indices}\")\n",
    "        self.shift_groups_indices = []\n",
    "        current_key = None\n",
    "        for i, term in enumerate(self.transformation_terms_preprocessing):\n",
    "            if term.startswith(('cs', 'ls')):\n",
    "                if len(term) > 2 and term[2].isdigit():\n",
    "                    grp = term[:3]\n",
    "                    if not self.shift_groups_indices or current_key != grp:\n",
    "                        self.shift_groups_indices.append([i])\n",
    "                        current_key = grp\n",
    "                    else:\n",
    "                        self.shift_groups_indices[-1].append(i)\n",
    "                else:\n",
    "                    self.shift_groups_indices.append([i])\n",
    "                    current_key = None\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Shift group indices: {self.shift_groups_indices}\")\n",
    "\n",
    "    def _preprocess_inputs(self, x):\n",
    "        its = [x[i] for i in self.intercept_indices]\n",
    "        if not its:\n",
    "            raise ValueError(\"No intercept tensors found!\")\n",
    "        int_inputs = torch.cat([t.view(t.shape[0], -1) for t in its], dim=1)\n",
    "        shifts = []\n",
    "        for grp in self.shift_groups_indices:\n",
    "            parts = [x[i].view(x[i].shape[0], -1) for i in grp]\n",
    "            shifts.append(torch.cat(parts, dim=1))\n",
    "        return int_inputs, (shifts if shifts else None)\n",
    "\n",
    "    def _transform_y(self, row):\n",
    "        if self.target_data_type in ('continous',) or 'yc' in self.target_data_type:\n",
    "            return torch.tensor(row[self.target_col], dtype=torch.float32)\n",
    "        elif self.target_num_classes:\n",
    "            yi = int(row[self.target_col])\n",
    "            return F.one_hot(\n",
    "                torch.tensor(yi, dtype=torch.long),\n",
    "                num_classes=self.target_num_classes\n",
    "            ).float().squeeze()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Cannot encode target '{self.target_col}': {self.target_data_type}/{self.target_num_classes}\"\n",
    "            )\n",
    "    #checks\n",
    "    \n",
    "    def _check_multiclass_predictors_of_df(self):\n",
    "        for v in self.predictors:\n",
    "            dt = self.parents_datatype_dict[v].lower()\n",
    "            if 'ordinal' in dt and 'xn' in dt:\n",
    "                vals = set(self.df[v].dropna().unique())\n",
    "                if vals != set(range(len(vals))):\n",
    "                    raise ValueError(\n",
    "                        f\"Ordinal predictor '{v}' must be zero‑indexed; got {sorted(vals)}\"\n",
    "                    )\n",
    "\n",
    "    def _check_ordinal_levels(self):\n",
    "        ords = []\n",
    "        if 'ordinal' in self.target_nodes.get(self.target_col, {}).get('data_type', '').lower():\n",
    "            ords.append(self.target_col)\n",
    "        ords += [\n",
    "            v for v in self.predictors\n",
    "            if 'ordinal' in self.parents_datatype_dict[v].lower()\n",
    "            and 'xn' in self.parents_datatype_dict[v].lower()\n",
    "        ]\n",
    "        for v in ords:\n",
    "            lvl = self.target_nodes[v].get('levels')\n",
    "            if lvl is None:\n",
    "                raise ValueError(f\"Ordinal '{v}' missing 'levels' metadata.\")\n",
    "            uniq = sorted(self.df[v].dropna().unique())\n",
    "            if uniq != list(range(lvl)):\n",
    "                raise ValueError(\n",
    "                    f\"Ordinal '{v}' values {uniq} != expected 0…{lvl-1}.\"\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        x_data = []\n",
    "\n",
    "        if self.h_needs_simple_intercept:\n",
    "            x_data.append(torch.tensor(1.0))\n",
    "\n",
    "        if self.target_is_source:\n",
    "            y = self._transform_y(row)\n",
    "            if not self.return_intercept_shift:\n",
    "                return (tuple(x_data), y) if self.return_y else tuple(x_data)\n",
    "            batched = [x.unsqueeze(0) for x in x_data]\n",
    "            int_in, shifts = self._preprocess_inputs(batched)\n",
    "            int_in = int_in.squeeze(0)\n",
    "            shifts = [] if shifts is None else [s.squeeze(0) for s in shifts]\n",
    "            return ((int_in, shifts), y) if self.return_y else (int_in, shifts)\n",
    "\n",
    "        for var in self.predictors:\n",
    "            dt = self.parents_datatype_dict[var].lower()\n",
    "            if dt in ('continous',) or 'xc' in dt:\n",
    "                x_data.append(torch.tensor(row[var], dtype=torch.float32))\n",
    "            elif 'ordinal' in dt and 'xn' in dt:\n",
    "                c = self.ordinal_num_classes[var]\n",
    "                o = int(row[var])\n",
    "                x_data.append(\n",
    "                    F.one_hot(torch.tensor(o, dtype=torch.long), num_classes=c).float().squeeze()\n",
    "                )\n",
    "            else:\n",
    "                img = Image.open(row[var]).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                x_data.append(img)\n",
    "\n",
    "        if not self.return_intercept_shift:\n",
    "            if self.return_y:\n",
    "                y = self._transform_y(row)\n",
    "                return tuple(x_data), y\n",
    "            else:\n",
    "                return tuple(x_data)\n",
    "\n",
    "        batched = [x.unsqueeze(0) for x in x_data]\n",
    "        int_in, shifts = self._preprocess_inputs(batched)\n",
    "        int_in = int_in.squeeze(0)\n",
    "        shifts = [] if shifts is None else [s.squeeze(0) for s in shifts]\n",
    "        if self.return_y:\n",
    "            y = self._transform_y(row)\n",
    "            return (int_in, shifts), y\n",
    "        return int_in, shifts\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloader_v6(node, target_nodes, train_df, val_df, batch_size=32,return_intercept_shift=False, verbose=False):\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_ds = GenericDataset_v6(train_df,target_col=node,target_nodes=target_nodes,transform=transform,return_intercept_shift=return_intercept_shift)\n",
    "    val_ds = GenericDataset_v6(val_df,target_col=node,target_nodes=target_nodes,transform=transform,return_intercept_shift=return_intercept_shift)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52647e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type={key:value for key, value in zip(train_df.columns, ['continous']+['ordinal_Xn_Yc']+['ordinal_Xc_Yo']+['ordinal_Xn_Yc'])}\n",
    "print(data_type)\n",
    "configuration_dict=new_conf_dict(experiment_name,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)\n",
    "\n",
    "adj_matrix=np.array([['0', 'cs', 'cs', 'ls'],\n",
    "                     ['0', '0', 'ls', 'ci'],\n",
    "                     ['0', '0', '0', 'cs'],\n",
    "                     ['0', '0', '0', '0']])\n",
    "\n",
    "return_intercept_shift=True\n",
    "      \n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "levels_dict=create_levels_dict_v2(df,data_type)\n",
    "target_nodes=create_node_dict_v3(adj_matrix, nn_names_matrix, data_type, min_vals, max_vals,levels_dict)\n",
    "\n",
    "for node in target_nodes:\n",
    "    print(f'-----------------{node}-------------------')\n",
    "    train_loader, _ = get_dataloader_v6(node, target_nodes, train_df, val_df, batch_size=1,return_intercept_shift=return_intercept_shift, verbose=False)\n",
    "    \n",
    "    if return_intercept_shift:\n",
    "        (int_input, shift_list), y =next(iter(train_loader))\n",
    "        print(f'int_input {int_input}')\n",
    "        print(f'shift_list {shift_list}')\n",
    "\n",
    "        print(int_input.shape)\n",
    "    # else:\n",
    "    #     x, y =next(iter(train_loader))\n",
    "\n",
    "        \n",
    "    #     _, ordered_transformation_terms_in_h, _=ordered_parents(node, target_nodes)\n",
    "    #     int_input, shift_list= preprocess_inputs_v2(x, ordered_transformation_terms_in_h.values(), device='cuda')\n",
    "        \n",
    "    #     print(f'int_input {int_input}')\n",
    "    #     print(f'shift_list {shift_list}')\n",
    "    #     print(int_input.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68010ff2",
   "metadata": {},
   "source": [
    "## FULLY SPECIFIED TRAM MODEL LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd7c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fully_specified_tram_model_v5(node: str, target_nodes: dict, verbose=True):\n",
    "    \"\"\"\n",
    "    returns a Trammodel fully specified , according to CI groups and CS groups , for ordinal outcome and inputs\n",
    "\n",
    "    \"\"\"\n",
    "    # Helper to detect ordinal with 'yo'\n",
    "    def is_ordinal_yo(data_type: str) -> bool:\n",
    "        return 'ordinal' in data_type and 'yo' in data_type.lower()\n",
    "\n",
    "    # Determine number of thetas for ordinal nodes\n",
    "    def compute_n_thetas(node_meta: dict):\n",
    "        return node_meta['levels'] - 1 if is_ordinal_yo(node_meta['data_type']) else None\n",
    "\n",
    "    # Compute number of input features for a given feature set\n",
    "    def compute_n_features(feats, parents_datatype):\n",
    "        n_features = 0\n",
    "        for parent_name, _ in feats:\n",
    "            parent_dt = parents_datatype[parent_name]\n",
    "            if 'xn' in parent_dt.lower():\n",
    "                n_features += target_nodes[parent_name]['levels']\n",
    "            else:\n",
    "                n_features += 1\n",
    "        return n_features\n",
    "\n",
    "    # Gather transformation terms and model class names\n",
    "    _, terms_dict, model_names_dict = ordered_parents(node, target_nodes)\n",
    "    model_dict = merge_transformation_dicts(terms_dict, model_names_dict)\n",
    "\n",
    "    # Split into intercept and shift components by h_term prefixes\n",
    "    intercepts_dict = {\n",
    "        k: v for k, v in model_dict.items()\n",
    "        if any(pref in v['h_term'] for pref in (\"ci\", \"si\"))\n",
    "    }\n",
    "    shifts_dict = {\n",
    "        k: v for k, v in model_dict.items()\n",
    "        if not any(pref in v['h_term'] for pref in (\"ci\", \"si\"))\n",
    "    }\n",
    "\n",
    "    # Build intercept network\n",
    "    intercept_groups = group_by_base(intercepts_dict, prefixes=(\"ci\", \"si\"))\n",
    "    if intercept_groups:\n",
    "        if len(intercept_groups) > 1:\n",
    "            raise ValueError(\"Multiple complex intercept groups detected; only one is supported.\")\n",
    "        feats = next(iter(intercept_groups.values()))\n",
    "        cls_name = feats[0][1]['class_name']\n",
    "        base_cls = get_base_model_class(cls_name)\n",
    "        # Use specified thetas or default for continuous\n",
    "        number_thetas = compute_n_thetas(target_nodes[node]) or 20\n",
    "        n_features = compute_n_features(feats, target_nodes[node]['parents_datatype'])\n",
    "        nn_int = globals()[base_cls](n_features=n_features, n_thetas=number_thetas)\n",
    "    else:\n",
    "        theta_count = compute_n_thetas(target_nodes[node])\n",
    "        nn_int = SimpleIntercept(n_thetas=theta_count) if theta_count is not None else SimpleIntercept()\n",
    "\n",
    "    # Build shift networks\n",
    "    shift_groups = group_by_base(shifts_dict, prefixes=(\"cs\", \"ls\"))\n",
    "    nn_shifts = []\n",
    "    for feats in shift_groups.values():\n",
    "        cls_name = feats[0][1]['class_name']\n",
    "        base_cls = get_base_model_class(cls_name)\n",
    "        n_features = compute_n_features(feats, target_nodes[node]['parents_datatype'])\n",
    "        nn_shifts.append(globals()[base_cls](n_features=n_features))\n",
    "\n",
    "    # Combine into final TramModel\n",
    "    tram_model = TramModel(nn_int, nn_shifts)\n",
    "    return tram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fully_specified_tram_model_v5(node, target_nodes, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6bdf9",
   "metadata": {},
   "source": [
    "## TEST DATALAODER V5 and MODEL loader V5 together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d2692",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_TRAINING=True\n",
    "train_list=['x1','x2','x3']#['x2']#'x1','x2']#,'x3']#['x1']#['x1','x2','x3']#,#,['x1','x2','x3'] # <-  set the nodes which have to be trained , useful if further training is required else lsit all vars\n",
    "\n",
    "batch_size = 512#4112\n",
    "epochs = 25# <- if you want a higher numbe rof epochs, set the number higher and it loads the old model and starts from there\n",
    "learning_rate=0.01\n",
    "use_scheduler =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Train with GPU support.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU found, train with CPU support.\")\n",
    "\n",
    "data_type={key:value for key, value in zip(train_df.columns, ['continous']+['ordinal_Xn_Yo']+['ordinal_Xn_Yo']+['ordinal_Xn_Yo'])}\n",
    "print(data_type)\n",
    "configuration_dict=new_conf_dict(experiment_name,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)\n",
    "\n",
    "adj_matrix=np.array([['0', '0', 'cs11', 'cs11'],\n",
    "                     ['0', '0', 'cs12', 'cs12'],\n",
    "                     ['0', '0', '0', 'cs'],\n",
    "                     ['0', '0', '0', '0']])\n",
    "\n",
    "return_intercept_shift=True\n",
    "\n",
    "\n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "levels_dict=create_levels_dict_v2(df,data_type)\n",
    "target_nodes=create_node_dict_v3(adj_matrix, nn_names_matrix, data_type, min_vals, max_vals,levels_dict)\n",
    "\n",
    "\n",
    "for node in target_nodes:\n",
    "    print(f'-----------------{node}-------------------')\n",
    "    \n",
    "    tram_model= get_fully_specified_tram_model_v5(node, target_nodes, verbose=True)\n",
    "    train_loader, val_loader = get_dataloader_v5(node, target_nodes, train_df, val_df, batch_size=batch_size,return_intercept_shift=return_intercept_shift, verbose=False)\n",
    "    \n",
    "    NODE_DIR = os.path.join(EXPERIMENT_DIR, f'{node}')\n",
    "    os.makedirs(NODE_DIR, exist_ok=True)\n",
    "    \n",
    "    optimizer =torch.optim.Adam(tram_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    \n",
    "    if return_intercept_shift:\n",
    "        (int_input, shift_list), y =next(iter(train_loader))\n",
    "        train_val_loop_v5(\n",
    "                   node,\n",
    "                   target_nodes,\n",
    "                   NODE_DIR,\n",
    "                   tram_model,\n",
    "                   train_loader,\n",
    "                   val_loader,\n",
    "                   epochs,\n",
    "                   optimizer,\n",
    "                   use_scheduler,\n",
    "                   scheduler,\n",
    "                   save_linear_shifts=False,\n",
    "                   verbose=1,\n",
    "                   device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b46a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training_history(target_nodes,EXPERIMENT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037057e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1dd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sampling_tram_data import show_hdag_for_source_nodes_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faf8b19",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hdag_for_source_nodes_v3(target_nodes,EXPERIMENT_DIR,device,xmin_plot=-3,xmax_plot=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081fab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sampling_tram_data import inspect_trafo_standart_logistic_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_trafo_standart_logistic_v3(target_nodes, EXPERIMENT_DIR, train_df, val_df, device, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2453b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sampling_tram_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_nodes_dict=target_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO sample_full_dag_chandru for ordinal case too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_all_previously_sampled=True\n",
    "# do_interventions={}\n",
    "# n= 10_000\n",
    "# batch_size = 32\n",
    "# delete_all_previously_sampled=True\n",
    "# verbose=True\n",
    "\n",
    "\n",
    "\n",
    "# # delete the previolusly sampled data\n",
    "# if delete_all_previously_sampled:\n",
    "#     delete_all_samplings(target_nodes_dict, EXPERIMENT_DIR)\n",
    "\n",
    "\n",
    "# # repeat process until all nodes are sampled\n",
    "# processed_nodes=[] # stack\n",
    "# while set(processed_nodes) != set(target_nodes_dict.keys()): \n",
    "#     for node in target_nodes_dict: # for each node in the target_nodes_dict\n",
    "#         if node in processed_nodes:\n",
    "#             if verbose :\n",
    "#                 print('node is already  in sampled list')\n",
    "#             continue\n",
    "        \n",
    "#         _, ordered_transformation_terms_in_h, _=ordered_parents(node, target_nodes_dict)\n",
    "\n",
    "        \n",
    "#         print(f'\\n----*----------*-------------*--------Sample Node: {node} ------------*-----------------*-------------------*--') \n",
    "        \n",
    "#         ## 1. Paths \n",
    "#         NODE_DIR = os.path.join(EXPERIMENT_DIR, f'{node}')\n",
    "#         SAMPLING_DIR = os.path.join(NODE_DIR, 'sampling')\n",
    "#         os.makedirs(SAMPLING_DIR, exist_ok=True)\n",
    "        \n",
    "        \n",
    "#         ## 2. Check if sampled and latents already exist \n",
    "#         if check_sampled_and_latents(NODE_DIR, rootfinder='chandrupatla', verbose=verbose):\n",
    "#             processed_nodes.append(node)\n",
    "#             continue\n",
    "        \n",
    "#         ## 3. logic to make sure parents are always sampled first\n",
    "#         skipping_node = False\n",
    "#         if target_nodes_dict[node]['node_type'] != 'source':\n",
    "#             for parent in target_nodes_dict[node]['parents']:\n",
    "#                 if not check_sampled_and_latents(os.path.join(EXPERIMENT_DIR, parent), rootfinder='chandrupatla', verbose=verbose):\n",
    "#                     skipping_node = True\n",
    "#                     break\n",
    "                \n",
    "#         if skipping_node:\n",
    "#             print(f\"Skipping {node} as parent {parent} is not sampled yet.\")\n",
    "#             continue\n",
    "        \n",
    "        \n",
    "        \n",
    "#         ## INTERVENTION, if node is to be intervened on , data is just saved\n",
    "#         if do_interventions: \n",
    "#             if node in do_interventions.keys():\n",
    "#                 intervention_value = do_interventions[node]\n",
    "#                 intervention_vals = torch.full((n,), intervention_value)\n",
    "                \n",
    "                \n",
    "#                 sampled_path = os.path.join(SAMPLING_DIR, \"sampled_chandrupatla.pt\") # TODO change the sampled chandrupatla to differnent name \n",
    "#                 torch.save(intervention_vals, sampled_path)\n",
    "                \n",
    "#                 ### dummy latents jsut for the check , not needed\n",
    "#                 dummy_latents = torch.full((n,), float('nan'))  \n",
    "#                 latents_path = os.path.join(SAMPLING_DIR, \"latents.pt\")\n",
    "#                 torch.save(dummy_latents, latents_path)\n",
    "#                 processed_nodes.append(node)\n",
    "            \n",
    "#         ## no intervention, based on the sampled data from the parents though the latents for each node the observational distribution is generated    \n",
    "#         else:\n",
    "#             ### sampling latents\n",
    "#             latent_sample = torch.tensor(logistic.rvs(size=n), dtype=torch.float32).to(device)\n",
    "#             #latent_sample = truncated_logistic_sample(n=n, low=0, high=1, device=device)\n",
    "            \n",
    "#             if verbose:\n",
    "#                 print(\"-- sampled latents\")\n",
    "            \n",
    "#             ### load modelweights\n",
    "#             model_path = os.path.join(NODE_DIR, \"best_model.pt\")\n",
    "#             tram_model = get_fully_specified_tram_model_v5(node, target_nodes, verbose=True).to(device)\n",
    "#             tram_model.load_state_dict(torch.load(model_path))\n",
    "            \n",
    "#             if verbose:\n",
    "#                 print(\"-- loaded modelweights\")\n",
    "            \n",
    "            \n",
    "#             # TODO samples for ordinal target\n",
    "            \n",
    "                \n",
    "#             dataset = SamplingDataset(node=node, EXPERIMENT_DIR=EXPERIMENT_DIR, rootfinder='chandrupatla', number_of_samples=n, target_nodes=target_nodes_dict, transform=None)\n",
    "#             sample_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25353c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_list = []\n",
    "# with torch.no_grad():\n",
    "#     for x in tqdm(sample_loader, desc=f\"h() for samples in  {node}\"):\n",
    "#         x = [xi.to(device) for xi in x]\n",
    "        \n",
    "#         print(f\"x {x}\")\n",
    "#         int_input, shift_list = preprocess_inputs(x,ordered_transformation_terms_in_h.values(), device=device)\n",
    "        \n",
    "    \n",
    "#         if int_input is not None:\n",
    "#             print(f\"int_input {int_input.shape}\")\n",
    "#         if shift_list is not None:\n",
    "#             print(f\"shift_list {[s.shape for s in shift_list]}\") \n",
    "        \n",
    "#         print([t.shape for t in x])\n",
    "        \n",
    "#         model_outputs = tram_model(int_input=int_input, shift_input=shift_list)\n",
    "        \n",
    "#         print(f\"model_outputs {model_outputs}\")\n",
    "        \n",
    "#         output_list.append(model_outputs)\n",
    "        \n",
    "# if target_nodes_dict[node]['node_type'] == 'source':\n",
    "#     if verbose:\n",
    "#         print(\"source node, Defaults to SI and 1 as inputs\")\n",
    "#     theta_single = output_list[0]['int_out'][0]\n",
    "#     theta_single = transform_intercepts_continous(theta_single)\n",
    "#     thetas_expanded = theta_single.repeat(n, 1)\n",
    "#     shifts = torch.zeros(n, device=device)\n",
    "# else:\n",
    "#     if verbose:\n",
    "#         print(\"node has parents, previously sampled data is loaded for each pa(node)\")\n",
    "#     y_pred = merge_outputs(output_list, skip_nan=True)\n",
    "#     shifts = y_pred['shift_out']\n",
    "#     if shifts is None:\n",
    "#         print(\"shift_out was None; defaulting to zeros.\")\n",
    "#         shifts = torch.zeros(n, device=device)\n",
    "#     thetas = y_pred['int_out']\n",
    "#     thetas_expanded = transform_intercepts_continous(thetas).squeeze()\n",
    "#     shifts = shifts.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c6ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_interventions={}\n",
    "if do_interventions:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1628f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_df_from_sampled(node,target_nodes_dict,SAMPLING_DIR):\n",
    "#     sampling_dict={}\n",
    "#     for parent in target_nodes_dict[node]['parents']:\n",
    "#         sampled=torch.load(os.path.join(SAMPLING_DIR,f\"sampled.pt\"))\n",
    "#         sampling_dict[parent]=sampled\n",
    "#     sampling_df=pd.DataFrame(sampling_dict)\n",
    "#     return sampling_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def criteria_for_continous_modelled_outcome(node,target_nodes_dict):\n",
    "#     if 'yc'in target_nodes_dict[node]['data_type'].lower() or 'continous' in target_nodes_dict[node]['data_type'].lower():\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "\n",
    "# def criteria_for_ordinal_modelled_outcome(node,target_nodes_dict):\n",
    "#     if 'yo'in target_nodes_dict[node]['data_type'].lower() and 'ordinal' in target_nodes_dict[node]['data_type'].lower():\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False  \n",
    "    \n",
    "    \n",
    "# def sample_continous_modelled_target(node,target_nodes_dict,sample_loader,tram_model,latent_sample):\n",
    "    \n",
    "#     _, ordered_transformation_terms_in_h, _=ordered_parents(node, target_nodes_dict)\n",
    "#     output_list = []\n",
    "#     with torch.no_grad():\n",
    "#         for x in tqdm(sample_loader, desc=f\"h() for samples in  {node}\"):\n",
    "#             # x = [xi.to(device) for xi in x]\n",
    "            \n",
    "#             # print(f\"x {x}\")\n",
    "#             # int_input, shift_list = preprocess_inputs_v2(x,ordered_transformation_terms_in_h.values(), device=device)\n",
    "            \n",
    "        \n",
    "#             if int_input is not None:\n",
    "#                 print(f\"int_input {int_input.shape}\")\n",
    "#             if shift_list is not None:\n",
    "#                 print(f\"shift_list {[s.shape for s in shift_list]}\") \n",
    "            \n",
    "#             print([t.shape for t in x])\n",
    "            \n",
    "#             model_outputs = tram_model(int_input=int_input, shift_input=shift_list)\n",
    "            \n",
    "#             print(f\"model_outputs {model_outputs}\")\n",
    "            \n",
    "#             output_list.append(model_outputs)\n",
    "            \n",
    "#     if target_nodes_dict[node]['node_type'] == 'source':\n",
    "#         if verbose:\n",
    "#             print(\"source node, Defaults to SI and 1 as inputs\")\n",
    "#         theta_single = output_list[0]['int_out'][0]\n",
    "#         theta_single = transform_intercepts_continous(theta_single)\n",
    "#         thetas_expanded = theta_single.repeat(n, 1)\n",
    "#         shifts = torch.zeros(n, device=device)\n",
    "#     else:\n",
    "#         if verbose:\n",
    "#             print(\"node has parents, previously sampled data is loaded for each pa(node)\")\n",
    "#         y_pred = merge_outputs(output_list, skip_nan=True)\n",
    "#         shifts = y_pred['shift_out']\n",
    "#         if shifts is None:\n",
    "#             print(\"shift_out was None; defaulting to zeros.\")\n",
    "#             shifts = torch.zeros(n, device=device)\n",
    "#         thetas = y_pred['int_out']\n",
    "#         thetas_expanded = transform_intercepts_continous(thetas).squeeze()\n",
    "#         shifts = shifts.squeeze()\n",
    "    \n",
    "#     low = torch.full((n,), -1e5, device=device)\n",
    "#     high = torch.full((n,), 1e5, device=device)\n",
    "#     min_vals = torch.tensor(target_nodes_dict[node]['min'], dtype=torch.float32).to(device)\n",
    "#     max_vals = torch.tensor(target_nodes_dict[node]['max'], dtype=torch.float32).to(device)\n",
    "#     min_max = torch.stack([min_vals, max_vals], dim=0)\n",
    "    \n",
    "#     ## Root finder using Chandrupatla's method\n",
    "#     def f_vectorized(targets):\n",
    "#         return vectorized_object_function(\n",
    "#             thetas_expanded,\n",
    "#             targets,\n",
    "#             shifts,\n",
    "#             latent_sample,\n",
    "#             k_min=min_max[0],\n",
    "#             k_max=min_max[1]\n",
    "#         )\n",
    "        \n",
    "#     sampled = chandrupatla_root_finder(\n",
    "#         f_vectorized,\n",
    "#         low,\n",
    "#         high,\n",
    "#         max_iter=10_000,\n",
    "#         tol=1e-9\n",
    "#     )\n",
    "#     return sampled\n",
    "    \n",
    "    \n",
    "    \n",
    "# def sample_ordinal_modelled_target(node,target_nodes_dict,sample_loader,tram_model):\n",
    "#     print('not implemented yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9118093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_full_dag_chandru_v2(target_nodes_dict,\n",
    "#                             EXPERIMENT_DIR,\n",
    "#                             device,\n",
    "#                             do_interventions={},\n",
    "#                             n= 10_000,\n",
    "#                             batch_size = 32,\n",
    "#                             delete_all_previously_sampled=True,\n",
    "#                             verbose=True):\n",
    "#     \"\"\"\n",
    "#     Samples data for all nodes in a DAG defined by `conf_dict`, ensuring that each node's\n",
    "#     parents are sampled before the node itself. Supports interventions on any subset of nodes.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     conf_dict : dict\n",
    "#         Dictionary defining the DAG. Each key is a node name, and each value is a config\n",
    "#         dict that includes at least:\n",
    "#             - 'node_type': str, either 'source' or other\n",
    "#             - 'parents': list of parent node names\n",
    "#             - 'min': float, minimum allowed value for the node\n",
    "#             - 'max': float, maximum allowed value for the node\n",
    "\n",
    "#     EXPERIMENT_DIR : str\n",
    "#         Base directory where all per-node directories are located.\n",
    "\n",
    "#     device : torch.device\n",
    "#         The device to run computations on (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "#     do_interventions : dict, optional\n",
    "#         A dictionary specifying interventions for some nodes. Keys are node names (str),\n",
    "#         values are floats. For each intervened node, the specified value is used as the\n",
    "#         sampled value for all samples, and the model is bypassed. e.g. {'x1':1.0}\n",
    "\n",
    "#     n : int, optional\n",
    "#         Number of samples to draw for each node (default is 10_000).\n",
    "\n",
    "#     batch_size : int, optional\n",
    "#         Batch size for model evaluation during sampling (default is 32).\n",
    "\n",
    "#     delete_all_previously_sampled : bool, optional\n",
    "#         If True, removes previously sampled data before starting (default is True).\n",
    "\n",
    "#     verbose : bool, optional\n",
    "#         If True, prints debug/status information (default is True).\n",
    "\n",
    "#     Notes\n",
    "#     -----\n",
    "#     - The function ensures that nodes are only sampled after their parents.\n",
    "#     - Nodes with `node_type='source'` are treated as having no parents.\n",
    "#     - If a node is in `do_interventions`, `sampled_chandrupatla.pt` and a dummy `latents.pt`\n",
    "#       are created, enabling downstream nodes to proceed.\n",
    "#     - Sampling is done using a vectorized root-finding method (Chandrupatla's algorithm).\n",
    "#     \"\"\"\n",
    "\n",
    "\n",
    "#     # delete the previolusly sampled data\n",
    "#     if delete_all_previously_sampled:\n",
    "#         delete_all_samplings(target_nodes_dict, EXPERIMENT_DIR)\n",
    "    \n",
    "    \n",
    "#     # repeat process until all nodes are sampled\n",
    "#     processed_nodes=[] # log the processed nodes in this list\n",
    "#     while set(processed_nodes) != set(target_nodes_dict.keys()): \n",
    "#         for node in target_nodes_dict: # for each node in the target_nodes_dict\n",
    "#             if node in processed_nodes:\n",
    "#                 if verbose :\n",
    "#                     print('node is already  in sampled list')\n",
    "#                 continue\n",
    "                        \n",
    "#             print(f'\\n----*----------*-------------*--------Sample Node: {node} ------------*-----------------*-------------------*--') \n",
    "            \n",
    "#             ## 1. Paths \n",
    "#             NODE_DIR = os.path.join(EXPERIMENT_DIR, f'{node}')\n",
    "#             SAMPLING_DIR = os.path.join(NODE_DIR, 'sampling')\n",
    "#             os.makedirs(SAMPLING_DIR, exist_ok=True)\n",
    "#             SAMPLED_PATH = os.path.join(SAMPLING_DIR, \"sampled_chandrupatla.pt\")\n",
    "#             LATENTS_PATH = os.path.join(SAMPLING_DIR, \"latents.pt\")\n",
    "            \n",
    "            \n",
    "#             ## 2. Check if sampled and latents already exist \n",
    "#             if check_sampled_and_latents(NODE_DIR, rootfinder='chandrupatla', verbose=verbose):\n",
    "#                 processed_nodes.append(node)\n",
    "#                 continue\n",
    "            \n",
    "#             ## 3. logic to make sure parents are always sampled first\n",
    "#             skipping_node = False\n",
    "#             if target_nodes_dict[node]['node_type'] != 'source':\n",
    "#                 for parent in target_nodes_dict[node]['parents']:\n",
    "#                     if not check_sampled_and_latents(os.path.join(EXPERIMENT_DIR, parent), rootfinder='chandrupatla', verbose=verbose):\n",
    "#                         skipping_node = True\n",
    "#                         break\n",
    "                    \n",
    "#             if skipping_node:\n",
    "#                 print(f\"Skipping {node} as parent {parent} is not sampled yet.\")\n",
    "#                 continue\n",
    "            \n",
    "            \n",
    "#             ## INTERVENTION, if node is to be intervened on , data is just saved\n",
    "#             if do_interventions and node in do_interventions.keys():\n",
    "#                     # For interventions make all the values the same for \n",
    "#                     intervention_value = do_interventions[node]\n",
    "#                     intervention_vals = torch.full((n,), intervention_value)\n",
    "#                     torch.save(intervention_vals, SAMPLED_PATH)\n",
    "                    \n",
    "#                     ### dummy latents jsut for the check , not needed\n",
    "#                     dummy_latents = torch.full((n,), float('nan'))  \n",
    "#                     torch.save(dummy_latents, LATENTS_PATH)\n",
    "#                     processed_nodes.append(node)\n",
    "#                     print(f'Interventional data for node {node} is saved')\n",
    "#                     continue\n",
    "                \n",
    "#             ##### %%%%%%% no intervention, based on the sampled data from the parents though the latents for each node the observational distribution is generated    \n",
    "#             else:\n",
    "#                 ### sampling latents\n",
    "#                 latent_sample = torch.tensor(logistic.rvs(size=n), dtype=torch.float32).to(device)\n",
    "                \n",
    "#                 ### load modelweights\n",
    "#                 MODEL_PATH = os.path.join(NODE_DIR, \"best_model.pt\")\n",
    "#                 tram_model = get_fully_specified_tram_model_v5(node, target_nodes, verbose=True).to(device)\n",
    "#                 tram_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "                \n",
    "                \n",
    "#                 # isntead of sample loader use Generic Dataset but the df is just to sampled data from befor -> create df on the fly\n",
    "#                 sampled_df=create_df_from_sampled(node,target_nodes_dict,SAMPLING_DIR)\n",
    "                \n",
    "    \n",
    "#                 transform = transforms.Compose([\n",
    "#                     transforms.Resize((128, 128)),\n",
    "#                     transforms.ToTensor()\n",
    "#                 ])\n",
    "\n",
    "#                 ordered_parents_dataype_dict, ordered_transformation_terms_in_h, _ = ordered_parents(node, target_nodes)\n",
    "                    \n",
    "#                 sample_dataset = GenericDataset_v5(sampled_df,target_col=node,\n",
    "#                                                    target_nodes=target_nodes,\n",
    "#                                                    parents_dataype_dict=ordered_parents_dataype_dict,\n",
    "#                                                    transform=transform,transformation_terms_in_h=ordered_transformation_terms_in_h,\n",
    "#                                                    return_intercept_shift=return_intercept_shift,\n",
    "#                                                    return_y=False)\n",
    "                \n",
    "#                 sample_loader = DataLoader(sample_dataset, batch_size=batch_size, shuffle=True,num_workers=4, pin_memory=True)\n",
    "                \n",
    "                \n",
    "                \n",
    "#                 # # dataset = SamplingDataset(node=node, EXPERIMENT_DIR=EXPERIMENT_DIR, rootfinder='chandrupatla', number_of_samples=n, target_nodes=target_nodes_dict, transform=None)\n",
    "#                 # sample_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "                \n",
    "                \n",
    "#                 ###*************************************************** Continous Modelled Outcome ************************************************\n",
    "                \n",
    "#                 if criteria_for_continous_modelled_outcome(node,target_nodes_dict):\n",
    "#                     sampled=sample_continous_modelled_target(node,target_nodes_dict,sample_loader,tram_model,latent_sample)\n",
    "                    \n",
    "#                 ###*************************************************** Ordinal Modelled Outcome ************************************************\n",
    "                \n",
    "#                 if criteria_for_ordinal_modelled_outcome(node,target_nodes_dict):\n",
    "#                     sampled=sample_ordinal_modelled_target(node,target_nodes_dict,sample_loader,tram_model)\n",
    "                \n",
    "#                 else:\n",
    "#                     raise ValueError(f\"Unsupported data_type '{target_nodes_dict[node]['data_type']}' for node '{node}' in sampling.\")\n",
    "                    \n",
    "                    \n",
    "#                 ###*************************************************** Saving the latenst and sampled  ************************************************\n",
    "#                 if torch.isnan(sampled).any():\n",
    "#                     print(f'Caution! Sampling for {node} consists of NaNs')\n",
    "                    \n",
    "#                 torch.save(sampled, SAMPLED_PATH)\n",
    "#                 torch.save(latent_sample, LATENTS_PATH)\n",
    "                \n",
    "#                 processed_nodes.append(node)\n",
    "                \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
