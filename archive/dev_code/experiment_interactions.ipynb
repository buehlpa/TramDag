{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e076b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Train with GPU support.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU found, train with CPU support.\")\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import logit\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "# own utils\n",
    "from utils.graph import *\n",
    "from utils.tram_models import *\n",
    "from utils.tram_model_helpers import *\n",
    "from utils.tram_data import *\n",
    "from utils.loss_continous import *\n",
    "from utils.sampling_tram_data import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2418a",
   "metadata": {},
   "source": [
    "# 0. Describe Experiment\n",
    "\n",
    "- Modelling interactions -> is it possible to learn the proper functions:\n",
    "\n",
    "h(x4|x1,x2,x3)=SI(1)+CS(x1,x2)+LS(X3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923b899",
   "metadata": {},
   "source": [
    "# 1. Experiments and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b984f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"modelling_interactions7_std\"   ## <--- set experiment name\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "\n",
    "LOG_DIR=\"/home/bule/TramDag/dev_experiment_logs\"\n",
    "EXPERIMENT_DIR = os.path.join(LOG_DIR, experiment_name)\n",
    "DATA_PATH = EXPERIMENT_DIR # <----------- change to different source if needed\n",
    "CONF_DICT_PATH = os.path.join(EXPERIMENT_DIR, f\"configuration.json\")\n",
    "\n",
    "os.makedirs(EXPERIMENT_DIR,exist_ok=True)\n",
    "# check if configration dict already exists if not create:\n",
    "\n",
    "if os.path.exists(CONF_DICT_PATH):\n",
    "    configuration_dict=load_configuration_dict(CONF_DICT_PATH)\n",
    "    print(f\"Loaded existing configuration from {CONF_DICT_PATH}\")\n",
    "else:\n",
    "    configuration_dict=create_and_write_new_configuration_dict(experiment_name,CONF_DICT_PATH,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)\n",
    "    print(f\"Created new configuration file at {CONF_DICT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76549b97",
   "metadata": {},
   "source": [
    "# 2.  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1cc1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x1, x2):\n",
    "    return np.sin(np.pi * x1) * np.cos(np.pi * x2)\n",
    "\n",
    "\n",
    "def dgp_continuous_interactions(n_obs=100, seed=42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Independent variables\n",
    "    x1 = np.random.uniform(0, 2, size=n_obs)\n",
    "    x2 = np.random.uniform(0, 2, size=n_obs)\n",
    "    x3 = np.random.uniform(0, 2, size=n_obs)\n",
    "\n",
    "    \n",
    "    x4 = f1(x1, x2) + 1.5 * x3\n",
    "\n",
    "    df = pd.DataFrame({'x1': x1, 'x2': x2, 'x3': x3, 'x4': x4})\n",
    "    return df\n",
    "\n",
    "\n",
    "EXP_DATA_PATH=os.path.join(DATA_PATH, f\"{experiment_name}.csv\")\n",
    "if not os.path.exists(EXP_DATA_PATH):\n",
    "    df = dgp_continuous_interactions()\n",
    "\n",
    "    print(df.head())\n",
    "    df.to_csv(EXP_DATA_PATH, index=False)\n",
    "else:\n",
    "    df = pd.read_csv(EXP_DATA_PATH)\n",
    "    print(f\"Loaded data from {EXP_DATA_PATH}\")\n",
    "\n",
    "\n",
    "\n",
    "data_type= {'x1':'cont','x2':'cont','x3':'cont','x4':'cont'} # cont:continous, ord:ordinal, oher:everything else than images\n",
    "\n",
    "\n",
    "# write the data types to configuration file\n",
    "write_data_type_to_configuration(data_type, CONF_DICT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e1b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Visualize the 3 interaction functions\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "# f1(x1, x2)\n",
    "ax = fig.add_subplot(131, projection='3d')\n",
    "x = np.linspace(0, 2, 50)\n",
    "y = np.linspace(0, 2, 50)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f1(X, Y)\n",
    "ax.plot_surface(X, Y, Z, alpha=0.8)\n",
    "ax.set_title(\"f1(x1, x2) = sin(pi*x1)*cos(pi*x2)\")\n",
    "ax.set_xlabel(\"x1\")\n",
    "ax.set_ylabel(\"x2\")\n",
    "ax.set_zlabel(\"f1\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.pairplot(df)\n",
    "plt.suptitle(\"\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e5a0fb",
   "metadata": {},
   "source": [
    "## 2.1 train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2733e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split the data\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# 2. Compute quantiles from training data\n",
    "quantiles = train_df.quantile([0.05, 0.95])\n",
    "min_vals = quantiles.loc[0.05]\n",
    "max_vals = quantiles.loc[0.95]\n",
    "\n",
    "# # 3. Normalize all sets using training quantiles\n",
    "# def normalize_with_quantiles(df, min_vals, max_vals):\n",
    "#     return (df - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "# train_df = normalize_with_quantiles(train_df, min_vals, max_vals)\n",
    "# val_df = normalize_with_quantiles(val_df, min_vals, max_vals)\n",
    "# test_df = normalize_with_quantiles(test_df, min_vals, max_vals)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b97f69",
   "metadata": {},
   "source": [
    "# 3. Define graph Structure\n",
    "\n",
    "- define graph and which shift and intercept terms to use\n",
    "\n",
    "this case: h(x4|x1,x2,x3)=SI(1)+CS(x1,x2)+LS(X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedcd902",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_adj_matrix(CONF_DICT_PATH,seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7328e1",
   "metadata": {},
   "source": [
    "# 4. Configuration for the Models\n",
    "\n",
    "- all SI and LS model are generated outmatically since these are shallow NN's\n",
    "- CI and CS have to be defined by the User and can be Passed for each model, -> still there are default networs which one can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314149ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_nn_names_matrix(CONF_DICT_PATH, seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_nodes_information_to_configuration(CONF_DICT_PATH, min_vals, max_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1ee8d",
   "metadata": {},
   "source": [
    "# 5. Train models\n",
    "\n",
    "- set epochs and batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list=['x1','x2','x3','x4'] #<-  set the nodes which have to be trained , useful if further training is required else lsit all vars\n",
    "\n",
    "batch_size = 32#4112\n",
    "epochs = 3# <- if you want a higher numbe rof epochs, set the number higher and it loads the old model and starts from there\n",
    "learning_rate=0.1\n",
    "use_scheduler =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_loop_v2(\n",
    "                   node,\n",
    "                   target_nodes,\n",
    "                   NODE_DIR,\n",
    "                   tram_model,\n",
    "                   train_loader,\n",
    "                   val_loader,\n",
    "                   epochs,\n",
    "                   optimizer,\n",
    "                   use_scheduler,\n",
    "                   scheduler,\n",
    "                   save_linear_shifts=False,\n",
    "                   verbose=1,\n",
    "                   device='cpu'):\n",
    "    \n",
    "        # get all paths  for this training run\n",
    "        MODEL_PATH,LAST_MODEL_PATH,TRAIN_HIST_PATH,VAL_HIST_PATH=model_train_val_paths(NODE_DIR)\n",
    "        \n",
    "        # this is needed for the preprocessing of the inputs such that they are in the correct order\n",
    "        _, ordered_transformation_terms_in_h, _=ordered_parents(node, target_nodes)\n",
    "        \n",
    "        # this is needed for the scaling if there is a bernstein polynomial for contionous outcomes\n",
    "        min_vals = torch.tensor(target_nodes[node]['min'], dtype=torch.float32).to(device)\n",
    "        max_vals = torch.tensor(target_nodes[node]['max'], dtype=torch.float32).to(device)\n",
    "        min_max = torch.stack([min_vals, max_vals], dim=0)\n",
    "        print(f\"Min-Max values for {node}: {min_max}\")\n",
    "        print(min_max.shape)\n",
    "        \n",
    "        \n",
    "        ###### Load Model & History #####\n",
    "        if os.path.exists(MODEL_PATH) and os.path.exists(TRAIN_HIST_PATH) and os.path.exists(VAL_HIST_PATH):\n",
    "            print(\"Existing model found. Loading weights and history...\")\n",
    "            tram_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "            with open(TRAIN_HIST_PATH, 'r') as f:\n",
    "                train_loss_hist = json.load(f)\n",
    "            with open(VAL_HIST_PATH, 'r') as f:\n",
    "                val_loss_hist = json.load(f)\n",
    "\n",
    "            start_epoch = len(train_loss_hist)\n",
    "            best_val_loss = min(val_loss_hist)\n",
    "        else:\n",
    "            print(\"No existing model found. Starting fresh...\")\n",
    "            train_loss_hist, val_loss_hist = [], []\n",
    "            start_epoch = 0\n",
    "            best_val_loss = float('inf')\n",
    "        \n",
    "        ##### Training and Validation loop\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            epoch_start = time.time()\n",
    "\n",
    "            #####  Training #####\n",
    "            train_start = time.time()\n",
    "            train_loss = 0.0\n",
    "            tram_model.train()\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                y = y.to(device)\n",
    "                int_input, shift_list = preprocess_inputs(x, ordered_transformation_terms_in_h.values(), device=device)\n",
    "                y_pred = tram_model(int_input=int_input, shift_input=shift_list)\n",
    "                \n",
    "                \n",
    "                print(f\"y_pred shape: {y_pred}, y shape: {y}\")\n",
    "                print(f'min_max^:{min_max}')\n",
    "                \n",
    "                loss = contram_nll(y_pred, y, min_max=min_max)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "            if use_scheduler:\n",
    "                scheduler.step()\n",
    "                \n",
    "            train_time = time.time() - train_start\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            train_loss_hist.append(avg_train_loss)\n",
    "\n",
    "            ##### Validation #####\n",
    "            val_start = time.time()\n",
    "            val_loss = 0.0\n",
    "            tram_model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x, y in val_loader:\n",
    "                    y = y.to(device)\n",
    "                    int_input, shift_list = preprocess_inputs(x, ordered_transformation_terms_in_h.values(), device=device)\n",
    "                    y_pred = tram_model(int_input=int_input, shift_input=shift_list)\n",
    "                    loss = contram_nll(y_pred, y, min_max=min_max)\n",
    "                    val_loss += loss.item()\n",
    "            val_time = time.time() - val_start\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_loss_hist.append(avg_val_loss)\n",
    "\n",
    "            ##### Save linear shift weights #####\n",
    "\n",
    "            if save_linear_shifts and tram_model.nn_shift is not None:\n",
    "                # Define the path for the cumulative JSON file\n",
    "                shift_path = os.path.join(NODE_DIR, \"linear_shifts_all_epochs.json\")\n",
    "\n",
    "                # Load existing data if the file exists\n",
    "                if os.path.exists(shift_path):\n",
    "                    with open(shift_path, 'r') as f:\n",
    "                        all_shift_weights = json.load(f)\n",
    "                else:\n",
    "                    all_shift_weights = {}\n",
    "\n",
    "                # Prepare current epoch's shift weights\n",
    "                epoch_weights = {}\n",
    "                for i in range(len(tram_model.nn_shift)):\n",
    "                    shift_layer = tram_model.nn_shift[i]\n",
    "                    \n",
    "                    if hasattr(shift_layer, 'fc') and hasattr(shift_layer.fc, 'weight'):\n",
    "                        epoch_weights[f\"shift_{i}\"] = shift_layer.fc.weight.detach().cpu().tolist()\n",
    "                    else:\n",
    "                        print(f\"shift_{i}: 'fc' or 'weight' layer does not exist.\")\n",
    "                \n",
    "                # Add to the dictionary under current epoch\n",
    "                all_shift_weights[f\"epoch_{epoch+1}\"] = epoch_weights\n",
    "                \n",
    "                # Write back the updated dictionary\n",
    "                with open(shift_path, 'w') as f:\n",
    "                    json.dump(all_shift_weights, f)\n",
    "                if verbose > 1:\n",
    "                    print(f'shift weights: {epoch_weights}')\n",
    "                    print(f\"Appended linear shift weights for epoch {epoch+1} to: {shift_path}\")\n",
    "\n",
    "            ##### Saving #####\n",
    "            save_start = time.time()\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save(tram_model.state_dict(), MODEL_PATH)\n",
    "                if verbose > 0:\n",
    "                    print(\"Saved new best model.\")\n",
    "\n",
    "            torch.save(tram_model.state_dict(), LAST_MODEL_PATH)\n",
    "\n",
    "            with open(TRAIN_HIST_PATH, 'w') as f:\n",
    "                json.dump(train_loss_hist, f)\n",
    "            with open(VAL_HIST_PATH, 'w') as f:\n",
    "                json.dump(val_loss_hist, f)\n",
    "            save_time = time.time() - save_start\n",
    "\n",
    "            epoch_total = time.time() - epoch_start\n",
    "\n",
    "            ##### Epoch Summary #####\n",
    "            if verbose>0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "                print(f\"  [Train: {train_time:.2f}s | Val: {val_time:.2f}s | Save: {save_time:.2f}s | Total: {epoch_total:.2f}s]\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df0da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each NODE \n",
    "configuration_dict = load_configuration_dict(CONF_DICT_PATH)\n",
    "target_nodes = configuration_dict['nodes']\n",
    "\n",
    "\n",
    "for node in target_nodes:\n",
    "    \n",
    "    print(f'\\n----*----------*-------------*--------------- Node: {node} ------------*-----------------*-------------------*--')\n",
    "    ########################## 0. Skip nodes ###############################\n",
    "    if node not in train_list:# Skip if node is not in train_list\n",
    "        print(f\"Skipping node {node} as it's not in the training list.\")\n",
    "        continue\n",
    "    if (target_nodes[node]['node_type'] == 'source') and (target_nodes[node]['node_type'] == 'other'):# Skip unsupported types\n",
    "        print(f\"Node type : other , is not supported yet\")\n",
    "        continue\n",
    "\n",
    "    ########################## 1. Setup Paths ###############################\n",
    "    NODE_DIR = os.path.join(EXPERIMENT_DIR, f'{node}')\n",
    "    os.makedirs(NODE_DIR, exist_ok=True)\n",
    "    \n",
    "    # Check if training is complete\n",
    "    if not check_if_training_complete(node, NODE_DIR, epochs):\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    ########################## 2. Create Model ##############################\n",
    "    tram_model = get_fully_specified_tram_model(node, target_nodes, verbose=True).to(device)\n",
    "\n",
    "    \n",
    "    ########################## 3. Create Dataloaders ########################\n",
    "    train_loader, val_loader = get_dataloader(node, target_nodes, train_df, val_df, batch_size=batch_size, verbose=True)\n",
    "\n",
    "    ########################## 5. Optimizer & Scheduler ######################.\n",
    "    \n",
    "    optimizer =torch.optim.Adam(tram_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    ########################## 7. Training Loop ##############################\n",
    "    \n",
    "    train_val_loop_v2(\n",
    "                   node,\n",
    "                   target_nodes,\n",
    "                   NODE_DIR,\n",
    "                   tram_model,\n",
    "                   train_loader,\n",
    "                   val_loader,\n",
    "                   epochs,\n",
    "                   optimizer,\n",
    "                   use_scheduler,\n",
    "                   scheduler,\n",
    "                   save_linear_shifts=False,\n",
    "                   verbose=1,\n",
    "                   device=device)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13c140",
   "metadata": {},
   "source": [
    "# 6 Inspect Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f4e2c7",
   "metadata": {},
   "source": [
    "## 6.1 Loss vs epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a93ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training_history(target_nodes,EXPERIMENT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b5922",
   "metadata": {},
   "source": [
    "## 6.2 inspect transformation function for source nodes h()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b76f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hdag_for_source_nodes(target_nodes,EXPERIMENT_DIR,device=device,xmin_plot=0,xmax_plot=2) # TODO for other nodes funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04399f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_trafo_standart_logistic(target_nodes,EXPERIMENT_DIR,train_df,val_df,device,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e17a4b",
   "metadata": {},
   "source": [
    "### Coefficient estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f050b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # enable 3D plotting\n",
    "\n",
    "# --- Your existing setup ---\n",
    "verbose    = False\n",
    "batch_size = 4112\n",
    "device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# this x is only for overlaying your target curve in 2D plots\n",
    "x1d = torch.linspace(-1, 1, steps=1000).unsqueeze(1).to(device)  # (1000, 1)\n",
    "def f(x):\n",
    "    return 0.75 * np.arctan(5 * (x + 0.12))\n",
    "\n",
    "for node in target_nodes:\n",
    "    print(f\"\\n---- check CS of {node} ----\")\n",
    "    if target_nodes[node]['node_type'] == 'source':\n",
    "        print(\"Node type: source — skipping.\")\n",
    "        continue\n",
    "\n",
    "    # load your model\n",
    "    NODE_DIR   = os.path.join(EXPERIMENT_DIR, node)\n",
    "    model_path = os.path.join(NODE_DIR, \"best_model.pt\")\n",
    "    tram_model = get_fully_specified_tram_model(node, target_nodes, verbose=verbose).to(device)\n",
    "    tram_model.load_state_dict(torch.load(model_path))\n",
    "    tram_model.eval()\n",
    "\n",
    "\n",
    "    for i, module in enumerate(tram_model.nn_shift):\n",
    "        name = module.__class__.__name__\n",
    "        print(f\"\\nModule {i}: {name}\")\n",
    "        print(module)\n",
    "\n",
    "        if name == 'LinearShift':\n",
    "            print(\"  LinearShift weights:\")\n",
    "            print(module.fc.weight.data)\n",
    "            continue\n",
    "\n",
    "        # read wanted input dims\n",
    "        in_feats = module.fc1.in_features\n",
    "        print(f\"  expects input shape = (batch_size, {in_feats})\")\n",
    "\n",
    "        # 2-input case → 3D surface plot\n",
    "        if in_feats == 2:\n",
    "            N = 100\n",
    "            a = torch.linspace(-0, 2, steps=N, device=device)\n",
    "            b = torch.linspace(-0, 2, steps=N, device=device)\n",
    "            A, B = torch.meshgrid(a, b, indexing='ij')       # both (N, N)\n",
    "            grid = torch.stack([A, B], dim=-1).view(-1, 2)   # (N*N, 2)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                Y = module(grid).view(N, N).cpu().numpy()    # back to (N, N)\n",
    "\n",
    "            A_np = A.cpu().numpy()\n",
    "            B_np = B.cpu().numpy()\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax  = fig.add_subplot(111, projection='3d')\n",
    "            ax.plot_surface(A_np, B_np, Y, cmap='viridis', edgecolor='none')\n",
    "            ax.set_xlabel('input 1')\n",
    "            ax.set_ylabel('input 2')\n",
    "            ax.set_zlabel(f'{name}(x₁, x₂)')\n",
    "            ax.set_title(f\"{node} | Module {i}: {name} (3D surface)\")\n",
    "            plt.show()\n",
    "\n",
    "        # 1-input case → 2D scatter\n",
    "        else:\n",
    "            N = 1000\n",
    "            lin = torch.linspace(-1, 1, steps=N, device=device)\n",
    "            dummy = lin.unsqueeze(1).repeat(1, in_feats)      # (N, in_feats)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y = module(dummy).squeeze().cpu().numpy()\n",
    "\n",
    "            plt.figure()\n",
    "            plt.scatter(dummy[:, 0].cpu().numpy(), y, s=5, label=f\"{name} output\")\n",
    "            # overlay target\n",
    "            plt.scatter(x1d.cpu().numpy(), -f(x1d.cpu().numpy()), c=\"red\", s=5, label=\"target\")\n",
    "            plt.xlabel(\"input value\")\n",
    "            plt.ylabel(\"output value\")\n",
    "            plt.title(f\"{node} | Module {i}: {name} (2D)\")\n",
    "            plt.legend()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eeb428",
   "metadata": {},
   "source": [
    "# 7. Sample from Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_full_dag_chandru(target_nodes,\n",
    "                            EXPERIMENT_DIR,\n",
    "                            device,\n",
    "                            n= 100,\n",
    "                            batch_size = 2,\n",
    "                            delete_all_previously_sampled=True,\n",
    "                            verbose=True)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2fc3ba",
   "metadata": {},
   "source": [
    "## 7.3 Inspect Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples_vs_true(test_df,target_nodes,EXPERIMENT_DIR,rootfinder='chandrupatla')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6be30f",
   "metadata": {},
   "source": [
    "## 7.4 Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6986aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return -0.3*x  \n",
    "\n",
    "intervention_df=dgp(10_000, doX=[-1.0, None, None], seed=-1)\n",
    "sns.pairplot(intervention_df)\n",
    "plt.suptitle(\"\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_full_dag_chandru(target_nodes,\n",
    "                            EXPERIMENT_DIR,\n",
    "                            device,\n",
    "                            do_interventions={'x1':-1.0},\n",
    "                            n= 10_000,\n",
    "                            batch_size = 32,\n",
    "                            delete_all_previously_sampled=True,\n",
    "                            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cb87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples_vs_true(intervention_df,target_nodes,EXPERIMENT_DIR,rootfinder='chandrupatla')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
