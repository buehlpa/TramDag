{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e9494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Train with GPU support.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU found, train with CPU support.\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# own utils # TODO resolve namespace pollution by restricitn g importis in init\n",
    "from utils.configuration import *\n",
    "from utils.tram_models import *\n",
    "from utils.tram_model_helpers import *\n",
    "from utils.tram_data import *\n",
    "from utils.loss_continous import *\n",
    "from utils.tram_data_helpers import *\n",
    "from scipy.special import logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67598616",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e597164",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"working_version\"   ## <--- set experiment name\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "\n",
    "LOG_DIR=\"/home/bule/TramDag/dev_experiment_logs\"\n",
    "EXPERIMENT_DIR = os.path.join(LOG_DIR, experiment_name)\n",
    "CONF_DICT_PATH = os.path.join(EXPERIMENT_DIR, f\"configuration.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4807eefe",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(EXPERIMENT_DIR, \"data.csv\"))\n",
    "# 1. Split the data\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# 2. Compute quantiles from training data\n",
    "quantiles = train_df.quantile([0.05, 0.95])\n",
    "min_vals = quantiles.loc[0.05]\n",
    "max_vals = quantiles.loc[0.95]\n",
    "\n",
    "train_df.to_csv(os.path.join(EXPERIMENT_DIR, f\"{experiment_name}_train.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(EXPERIMENT_DIR, f\"{experiment_name}_val.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(EXPERIMENT_DIR, f\"{experiment_name}_test.csv\"), index=False)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380e318",
   "metadata": {},
   "source": [
    "## Load the configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e783960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams:\n",
    "configuration_dict = load_configuration_dict(CONF_DICT_PATH)\n",
    "# if you change the hyperparams -> write them to the config file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c038988",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list=['x1','x2','x3']#<-  set the nodes which have to be trained , useful if further training is required else lsit all vars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad74f8e",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81726cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each NODE \n",
    "node_list = configuration_dict['nodes'] \n",
    "for node in node_list:\n",
    "    print(f'\\n----*----------*-------------*--------------- Node: {node} ------------*-----------------*-------------------*--')\n",
    "\n",
    "    ########################## 0. Get hyperparameters ###############################\n",
    "    # batch_size, epochs, learning_rate, use_scheduler = get_hyperparameters_for_node(node,node_list)\n",
    "    \n",
    "    ############ Edited hyperparameters ##########\n",
    "    batch_size = 512\n",
    "    epochs = 800\n",
    "    learning_rate = 0.01\n",
    "    use_scheduler = False\n",
    "    ###########################################\n",
    "    \n",
    "    print(f\"Using hyperparameters: batch_size={batch_size}, epochs={epochs}, learning_rate={learning_rate}, use_scheduler={use_scheduler}\\n\")\n",
    "    \n",
    "    ########################## 1. Skip nodes ###############################\n",
    "    if node not in train_list:# Skip if node is not in train_list\n",
    "        print(f\"Skipping node {node} as it's not in the training list.\")\n",
    "        continue\n",
    "    if (node_list[node]['node_type'] == 'source') and (node_list[node]['node_type'] == 'other'):# Skip unsupported types\n",
    "        print(f\"Node type : other , is not supported yet\")\n",
    "        continue\n",
    "\n",
    "    ########################## 2. Setup Paths ###############################\n",
    "    NODE_DIR = os.path.join(EXPERIMENT_DIR, f'{node}')\n",
    "    os.makedirs(NODE_DIR, exist_ok=True)\n",
    "    \n",
    "\n",
    "    ########################## 3. Check if training is complete ###############################\n",
    "    if not check_if_training_complete(node, NODE_DIR, epochs):\n",
    "        continue\n",
    "    \n",
    "    ########################## 4. Create Model ##############################\n",
    "    tram_model= get_fully_specified_tram_model(node, configuration_dict, debug=True, set_initial_weights=True)\n",
    "    \n",
    "    ########################## 5. Create Dataloaders ########################\n",
    "    train_loader, val_loader = get_dataloader(node, node_list, train_df, val_df, batch_size=batch_size,return_intercept_shift=True, debug=False)\n",
    "\n",
    "    ########################## 6. Optimizer & Scheduler ######################.\n",
    "    optimizer =torch.optim.Adam(tram_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    ########################## 7. Training Loop ##############################\n",
    "    \n",
    "    train_val_loop(\n",
    "                node,\n",
    "                node_list,\n",
    "                NODE_DIR,\n",
    "                tram_model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                epochs,\n",
    "                optimizer,\n",
    "                use_scheduler,\n",
    "                scheduler,\n",
    "                save_linear_shifts=True,\n",
    "                verbose=1,\n",
    "                device=device,\n",
    "                debug=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad7f5c",
   "metadata": {},
   "source": [
    "# Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf2096a",
   "metadata": {},
   "source": [
    "## training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dbe34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training_history(node_list,EXPERIMENT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c5e7e2",
   "metadata": {},
   "source": [
    "## hdag for sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hdag_for_source_nodes(configuration_dict,EXPERIMENT_DIR,device=device,xmin_plot=0,xmax_plot=1) # TODO for other nodes funciton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5c9c26",
   "metadata": {},
   "source": [
    "## latent distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e710414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_trafo_standart_logistic(configuration_dict,EXPERIMENT_DIR,train_df,val_df,device,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f57e3d",
   "metadata": {},
   "source": [
    "# Sample from trained Tramdag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed30fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the Us from the trained model for a given DF\n",
    "all_latents_df = create_latent_df_for_full_dag(configuration_dict, EXPERIMENT_DIR, train_df, verbose=True)\n",
    "all_latents_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f78d136",
   "metadata": {},
   "source": [
    "## sampled vs true "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b96130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if predefined_latent_samples_df is given, these latents will be used instead of sampling new ones\n",
    "sampled_by_node, latents_by_node=sample_full_dag(configuration_dict,\n",
    "                EXPERIMENT_DIR,\n",
    "                device,\n",
    "                do_interventions={},\n",
    "                predefined_latent_samples_df=None,#all_latents_df,\n",
    "                number_of_samples= 10_000,\n",
    "                batch_size = 32,\n",
    "                delete_all_previously_sampled=True,\n",
    "                verbose=True,\n",
    "                debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c9f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples_vs_true(train_df,node_list,EXPERIMENT_DIR)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
