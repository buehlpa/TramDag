{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32793ca7",
   "metadata": {},
   "source": [
    "# minimal example of ontram implementation\n",
    "- anlogous to https://github.com/liherz/ontram_pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee5f4e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with GPU support.\n"
     ]
    }
   ],
   "source": [
    "# Load dependencies\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from utils.configuration import *\n",
    "from utils.loss_ordinal import *\n",
    "from utils.tram_model_helpers import *\n",
    "from utils.tram_models import *\n",
    "from utils.tram_data import *\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Train with GPU support.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU found, train with CPU support.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ada57",
   "metadata": {},
   "source": [
    "adjustet funcitnos for ordinal outcomes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8678f4f3",
   "metadata": {},
   "source": [
    "dev ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd61311",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"ordinal_wine_example_8\"   ## <--- set experiment name\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "\n",
    "LOG_DIR=\"/home/bule/TramDag/dev_experiment_logs\"\n",
    "EXPERIMENT_DIR = os.path.join(LOG_DIR, experiment_name)\n",
    "DATA_PATH = EXPERIMENT_DIR # <----------- change to different source if needed\n",
    "CONF_DICT_PATH = os.path.join(EXPERIMENT_DIR, f\"configuration.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64449eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
       "         1.065e+03],\n",
       "        [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
       "         1.050e+03],\n",
       "        [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
       "         1.185e+03],\n",
       "        ...,\n",
       "        [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
       "         8.350e+02],\n",
       "        [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
       "         8.400e+02],\n",
       "        [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
       "         5.600e+02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['class_0', 'class_1', 'class_2'], dtype='<U7'),\n",
       " 'DESCR': '.. _wine_dataset:\\n\\nWine recognition dataset\\n------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 178\\n:Number of Attributes: 13 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - Alcohol\\n    - Malic acid\\n    - Ash\\n    - Alcalinity of ash\\n    - Magnesium\\n    - Total phenols\\n    - Flavanoids\\n    - Nonflavanoid phenols\\n    - Proanthocyanins\\n    - Color intensity\\n    - Hue\\n    - OD280/OD315 of diluted wines\\n    - Proline\\n    - class:\\n        - class_0\\n        - class_1\\n        - class_2\\n\\n:Summary Statistics:\\n\\n============================= ==== ===== ======= =====\\n                                Min   Max   Mean     SD\\n============================= ==== ===== ======= =====\\nAlcohol:                      11.0  14.8    13.0   0.8\\nMalic Acid:                   0.74  5.80    2.34  1.12\\nAsh:                          1.36  3.23    2.36  0.27\\nAlcalinity of Ash:            10.6  30.0    19.5   3.3\\nMagnesium:                    70.0 162.0    99.7  14.3\\nTotal Phenols:                0.98  3.88    2.29  0.63\\nFlavanoids:                   0.34  5.08    2.03  1.00\\nNonflavanoid Phenols:         0.13  0.66    0.36  0.12\\nProanthocyanins:              0.41  3.58    1.59  0.57\\nColour Intensity:              1.3  13.0     5.1   2.3\\nHue:                          0.48  1.71    0.96  0.23\\nOD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\\nProline:                       278  1680     746   315\\n============================= ==== ===== ======= =====\\n\\n:Missing Attribute Values: None\\n:Class Distribution: class_0 (59), class_1 (71), class_2 (48)\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThis is a copy of UCI ML Wine recognition datasets.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\\n\\nThe data is the results of a chemical analysis of wines grown in the same\\nregion in Italy by three different cultivators. There are thirteen different\\nmeasurements taken for different constituents found in the three types of\\nwine.\\n\\nOriginal Owners:\\n\\nForina, M. et al, PARVUS -\\nAn Extendible Package for Data Exploration, Classification and Correlation.\\nInstitute of Pharmaceutical and Food Analysis and Technologies,\\nVia Brigata Salerno, 16147 Genoa, Italy.\\n\\nCitation:\\n\\nLichman, M. (2013). UCI Machine Learning Repository\\n[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\\nSchool of Information and Computer Science.\\n\\n.. dropdown:: References\\n\\n    (1) S. Aeberhard, D. Coomans and O. de Vel,\\n    Comparison of Classifiers in High Dimensional Settings,\\n    Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of\\n    Mathematics and Statistics, James Cook University of North Queensland.\\n    (Also submitted to Technometrics).\\n\\n    The data was used with many others for comparing various\\n    classifiers. The classes are separable, though only RDA\\n    has achieved 100% correct classification.\\n    (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data))\\n    (All results using the leave-one-out technique)\\n\\n    (2) S. Aeberhard, D. Coomans and O. de Vel,\\n    \"THE CLASSIFICATION PERFORMANCE OF RDA\"\\n    Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of\\n    Mathematics and Statistics, James Cook University of North Queensland.\\n    (Also submitted to Journal of Chemometrics).\\n',\n",
       " 'feature_names': ['alcohol',\n",
       "  'malic_acid',\n",
       "  'ash',\n",
       "  'alcalinity_of_ash',\n",
       "  'magnesium',\n",
       "  'total_phenols',\n",
       "  'flavanoids',\n",
       "  'nonflavanoid_phenols',\n",
       "  'proanthocyanins',\n",
       "  'color_intensity',\n",
       "  'hue',\n",
       "  'od280/od315_of_diluted_wines',\n",
       "  'proline']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "wine = load_wine()\n",
    "wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9a21723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "alcohol",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "malic_acid",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ash",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "alcalinity_of_ash",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "magnesium",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_phenols",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "flavanoids",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nonflavanoid_phenols",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "proanthocyanins",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "color_intensity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "od280/od315_of_diluted_wines",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "proline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "target",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "b9d4ca96-c7cf-4d03-a3cb-bf26e339cfff",
       "rows": [
        [
         "0",
         "14.23",
         "1.71",
         "2.43",
         "15.6",
         "127.0",
         "2.8",
         "3.06",
         "0.28",
         "2.29",
         "5.64",
         "1.04",
         "3.92",
         "1065.0",
         "0"
        ],
        [
         "1",
         "13.2",
         "1.78",
         "2.14",
         "11.2",
         "100.0",
         "2.65",
         "2.76",
         "0.26",
         "1.28",
         "4.38",
         "1.05",
         "3.4",
         "1050.0",
         "0"
        ],
        [
         "2",
         "13.16",
         "2.36",
         "2.67",
         "18.6",
         "101.0",
         "2.8",
         "3.24",
         "0.3",
         "2.81",
         "5.68",
         "1.03",
         "3.17",
         "1185.0",
         "0"
        ],
        [
         "3",
         "14.37",
         "1.95",
         "2.5",
         "16.8",
         "113.0",
         "3.85",
         "3.49",
         "0.24",
         "2.18",
         "7.8",
         "0.86",
         "3.45",
         "1480.0",
         "0"
        ],
        [
         "4",
         "13.24",
         "2.59",
         "2.87",
         "21.0",
         "118.0",
         "2.8",
         "2.69",
         "0.39",
         "1.82",
         "4.32",
         "1.04",
         "2.93",
         "735.0",
         "0"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          3.92   1065.0       0  \n",
       "1                          3.40   1050.0       0  \n",
       "2                          3.17   1185.0       0  \n",
       "3                          3.45   1480.0       0  \n",
       "4                          2.93    735.0       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(wine['data'], columns=wine['feature_names'])\n",
    "df['target']=wine['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1851b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "quantiles = train_df.quantile([0.05, 0.95])\n",
    "min_vals = quantiles.loc[0.05]\n",
    "max_vals = quantiles.loc[0.95]\n",
    "\n",
    "\n",
    "\n",
    "# # 3. Normalize all sets using training quantiles\n",
    "# def normalize_with_quantiles(df, min_vals, max_vals):\n",
    "#     return (df - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "def normalize_with_quantiles(df, min_vals, max_vals, exclude_columns=None):\n",
    "    \"\"\"\n",
    "    Normalize df using (df - min) / (max - min), but skip any columns in exclude_columns.\n",
    "    min_vals and max_vals should come from the training set.\n",
    "    \"\"\"\n",
    "    exclude_columns = set() if exclude_columns is None else set(exclude_columns)\n",
    "    normalized = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in exclude_columns:\n",
    "            continue\n",
    "        if col not in min_vals.index or col not in max_vals.index:\n",
    "            continue\n",
    "        min_val = min_vals[col]\n",
    "        max_val = max_vals[col]\n",
    "        range_ = max_val - min_val\n",
    "        if range_ == 0 or pd.isna(range_):\n",
    "            normalized[col] = df[col].apply(lambda x: 0.0 if not pd.isna(x) else x)\n",
    "        else:\n",
    "            normalized[col] = (df[col] - min_val) / range_\n",
    "    return normalized\n",
    "\n",
    "\n",
    "\n",
    "train_df = normalize_with_quantiles(train_df, min_vals, max_vals,exclude_columns=['target'])\n",
    "val_df = normalize_with_quantiles(val_df, min_vals, max_vals,exclude_columns=['target'])\n",
    "test_df = normalize_with_quantiles(test_df, min_vals, max_vals,exclude_columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ba3478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "alcohol",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "malic_acid",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ash",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "alcalinity_of_ash",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "magnesium",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_phenols",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "flavanoids",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nonflavanoid_phenols",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "proanthocyanins",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "color_intensity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "od280/od315_of_diluted_wines",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "proline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "target",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9af14ba3-dc10-41d2-8477-29fa90b5dcbd",
       "rows": [
        [
         "158",
         "1.043088321310197",
         "0.17252124645892356",
         "0.940325497287523",
         "1.0",
         "0.3251748251748253",
         "0.7401615845712798",
         "0.25368782161235",
         "0.8290598290598292",
         "0.9464508094645084",
         "1.449339207048459",
         "0.0",
         "0.24918868799258234",
         "0.32017186055365204",
         "2"
        ],
        [
         "137",
         "0.33729771885357723",
         "1.2575070821529752",
         "0.8679927667269443",
         "1.0",
         "0.27855477855477867",
         "0.21370862653114425",
         "0.010120068610634641",
         "1.0732600732600732",
         "0.14943960149439608",
         "0.3649610301592683",
         "0.33852403520649976",
         "0.12401483541956423",
         "0.1624517322020993",
         "2"
        ],
        [
         "98",
         "0.2749073893546495",
         "-0.00028328611898020183",
         "0.2169981916817362",
         "0.4074749316317228",
         "0.09207459207459216",
         "1.1154547823820697",
         "1.0907375643224704",
         "0.12087912087912085",
         "0.5728518057285182",
         "0.2971873941036938",
         "0.6364251861882196",
         "0.6247102457116366",
         "0.32017186055365204",
         "1"
        ],
        [
         "159",
         "0.7077403002534608",
         "0.16968838526912183",
         "0.8679927667269443",
         "0.772105742935278",
         "0.11538461538461547",
         "0.6359134740682828",
         "0.18164665523156098",
         "0.8046398046398048",
         "0.7422166874221671",
         "1.279905116909523",
         "0.0",
         "0.16573945294390363",
         "0.27666285962908577",
         "2"
        ],
        [
         "38",
         "0.5478650809124586",
         "0.12152974504249293",
         "0.2169981916817362",
         "0.13400182315405643",
         "0.3251748251748253",
         "0.5316653635652856",
         "0.7099485420240139",
         "0.21855921855921864",
         "0.28393524283935256",
         "0.1887495764147748",
         "0.8259986459038594",
         "0.5876216968011126",
         "0.7117528688747484",
         "0"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1.043088</td>\n",
       "      <td>0.172521</td>\n",
       "      <td>0.940325</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.325175</td>\n",
       "      <td>0.740162</td>\n",
       "      <td>0.253688</td>\n",
       "      <td>0.829060</td>\n",
       "      <td>0.946451</td>\n",
       "      <td>1.449339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249189</td>\n",
       "      <td>0.320172</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.337298</td>\n",
       "      <td>1.257507</td>\n",
       "      <td>0.867993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.278555</td>\n",
       "      <td>0.213709</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>1.073260</td>\n",
       "      <td>0.149440</td>\n",
       "      <td>0.364961</td>\n",
       "      <td>0.338524</td>\n",
       "      <td>0.124015</td>\n",
       "      <td>0.162452</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.274907</td>\n",
       "      <td>-0.000283</td>\n",
       "      <td>0.216998</td>\n",
       "      <td>0.407475</td>\n",
       "      <td>0.092075</td>\n",
       "      <td>1.115455</td>\n",
       "      <td>1.090738</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.572852</td>\n",
       "      <td>0.297187</td>\n",
       "      <td>0.636425</td>\n",
       "      <td>0.624710</td>\n",
       "      <td>0.320172</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.707740</td>\n",
       "      <td>0.169688</td>\n",
       "      <td>0.867993</td>\n",
       "      <td>0.772106</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.635913</td>\n",
       "      <td>0.181647</td>\n",
       "      <td>0.804640</td>\n",
       "      <td>0.742217</td>\n",
       "      <td>1.279905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.165739</td>\n",
       "      <td>0.276663</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.547865</td>\n",
       "      <td>0.121530</td>\n",
       "      <td>0.216998</td>\n",
       "      <td>0.134002</td>\n",
       "      <td>0.325175</td>\n",
       "      <td>0.531665</td>\n",
       "      <td>0.709949</td>\n",
       "      <td>0.218559</td>\n",
       "      <td>0.283935</td>\n",
       "      <td>0.188750</td>\n",
       "      <td>0.825999</td>\n",
       "      <td>0.587622</td>\n",
       "      <td>0.711753</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\n",
       "158  1.043088    0.172521  0.940325           1.000000   0.325175   \n",
       "137  0.337298    1.257507  0.867993           1.000000   0.278555   \n",
       "98   0.274907   -0.000283  0.216998           0.407475   0.092075   \n",
       "159  0.707740    0.169688  0.867993           0.772106   0.115385   \n",
       "38   0.547865    0.121530  0.216998           0.134002   0.325175   \n",
       "\n",
       "     total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "158       0.740162    0.253688              0.829060         0.946451   \n",
       "137       0.213709    0.010120              1.073260         0.149440   \n",
       "98        1.115455    1.090738              0.120879         0.572852   \n",
       "159       0.635913    0.181647              0.804640         0.742217   \n",
       "38        0.531665    0.709949              0.218559         0.283935   \n",
       "\n",
       "     color_intensity       hue  od280/od315_of_diluted_wines   proline  target  \n",
       "158         1.449339  0.000000                      0.249189  0.320172       2  \n",
       "137         0.364961  0.338524                      0.124015  0.162452       2  \n",
       "98          0.297187  0.636425                      0.624710  0.320172       1  \n",
       "159         1.279905  0.000000                      0.165739  0.276663       2  \n",
       "38          0.188750  0.825999                      0.587622  0.711753       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a156cd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 142 entries, 158 to 102\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   alcohol                       142 non-null    float64\n",
      " 1   malic_acid                    142 non-null    float64\n",
      " 2   ash                           142 non-null    float64\n",
      " 3   alcalinity_of_ash             142 non-null    float64\n",
      " 4   magnesium                     142 non-null    float64\n",
      " 5   total_phenols                 142 non-null    float64\n",
      " 6   flavanoids                    142 non-null    float64\n",
      " 7   nonflavanoid_phenols          142 non-null    float64\n",
      " 8   proanthocyanins               142 non-null    float64\n",
      " 9   color_intensity               142 non-null    float64\n",
      " 10  hue                           142 non-null    float64\n",
      " 11  od280/od315_of_diluted_wines  142 non-null    float64\n",
      " 12  proline                       142 non-null    float64\n",
      " 13  target                        142 non-null    int64  \n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 16.6 KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f49ea145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "target",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "3640905d-511b-4d4f-8483-a3981dd83291",
       "rows": [
        [
         "158",
         "2"
        ],
        [
         "137",
         "2"
        ],
        [
         "98",
         "1"
        ],
        [
         "159",
         "2"
        ],
        [
         "38",
         "0"
        ],
        [
         "108",
         "1"
        ],
        [
         "85",
         "1"
        ],
        [
         "68",
         "1"
        ],
        [
         "143",
         "2"
        ],
        [
         "2",
         "0"
        ],
        [
         "100",
         "1"
        ],
        [
         "122",
         "1"
        ],
        [
         "154",
         "2"
        ],
        [
         "51",
         "0"
        ],
        [
         "76",
         "1"
        ],
        [
         "56",
         "0"
        ],
        [
         "26",
         "0"
        ],
        [
         "153",
         "2"
        ],
        [
         "138",
         "2"
        ],
        [
         "104",
         "1"
        ],
        [
         "78",
         "1"
        ],
        [
         "36",
         "0"
        ],
        [
         "93",
         "1"
        ],
        [
         "22",
         "0"
        ],
        [
         "146",
         "2"
        ],
        [
         "97",
         "1"
        ],
        [
         "69",
         "1"
        ],
        [
         "167",
         "2"
        ],
        [
         "11",
         "0"
        ],
        [
         "6",
         "0"
        ],
        [
         "27",
         "0"
        ],
        [
         "144",
         "2"
        ],
        [
         "4",
         "0"
        ],
        [
         "32",
         "0"
        ],
        [
         "95",
         "1"
        ],
        [
         "170",
         "2"
        ],
        [
         "75",
         "1"
        ],
        [
         "10",
         "0"
        ],
        [
         "147",
         "2"
        ],
        [
         "123",
         "1"
        ],
        [
         "0",
         "0"
        ],
        [
         "142",
         "2"
        ],
        [
         "126",
         "1"
        ],
        [
         "64",
         "1"
        ],
        [
         "44",
         "0"
        ],
        [
         "96",
         "1"
        ],
        [
         "28",
         "0"
        ],
        [
         "40",
         "0"
        ],
        [
         "127",
         "1"
        ],
        [
         "25",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 142
       }
      },
      "text/plain": [
       "158    2\n",
       "137    2\n",
       "98     1\n",
       "159    2\n",
       "38     0\n",
       "      ..\n",
       "71     1\n",
       "106    1\n",
       "14     0\n",
       "92     1\n",
       "102    1\n",
       "Name: target, Length: 142, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cdc6fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "alcohol",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "malic_acid",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ash",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "alcalinity_of_ash",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "magnesium",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_phenols",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "flavanoids",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nonflavanoid_phenols",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "proanthocyanins",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "color_intensity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "od280/od315_of_diluted_wines",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "proline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "target",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "c12c98ac-9ef0-46ed-b531-61245abb2e4d",
       "rows": [
        [
         "158",
         "1.043088321310197",
         "0.17252124645892356",
         "0.940325497287523",
         "1.0",
         "0.3251748251748253",
         "0.7401615845712798",
         "0.25368782161235",
         "0.8290598290598292",
         "0.9464508094645084",
         "1.449339207048459",
         "0.0",
         "0.24918868799258234",
         "0.32017186055365204",
         "2"
        ],
        [
         "137",
         "0.33729771885357723",
         "1.2575070821529752",
         "0.8679927667269443",
         "1.0",
         "0.27855477855477867",
         "0.21370862653114425",
         "0.010120068610634641",
         "1.0732600732600732",
         "0.14943960149439608",
         "0.3649610301592683",
         "0.33852403520649976",
         "0.12401483541956423",
         "0.1624517322020993",
         "2"
        ],
        [
         "98",
         "0.2749073893546495",
         "-0.00028328611898020183",
         "0.2169981916817362",
         "0.4074749316317228",
         "0.09207459207459216",
         "1.1154547823820697",
         "1.0907375643224704",
         "0.12087912087912085",
         "0.5728518057285182",
         "0.2971873941036938",
         "0.6364251861882196",
         "0.6247102457116366",
         "0.32017186055365204",
         "1"
        ],
        [
         "159",
         "0.7077403002534608",
         "0.16968838526912183",
         "0.8679927667269443",
         "0.772105742935278",
         "0.11538461538461547",
         "0.6359134740682828",
         "0.18164665523156098",
         "0.8046398046398048",
         "0.7422166874221671",
         "1.279905116909523",
         "0.0",
         "0.16573945294390363",
         "0.27666285962908577",
         "2"
        ],
        [
         "38",
         "0.5478650809124586",
         "0.12152974504249293",
         "0.2169981916817362",
         "0.13400182315405643",
         "0.3251748251748253",
         "0.5316653635652856",
         "0.7099485420240139",
         "0.21855921855921864",
         "0.28393524283935256",
         "0.1887495764147748",
         "0.8259986459038594",
         "0.5876216968011126",
         "0.7117528688747484",
         "0"
        ],
        [
         "108",
         "0.21641645544940533",
         "0.06203966005665722",
         "0.02411091018685958",
         "0.4530537830446672",
         "0.18531468531468542",
         "0.5108157414646861",
         "0.5041166380789024",
         "0.4871794871794872",
         "0.6376089663760899",
         "0.053202304303625936",
         "0.39268788083953976",
         "0.7406119610570236",
         "-0.05835644749007455",
         "1"
        ],
        [
         "85",
         "0.39188925716513917",
         "-0.025779036827195536",
         "0.38577456298975327",
         "0.3618960802187784",
         "0.3484848484848486",
         "0.4274172530622885",
         "0.4698113207547171",
         "0.2673992673992674",
         "0.32876712328767127",
         "0.04235852253473402",
         "0.8937034529451594",
         "0.8055169216504405",
         "0.09174960569967909",
         "1"
        ],
        [
         "68",
         "0.6531487619418989",
         "-0.03711048158640235",
         "0.5304400241109102",
         "0.27073837739288964",
         "0.6048951048951051",
         "0.5994266353922336",
         "0.25025728987993145",
         "0.877899877899878",
         "-0.1892901618929017",
         "0.11690952219586587",
         "0.6093432633716996",
         "0.23528048215113587",
         "0.41806711263392615",
         "1"
        ],
        [
         "143",
         "0.7623318385650221",
         "1.09886685552408",
         "0.5183845690174806",
         "0.544211485870556",
         "0.18531468531468542",
         "0.32316914255929124",
         "0.07873070325900518",
         "0.6825396825396826",
         "0.10958904109589043",
         "0.283632666892579",
         "0.46039268788083976",
         "0.2909133055169216",
         "0.20052210801109477",
         "2"
        ],
        [
         "2",
         "0.5829596412556053",
         "0.36515580736543923",
         "0.9041591320072334",
         "0.41659070191431186",
         "0.39510489510489527",
         "0.7401615845712798",
         "0.9157804459691256",
         "0.2673992673992674",
         "1.0012453300124535",
         "0.4571331751948495",
         "0.6228842247799596",
         "0.8101529902642559",
         "0.8912274976885843",
         "0"
        ],
        [
         "100",
         "0.16182491713784336",
         "0.28583569405099163",
         "-0.2652200120554551",
         "0.316317228805834",
         "0.30186480186480197",
         "0.443054469637738",
         "0.5487135506003432",
         "0.16971916971916975",
         "0.29887920298879206",
         "0.1345306675703152",
         "0.9478672985781994",
         "0.7127955493741308",
         "0.3745581117093599",
         "1"
        ],
        [
         "122",
         "0.2944043673230647",
         "0.9515580736543914",
         "0.9764918625678121",
         "1.1367365542388332",
         "0.41841491841491857",
         "0.4274172530622885",
         "0.5349914236706691",
         "0.5848595848595849",
         "0.45330012453300134",
         "-0.030837004405286368",
         "0.47393364928909976",
         "0.7869726471951786",
         "-0.000707021265024239",
         "1"
        ],
        [
         "154",
         "0.3567946968219924",
         "0.06203966005665722",
         "0.2169981916817362",
         "0.544211485870556",
         "0.44172494172494187",
         "0.05212405525149863",
         "0.0032590051457975834",
         "0.8290598290598292",
         "0.29887920298879206",
         "0.7173839376482553",
         "0.013540961408260001",
         "0.059109874826147504",
         "0.2984173600913689",
         "2"
        ],
        [
         "51",
         "0.844219146032365",
         "0.16402266288951842",
         "0.8197709463532251",
         "0.2889699179580673",
         "0.23193473193473205",
         "0.557727391191035",
         "0.8300171526586624",
         "0.07203907203907203",
         "0.7422166874221671",
         "0.4462893934259575",
         "0.9072444143534194",
         "0.9028743625405657",
         "0.9782454995377169",
         "0"
        ],
        [
         "76",
         "0.5322674985377263",
         "-0.048441926345609135",
         "-0.2531645569620253",
         "0.17958067456700083",
         "0.04545454545454553",
         "0.29710711493354197",
         "0.5006861063464838",
         "0.12087912087912085",
         "0.32876712328767127",
         "0.31074212131480866",
         "0.8395396073121194",
         "0.49026425591098755",
         "0.028661554359057997",
         "1"
        ],
        [
         "56",
         "0.9962955741860015",
         "0.17818696883852694",
         "0.4581072935503315",
         "0.20692798541476753",
         "0.7913752913752916",
         "0.9486578055772743",
         "0.8334476843910809",
         "0.16971916971916975",
         "0.6127023661270238",
         "0.5520162656726537",
         "0.5010155721056196",
         "0.8750579508576728",
         "0.6573666177190406",
         "0"
        ],
        [
         "26",
         "0.6726457399103141",
         "0.1980169971671389",
         "0.8438818565400847",
         "0.18869644484958983",
         "0.20862470862470872",
         "0.7662236121970293",
         "0.8128644939965697",
         "0.36507936507936517",
         "0.3237858032378581",
         "0.3378515757370385",
         "0.47393364928909976",
         "0.8333333333333335",
         "0.9021047479197258",
         "0"
        ],
        [
         "153",
         "0.6102554104113863",
         "0.6314447592067991",
         "0.43399638336347185",
         "0.4074749316317228",
         "0.3251748251748253",
         "0.21892103205629412",
         "0.08902229845626074",
         "1.0244200244200243",
         "0.5330012453300127",
         "1.1131819722128098",
         "-0.013540961408259852",
         "0.04056560037088555",
         "0.3364877359003644",
         "2"
        ],
        [
         "138",
         "0.7116396958471437",
         "0.7135977337110485",
         "0.32549728752260404",
         "0.49863263445761163",
         "0.09207459207459216",
         "0.12509773260359672",
         "-0.03104631217838767",
         "0.9511599511599511",
         "0.0398505603985056",
         "0.4598441206370725",
         "0.3249830737982399",
         "0.1842837273991656",
         "0.2331538587045195",
         "2"
        ],
        [
         "104",
         "0.3294989276662114",
         "0.18668555240793205",
         "0.07233273056057873",
         "0.5897903372835004",
         "0.022144522144522213",
         "0.4274172530622885",
         "0.46295025728988004",
         "0.3162393162393163",
         "0.3387297633872977",
         "0.08573364961030162",
         "0.6364251861882196",
         "0.9955957348168752",
         "0.3332245608310219",
         "1"
        ],
        [
         "78",
         "0.2593098069799179",
         "-0.02294617563739383",
         "0.03616636528028937",
         "0.07019143117593434",
         "1.2109557109557112",
         "0.27104508730779264",
         "0.4389365351629504",
         "0.38949938949938945",
         "0.9763387297633874",
         "0.1480853947814301",
         "0.6635071090047396",
         "0.4114510894761243",
         "0.41806711263392615",
         "1"
        ],
        [
         "36",
         "0.6297523883798007",
         "0.16118980169971672",
         "1.1091018685955396",
         "0.13400182315405643",
         "0.6048951048951051",
         "0.6359134740682828",
         "0.723670668953688",
         "0.36507936507936517",
         "0.27895392278953934",
         "0.31074212131480866",
         "0.7041299932295196",
         "0.629346314325452",
         "0.5594713656387664",
         "0"
        ],
        [
         "93",
         "0.2437122246051856",
         "0.49830028328611925",
         "0.3616636528028937",
         "0.3618960802187784",
         "0.09207459207459216",
         "0.557727391191035",
         "0.5761578044596914",
         "0.1452991452991453",
         "0.592777085927771",
         "-0.02134869535750597",
         "0.7853757616790793",
         "0.8704218822438572",
         "-0.082286397998586",
         "1"
        ],
        [
         "22",
         "0.7974263989081696",
         "0.22351274787535422",
         "0.5304400241109102",
         "0.23427529626253424",
         "0.39510489510489527",
         "0.6411258795934326",
         "0.7922813036020585",
         "0.1941391941391942",
         "0.44333748443337495",
         "0.20230430362588964",
         "0.7312119160460396",
         "1.1949466852109412",
         "0.7280687442214608",
         "0"
        ],
        [
         "146",
         "0.8637161240007802",
         "1.1243626062322951",
         "0.3737191078963232",
         "0.544211485870556",
         "-0.09440559440559436",
         "-0.20849622100599427",
         "-0.07907375643224701",
         "0.5115995115995117",
         "-0.05977584059775842",
         "0.35140630294815345",
         "0.013540961408260001",
         "-0.04288363467779314",
         "0.053679229890683605",
         "2"
        ],
        [
         "97",
         "0.2437122246051856",
         "0.0960339943342776",
         "0.07233273056057873",
         "0.17958067456700083",
         "0.022144522144522213",
         "0.6098514464425334",
         "0.6619210977701546",
         "0.24297924297924295",
         "0.4831880448318806",
         "0.08031175872585566",
         "0.8937034529451594",
         "0.6108020398701902",
         "0.06781965519116764",
         "1"
        ],
        [
         "69",
         "0.21251705985572242",
         "0.03371104815864018",
         "-0.20494273658830614",
         "0.2525068368277119",
         "1.560606060606061",
         "0.24498305968204342",
         "0.24339622641509442",
         "-0.1233211233211233",
         "0.8468244084682444",
         "0.07353439512029825",
         "0.9614082599864594",
         "0.763792304126101",
         "0.3832599118942731",
         "1"
        ],
        [
         "167",
         "0.450380191070384",
         "0.6512747875354111",
         "0.4581072935503315",
         "0.49863263445761163",
         "0.09207459207459216",
         "0.05212405525149863",
         "0.030703259005145814",
         "0.5115995115995117",
         "0.0846824408468244",
         "1.0779396814639113",
         "0.20311442112389988",
         "0.15183124710245718",
         "0.34736498613150596",
         "2"
        ],
        [
         "11",
         "0.9573016182491711",
         "0.11586402266288953",
         "0.48221820373719104",
         "0.2525068368277119",
         "0.2552447552447554",
         "0.4274172530622885",
         "0.637907375643225",
         "0.16971916971916975",
         "0.3835616438356166",
         "0.3649610301592683",
         "0.8124576844955993",
         "0.6478905887807139",
         "0.9945613748844292",
         "0"
        ],
        [
         "6",
         "1.0625852992786122",
         "0.2263456090651559",
         "0.6389391199517785",
         "0.05195989061075648",
         "0.27855477855477867",
         "0.5837894188167841",
         "0.6687821612349916",
         "0.2673992673992674",
         "0.5877957658779578",
         "0.3988478481870555",
         "0.6093432633716996",
         "1.000231803430691",
         "1.0054386251155707",
         "0"
        ],
        [
         "27",
         "0.6375511795671673",
         "0.18385269121813036",
         "0.26522001205545537",
         "0.27073837739288964",
         "0.23193473193473205",
         "0.5316653635652856",
         "0.5555746140651803",
         "0.1941391941391942",
         "0.2739726027397261",
         "0.222636394442562",
         "0.6093432633716996",
         "0.6247102457116366",
         "1.0",
         "0"
        ],
        [
         "144",
         "0.22811464223045402",
         "0.7957507082152978",
         "0.3375527426160341",
         "0.4074749316317228",
         "0.6515151515151517",
         "0.0",
         "0.07186963979416812",
         "0.24297924297924295",
         "0.1693648816936488",
         "0.8000677736360562",
         "0.10832769126608001",
         "0.2677329624478443",
         "0.5322782400609126",
         "2"
        ],
        [
         "4",
         "0.6141548060050692",
         "0.4303116147308783",
         "1.145268233875829",
         "0.6353691886964448",
         "0.7913752913752916",
         "0.7401615845712798",
         "0.7271012006861065",
         "0.4871794871794872",
         "0.5080946450809466",
         "0.2727888851236871",
         "0.6364251861882196",
         "0.6988873435326844",
         "0.4017512372872138",
         "0"
        ],
        [
         "32",
         "0.7857282121271202",
         "0.2150141643059491",
         "0.5304400241109102",
         "0.2889699179580673",
         "0.4650349650349652",
         "0.5420901746155853",
         "0.7271012006861065",
         "0.5604395604395604",
         "0.5828144458281446",
         "0.20772619451033558",
         "0.8937034529451594",
         "0.6710709318497915",
         "0.6791211181813237",
         "0"
        ],
        [
         "95",
         "0.31390134529147984",
         "0.12719546742209634",
         "0.3375527426160341",
         "0.4530537830446672",
         "1.8170163170163174",
         "0.5837894188167841",
         "0.5830188679245285",
         "0.3162393162393163",
         "1.2353673723536738",
         "0.03964757709251104",
         "0.7989167230873393",
         "0.5598052851182197",
         "0.6214716919562734",
         "1"
        ],
        [
         "170",
         "0.20861766426203884",
         "0.5549575070821532",
         "0.48221820373719104",
         "0.4530537830446672",
         "0.27855477855477867",
         "-0.0677612718269481",
         "-0.02761578044596914",
         "0.5115995115995117",
         "-0.034869240348692446",
         "0.4327346662148427",
         "0.12186865267434002",
         "0.1889197960129811",
         "0.1570131070865285",
         "2"
        ],
        [
         "75",
         "-0.0019496977968417953",
         "0.22917847025495755",
         "0.0",
         "0.17958067456700083",
         "0.30186480186480197",
         "0.11988532707844685",
         "0.3428816466552317",
         "0.36507936507936517",
         "0.174346201743462",
         "0.20230430362588964",
         "0.8937034529451594",
         "0.3326379230412611",
         "0.06781965519116764",
         "1"
        ],
        [
         "10",
         "0.9495028270618053",
         "0.30849858356940524",
         "0.4581072935503315",
         "0.3618960802187784",
         "0.4883449883449885",
         "0.8183476674485279",
         "0.9432246998284737",
         "0.07203907203907203",
         "0.7870485678704858",
         "0.4666214842426299",
         "0.9207853757616794",
         "0.8101529902642559",
         "1.2447381302006852",
         "0"
        ],
        [
         "147",
         "0.4698771690387985",
         "1.0025495750708222",
         "0.6751054852320677",
         "0.6809480401093893",
         "0.04545454545454553",
         "0.1667969768047955",
         "0.027272727272727285",
         "0.6825396825396826",
         "0.029887920298879184",
         "0.7241613012538128",
         "-0.04062288422477985",
         "0.20282800185442754",
         "0.2821014847446565",
         "2"
        ],
        [
         "123",
         "0.5400662897250927",
         "1.3396600566572243",
         "0.2531645569620253",
         "0.6809480401093893",
         "0.04545454545454553",
         "0.6463382851185825",
         "0.7133790737564324",
         "0.2673992673992674",
         "0.6027397260273973",
         "0.03964757709251104",
         "0.21665538253215988",
         "0.7777005099675476",
         "0.015608854081688113",
         "1"
        ],
        [
         "0",
         "1.0001949697796844",
         "0.18101983002832867",
         "0.6148282097649189",
         "0.14311759343664526",
         "1.0011655011655014",
         "0.7401615845712798",
         "0.854030874785592",
         "0.21855921855921864",
         "0.7422166874221671",
         "0.45171128431040347",
         "0.6364251861882196",
         "1.1578581363004172",
         "0.7607004949148855",
         "0"
        ],
        [
         "142",
         "0.7233378826281924",
         "0.594617563739377",
         "0.9644364074743826",
         "0.8632634457611668",
         "0.30186480186480197",
         "0.08861089392754767",
         "-0.017324185248713552",
         "0.7557997557997559",
         "-0.12453300124533005",
         "0.27685530328702146",
         "0.43331076506431976",
         "0.29554937413073723",
         "0.16789035731767007",
         "2"
        ],
        [
         "126",
         "0.29830376291674754",
         "0.13002832861189806",
         "0.44605183845690194",
         "0.6809480401093893",
         "0.04545454545454553",
         "0.7088871514203808",
         "0.8849056603773587",
         "0.4871794871794872",
         "0.4831880448318806",
         "0.22128092172145047",
         "0.16249153689911988",
         "0.6571627260083449",
         "-0.014847446565508278",
         "1"
        ],
        [
         "64",
         "0.19691947748099015",
         "0.10736543909348442",
         "0.7353827606992164",
         "0.4530537830446672",
         "0.4650349650349652",
         "0.2658326817826428",
         "0.4046312178387651",
         "0.6336996336996338",
         "0.11457036114570364",
         "0.08708912233141315",
         "1.191604603926879",
         "0.3743625405656004",
         "-0.011584271496165808",
         "1"
        ],
        [
         "44",
         "0.5400662897250927",
         "0.1980169971671389",
         "0.2169981916817362",
         "0.27073837739288964",
         "0.5349650349650351",
         "0.8444096950742771",
         "0.8334476843910809",
         "0.21855921855921864",
         "0.6127023661270238",
         "0.37038292104371423",
         "0.41976980365605976",
         "0.8936022253129348",
         "0.5649099907543372",
         "0"
        ],
        [
         "96",
         "0.05654123610840306",
         "0.29716713881019846",
         "0.9885473176612422",
         "0.6809480401093893",
         "1.1643356643356646",
         "0.11467292155329699",
         "0.14391080617495716",
         "-0.1233211233211233",
         "0.37858032378580336",
         "0.02609284988139614",
         "0.5145565335138796",
         "0.3882707464070468",
         "0.2821014847446565",
         "1"
        ],
        [
         "28",
         "0.8598167284070966",
         "0.23484419263456097",
         "1.0608800482218204",
         "0.4895168641750226",
         "0.5349650349650351",
         "0.8183476674485279",
         "0.8231560891938253",
         "0.43833943833943834",
         "0.4782067247820674",
         "0.2971873941036938",
         "0.9207853757616794",
         "0.9167825683820121",
         "0.597541741447762",
         "0"
        ],
        [
         "40",
         "0.7389354650029247",
         "0.18101983002832867",
         "0.4701627486437615",
         "0.19781221513217853",
         "0.7680652680652683",
         "0.9225957779515249",
         "0.9329331046312181",
         "0.36507936507936517",
         "0.7671232876712331",
         "0.5181294476448665",
         "0.5145565335138796",
         "0.9075104311543811",
         "0.4670147386740632",
         "0"
        ],
        [
         "127",
         "0.04874244492103657",
         "0.3000000000000001",
         "1.0367691380349608",
         "1.3190519598906107",
         "0.18531468531468542",
         "0.3909304143862394",
         "0.572727272727273",
         "0.9511599511599511",
         "0.4782067247820674",
         "0.09386648593697057",
         "0.5416384563303996",
         "0.4717199814557256",
         "0.1091532060695056",
         "1"
        ],
        [
         "25",
         "0.5400662897250927",
         "0.27733711048158644",
         "1.5672091621458715",
         "1.0",
         "0.9312354312354315",
         "0.6515506906437323",
         "0.723670668953688",
         "0.6825396825396826",
         "0.5579078455790786",
         "0.1724839037614369",
         "0.7582938388625593",
         "0.8240611961057025",
         "0.5050851144830587",
         "0"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 142
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1.043088</td>\n",
       "      <td>0.172521</td>\n",
       "      <td>0.940325</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.325175</td>\n",
       "      <td>0.740162</td>\n",
       "      <td>0.253688</td>\n",
       "      <td>0.829060</td>\n",
       "      <td>0.946451</td>\n",
       "      <td>1.449339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249189</td>\n",
       "      <td>0.320172</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.337298</td>\n",
       "      <td>1.257507</td>\n",
       "      <td>0.867993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.278555</td>\n",
       "      <td>0.213709</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>1.073260</td>\n",
       "      <td>0.149440</td>\n",
       "      <td>0.364961</td>\n",
       "      <td>0.338524</td>\n",
       "      <td>0.124015</td>\n",
       "      <td>0.162452</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.274907</td>\n",
       "      <td>-0.000283</td>\n",
       "      <td>0.216998</td>\n",
       "      <td>0.407475</td>\n",
       "      <td>0.092075</td>\n",
       "      <td>1.115455</td>\n",
       "      <td>1.090738</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.572852</td>\n",
       "      <td>0.297187</td>\n",
       "      <td>0.636425</td>\n",
       "      <td>0.624710</td>\n",
       "      <td>0.320172</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.707740</td>\n",
       "      <td>0.169688</td>\n",
       "      <td>0.867993</td>\n",
       "      <td>0.772106</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.635913</td>\n",
       "      <td>0.181647</td>\n",
       "      <td>0.804640</td>\n",
       "      <td>0.742217</td>\n",
       "      <td>1.279905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.165739</td>\n",
       "      <td>0.276663</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.547865</td>\n",
       "      <td>0.121530</td>\n",
       "      <td>0.216998</td>\n",
       "      <td>0.134002</td>\n",
       "      <td>0.325175</td>\n",
       "      <td>0.531665</td>\n",
       "      <td>0.709949</td>\n",
       "      <td>0.218559</td>\n",
       "      <td>0.283935</td>\n",
       "      <td>0.188750</td>\n",
       "      <td>0.825999</td>\n",
       "      <td>0.587622</td>\n",
       "      <td>0.711753</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.855917</td>\n",
       "      <td>0.124363</td>\n",
       "      <td>0.904159</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.818348</td>\n",
       "      <td>0.785420</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.533001</td>\n",
       "      <td>0.145374</td>\n",
       "      <td>1.069736</td>\n",
       "      <td>0.805517</td>\n",
       "      <td>0.048241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.228115</td>\n",
       "      <td>0.186686</td>\n",
       "      <td>0.241109</td>\n",
       "      <td>0.453054</td>\n",
       "      <td>-0.094406</td>\n",
       "      <td>0.140735</td>\n",
       "      <td>0.500686</td>\n",
       "      <td>0.438339</td>\n",
       "      <td>0.413450</td>\n",
       "      <td>0.148085</td>\n",
       "      <td>0.582261</td>\n",
       "      <td>0.810153</td>\n",
       "      <td>0.157013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.058686</td>\n",
       "      <td>0.226346</td>\n",
       "      <td>0.554551</td>\n",
       "      <td>-0.185050</td>\n",
       "      <td>0.418415</td>\n",
       "      <td>1.000782</td>\n",
       "      <td>1.053002</td>\n",
       "      <td>0.242979</td>\n",
       "      <td>1.075965</td>\n",
       "      <td>0.703829</td>\n",
       "      <td>0.853081</td>\n",
       "      <td>0.731340</td>\n",
       "      <td>1.284984</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.399688</td>\n",
       "      <td>0.130028</td>\n",
       "      <td>0.409885</td>\n",
       "      <td>0.608022</td>\n",
       "      <td>-0.094406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305146</td>\n",
       "      <td>0.951160</td>\n",
       "      <td>0.408468</td>\n",
       "      <td>0.100644</td>\n",
       "      <td>0.528097</td>\n",
       "      <td>0.295549</td>\n",
       "      <td>0.140697</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.263209</td>\n",
       "      <td>0.390652</td>\n",
       "      <td>0.650995</td>\n",
       "      <td>0.635369</td>\n",
       "      <td>0.325175</td>\n",
       "      <td>0.615064</td>\n",
       "      <td>0.528130</td>\n",
       "      <td>0.365079</td>\n",
       "      <td>0.254047</td>\n",
       "      <td>0.066757</td>\n",
       "      <td>0.311442</td>\n",
       "      <td>0.907510</td>\n",
       "      <td>0.078697</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\n",
       "158  1.043088    0.172521  0.940325           1.000000   0.325175   \n",
       "137  0.337298    1.257507  0.867993           1.000000   0.278555   \n",
       "98   0.274907   -0.000283  0.216998           0.407475   0.092075   \n",
       "159  0.707740    0.169688  0.867993           0.772106   0.115385   \n",
       "38   0.547865    0.121530  0.216998           0.134002   0.325175   \n",
       "..        ...         ...       ...                ...        ...   \n",
       "71   0.855917    0.124363  0.904159           1.000000   0.045455   \n",
       "106  0.228115    0.186686  0.241109           0.453054  -0.094406   \n",
       "14   1.058686    0.226346  0.554551          -0.185050   0.418415   \n",
       "92   0.399688    0.130028  0.409885           0.608022  -0.094406   \n",
       "102  0.263209    0.390652  0.650995           0.635369   0.325175   \n",
       "\n",
       "     total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "158       0.740162    0.253688              0.829060         0.946451   \n",
       "137       0.213709    0.010120              1.073260         0.149440   \n",
       "98        1.115455    1.090738              0.120879         0.572852   \n",
       "159       0.635913    0.181647              0.804640         0.742217   \n",
       "38        0.531665    0.709949              0.218559         0.283935   \n",
       "..             ...         ...                   ...              ...   \n",
       "71        0.818348    0.785420              0.047619         0.533001   \n",
       "106       0.140735    0.500686              0.438339         0.413450   \n",
       "14        1.000782    1.053002              0.242979         1.075965   \n",
       "92        0.000000    0.305146              0.951160         0.408468   \n",
       "102       0.615064    0.528130              0.365079         0.254047   \n",
       "\n",
       "     color_intensity       hue  od280/od315_of_diluted_wines   proline  target  \n",
       "158         1.449339  0.000000                      0.249189  0.320172       2  \n",
       "137         0.364961  0.338524                      0.124015  0.162452       2  \n",
       "98          0.297187  0.636425                      0.624710  0.320172       1  \n",
       "159         1.279905  0.000000                      0.165739  0.276663       2  \n",
       "38          0.188750  0.825999                      0.587622  0.711753       0  \n",
       "..               ...       ...                           ...       ...     ...  \n",
       "71          0.145374  1.069736                      0.805517  0.048241       1  \n",
       "106         0.148085  0.582261                      0.810153  0.157013       1  \n",
       "14          0.703829  0.853081                      0.731340  1.284984       0  \n",
       "92          0.100644  0.528097                      0.295549  0.140697       1  \n",
       "102         0.066757  0.311442                      0.907510  0.078697       1  \n",
       "\n",
       "[142 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0327150",
   "metadata": {},
   "source": [
    "data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90e93c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alcohol': 'continous',\n",
       " 'malic_acid': 'continous',\n",
       " 'ash': 'continous',\n",
       " 'alcalinity_of_ash': 'continous',\n",
       " 'magnesium': 'continous',\n",
       " 'total_phenols': 'continous',\n",
       " 'flavanoids': 'continous',\n",
       " 'nonflavanoid_phenols': 'continous',\n",
       " 'proanthocyanins': 'continous',\n",
       " 'color_intensity': 'continous',\n",
       " 'hue': 'continous',\n",
       " 'od280/od315_of_diluted_wines': 'continous',\n",
       " 'proline': 'continous',\n",
       " 'target': 'ordinal_Xc_Yo'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type={key:value for key, value in zip(train_df.columns, ['continous']*13+['ordinal_Xc_Yo'])}\n",
    "data_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31eb799",
   "metadata": {},
   "source": [
    "configartion dicitonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18adb654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date_of_creation': '2025-08-04 08:32:08',\n",
       " 'experiment_name': 'ordinal_wine_example_8',\n",
       " 'PATHS': {'DATA_PATH': '/home/bule/TramDag/dev_experiment_logs/ordinal_wine_example_8',\n",
       "  'LOG_DIR': '/home/bule/TramDag/dev_experiment_logs',\n",
       "  'EXPERIMENT_DIR': '/home/bule/TramDag/dev_experiment_logs/ordinal_wine_example_8'},\n",
       " 'data_type': None,\n",
       " 'adj_matrix': None,\n",
       " 'model_names': None,\n",
       " 'seed': None,\n",
       " 'nodes': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration_dict=new_conf_dict(experiment_name,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)\n",
    "configuration_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a49ae0",
   "metadata": {},
   "source": [
    "modeloling it as a graph with one sink node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6a166ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        '0']], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type={key:value for key, value in zip(train_df.columns, ['continous']*13+['ordinal_Xc_Yo'])}\n",
    "\n",
    "\n",
    "columns = [\n",
    "    'alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium',\n",
    "    'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins',\n",
    "    'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline', 'target'\n",
    "]\n",
    "\n",
    "adj_matrix = np.full((len(columns), len(columns)), \"0\", dtype=object)\n",
    "\n",
    "# Set last column (edges *to* 'target') as \"ls\", excluding self-loop\n",
    "for i in range(len(columns) - 1):\n",
    "    adj_matrix[i, -1] = \"ls\"\n",
    "    \n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "nn_names_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6ece2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alcohol': {'Modelnr': 0,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 11.665000000000001,\n",
       "  'max': 14.2295,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'malic_acid': {'Modelnr': 1,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 1.0710000000000002,\n",
       "  'max': 4.600999999999998,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'ash': {'Modelnr': 2,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 1.92,\n",
       "  'max': 2.7495,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'alcalinity_of_ash': {'Modelnr': 3,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 14.030000000000001,\n",
       "  'max': 25.0,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'magnesium': {'Modelnr': 4,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 84.05,\n",
       "  'max': 126.94999999999999,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'total_phenols': {'Modelnr': 5,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 1.38,\n",
       "  'max': 3.2984999999999993,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'flavanoids': {'Modelnr': 6,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 0.5705,\n",
       "  'max': 3.485499999999999,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'nonflavanoid_phenols': {'Modelnr': 7,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 0.1905,\n",
       "  'max': 0.6,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'proanthocyanins': {'Modelnr': 8,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 0.8,\n",
       "  'max': 2.8074999999999997,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'color_intensity': {'Modelnr': 9,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 2.3075,\n",
       "  'max': 9.684999999999995,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'hue': {'Modelnr': 10,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 0.57,\n",
       "  'max': 1.3084999999999998,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'od280/od315_of_diluted_wines': {'Modelnr': 11,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 1.4224999999999999,\n",
       "  'max': 3.5795,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'proline': {'Modelnr': 12,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 365.65000000000003,\n",
       "  'max': 1285.0,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'target': {'Modelnr': 13,\n",
       "  'data_type': 'ordinal_Xc_Yo',\n",
       "  'levels': 3,\n",
       "  'node_type': 'sink',\n",
       "  'parents': ['alcohol',\n",
       "   'malic_acid',\n",
       "   'ash',\n",
       "   'alcalinity_of_ash',\n",
       "   'magnesium',\n",
       "   'total_phenols',\n",
       "   'flavanoids',\n",
       "   'nonflavanoid_phenols',\n",
       "   'proanthocyanins',\n",
       "   'color_intensity',\n",
       "   'hue',\n",
       "   'od280/od315_of_diluted_wines',\n",
       "   'proline'],\n",
       "  'parents_datatype': {'alcohol': 'continous',\n",
       "   'malic_acid': 'continous',\n",
       "   'ash': 'continous',\n",
       "   'alcalinity_of_ash': 'continous',\n",
       "   'magnesium': 'continous',\n",
       "   'total_phenols': 'continous',\n",
       "   'flavanoids': 'continous',\n",
       "   'nonflavanoid_phenols': 'continous',\n",
       "   'proanthocyanins': 'continous',\n",
       "   'color_intensity': 'continous',\n",
       "   'hue': 'continous',\n",
       "   'od280/od315_of_diluted_wines': 'continous',\n",
       "   'proline': 'continous'},\n",
       "  'transformation_terms_in_h()': {'alcohol': 'ls',\n",
       "   'malic_acid': 'ls',\n",
       "   'ash': 'ls',\n",
       "   'alcalinity_of_ash': 'ls',\n",
       "   'magnesium': 'ls',\n",
       "   'total_phenols': 'ls',\n",
       "   'flavanoids': 'ls',\n",
       "   'nonflavanoid_phenols': 'ls',\n",
       "   'proanthocyanins': 'ls',\n",
       "   'color_intensity': 'ls',\n",
       "   'hue': 'ls',\n",
       "   'od280/od315_of_diluted_wines': 'ls',\n",
       "   'proline': 'ls'},\n",
       "  'min': 0.0,\n",
       "  'max': 2.0,\n",
       "  'transformation_term_nn_models_in_h()': {'alcohol': 'LinearShift',\n",
       "   'malic_acid': 'LinearShift',\n",
       "   'ash': 'LinearShift',\n",
       "   'alcalinity_of_ash': 'LinearShift',\n",
       "   'magnesium': 'LinearShift',\n",
       "   'total_phenols': 'LinearShift',\n",
       "   'flavanoids': 'LinearShift',\n",
       "   'nonflavanoid_phenols': 'LinearShift',\n",
       "   'proanthocyanins': 'LinearShift',\n",
       "   'color_intensity': 'LinearShift',\n",
       "   'hue': 'LinearShift',\n",
       "   'od280/od315_of_diluted_wines': 'LinearShift',\n",
       "   'proline': 'LinearShift'}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levels_dict=create_levels_dict(df,data_type)\n",
    "target_nodes=create_node_dict(adj_matrix, nn_names_matrix, data_type, min_vals, max_vals,levels_dict)\n",
    "target_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6cee33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TramModel(\n",
       "  (nn_int): SimpleIntercept(\n",
       "    (fc): Linear(in_features=1, out_features=2, bias=False)\n",
       "  )\n",
       "  (nn_shift): ModuleList(\n",
       "    (0-12): 13 x LinearShift(\n",
       "      (fc): Linear(in_features=1, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node='target'\n",
    "tram_model=get_fully_specified_tram_model(node, target_nodes, verbose=True)\n",
    "tram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfde5c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.tram_data import get_dataloader\n",
    "\n",
    "learning_rate=0.001\n",
    "use_scheduler=False\n",
    "\n",
    "\n",
    "NODE_DIR = os.path.join(EXPERIMENT_DIR, f'{node}')\n",
    "os.makedirs(NODE_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_PATH,LAST_MODEL_PATH,TRAIN_HIST_PATH,VAL_HIST_PATH=model_train_val_paths(NODE_DIR)\n",
    "\n",
    "\n",
    "optimizer =torch.optim.Adam(tram_model.parameters(), lr=learning_rate)\n",
    "\n",
    "if use_scheduler:\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "train_loader, val_loader = get_dataloader(node, target_nodes, train_df, val_df, batch_size=56,return_intercept_shift=True, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e48da1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Device: cuda\n",
      "[DEBUG] Model paths: /home/bule/TramDag/dev_experiment_logs/ordinal_wine_example_8/target/best_model.pt /home/bule/TramDag/dev_experiment_logs/ordinal_wine_example_8/target/last_model.pt\n",
      "[DEBUG] min_max shape: torch.Size([2])\n",
      "Existing model found. Loading weights and history...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n",
      "Epoch 201/1000  Train Loss: 0.8817  Val Loss: 0.8587  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 202/1000  Train Loss: 0.8898  Val Loss: 0.8571  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 203/1000  Train Loss: 0.8942  Val Loss: 0.8554  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 204/1000  Train Loss: 0.8885  Val Loss: 0.8537  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 205/1000  Train Loss: 0.9000  Val Loss: 0.8521  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 206/1000  Train Loss: 0.8832  Val Loss: 0.8503  [Train: 0.17s  Val: 0.12s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 207/1000  Train Loss: 0.8935  Val Loss: 0.8485  [Train: 0.16s  Val: 0.20s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 208/1000  Train Loss: 0.8949  Val Loss: 0.8467  [Train: 0.18s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 209/1000  Train Loss: 0.8916  Val Loss: 0.8449  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 210/1000  Train Loss: 0.8907  Val Loss: 0.8433  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 211/1000  Train Loss: 0.8822  Val Loss: 0.8416  [Train: 0.17s  Val: 0.12s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 212/1000  Train Loss: 0.8868  Val Loss: 0.8399  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 213/1000  Train Loss: 0.8593  Val Loss: 0.8382  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 214/1000  Train Loss: 0.8717  Val Loss: 0.8364  [Train: 0.16s  Val: 0.17s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 215/1000  Train Loss: 0.8846  Val Loss: 0.8345  [Train: 0.18s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 216/1000  Train Loss: 0.8746  Val Loss: 0.8327  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 217/1000  Train Loss: 0.8686  Val Loss: 0.8311  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 218/1000  Train Loss: 0.8755  Val Loss: 0.8294  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 219/1000  Train Loss: 0.8785  Val Loss: 0.8277  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 220/1000  Train Loss: 0.8639  Val Loss: 0.8261  [Train: 0.16s  Val: 0.16s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 221/1000  Train Loss: 0.8747  Val Loss: 0.8245  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 222/1000  Train Loss: 0.8627  Val Loss: 0.8227  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 223/1000  Train Loss: 0.8686  Val Loss: 0.8211  [Train: 0.16s  Val: 0.18s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 224/1000  Train Loss: 0.8699  Val Loss: 0.8196  [Train: 0.18s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 225/1000  Train Loss: 0.8721  Val Loss: 0.8182  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 226/1000  Train Loss: 0.8580  Val Loss: 0.8167  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 227/1000  Train Loss: 0.8629  Val Loss: 0.8151  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 228/1000  Train Loss: 0.8456  Val Loss: 0.8138  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 229/1000  Train Loss: 0.8644  Val Loss: 0.8123  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 230/1000  Train Loss: 0.8672  Val Loss: 0.8108  [Train: 0.19s  Val: 0.15s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 231/1000  Train Loss: 0.8757  Val Loss: 0.8093  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 232/1000  Train Loss: 0.8694  Val Loss: 0.8079  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 233/1000  Train Loss: 0.8406  Val Loss: 0.8068  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 234/1000  Train Loss: 0.8556  Val Loss: 0.8055  [Train: 0.16s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 235/1000  Train Loss: 0.8492  Val Loss: 0.8042  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 236/1000  Train Loss: 0.8472  Val Loss: 0.8029  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 237/1000  Train Loss: 0.8455  Val Loss: 0.8015  [Train: 0.17s  Val: 0.19s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 238/1000  Train Loss: 0.8536  Val Loss: 0.8001  [Train: 0.17s  Val: 0.12s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 239/1000  Train Loss: 0.8403  Val Loss: 0.7988  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 240/1000  Train Loss: 0.8523  Val Loss: 0.7974  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 241/1000  Train Loss: 0.8424  Val Loss: 0.7960  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 242/1000  Train Loss: 0.8524  Val Loss: 0.7945  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 243/1000  Train Loss: 0.8368  Val Loss: 0.7931  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 244/1000  Train Loss: 0.8289  Val Loss: 0.7916  [Train: 0.22s  Val: 0.17s  Total: 0.39s]\n",
      "Saved new best model.\n",
      "Epoch 245/1000  Train Loss: 0.8359  Val Loss: 0.7900  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 246/1000  Train Loss: 0.8447  Val Loss: 0.7882  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 247/1000  Train Loss: 0.8432  Val Loss: 0.7866  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 248/1000  Train Loss: 0.8300  Val Loss: 0.7852  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 249/1000  Train Loss: 0.8306  Val Loss: 0.7838  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 250/1000  Train Loss: 0.8345  Val Loss: 0.7823  [Train: 0.19s  Val: 0.18s  Total: 0.39s]\n",
      "Saved new best model.\n",
      "Epoch 251/1000  Train Loss: 0.8302  Val Loss: 0.7809  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 252/1000  Train Loss: 0.8361  Val Loss: 0.7794  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 253/1000  Train Loss: 0.8405  Val Loss: 0.7780  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 254/1000  Train Loss: 0.8345  Val Loss: 0.7768  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 255/1000  Train Loss: 0.8244  Val Loss: 0.7757  [Train: 0.16s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 256/1000  Train Loss: 0.8263  Val Loss: 0.7744  [Train: 0.15s  Val: 0.14s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 257/1000  Train Loss: 0.8372  Val Loss: 0.7732  [Train: 0.17s  Val: 0.16s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 258/1000  Train Loss: 0.8569  Val Loss: 0.7721  [Train: 0.21s  Val: 0.14s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 259/1000  Train Loss: 0.8261  Val Loss: 0.7711  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 260/1000  Train Loss: 0.8203  Val Loss: 0.7701  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 261/1000  Train Loss: 0.8318  Val Loss: 0.7689  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 262/1000  Train Loss: 0.8162  Val Loss: 0.7677  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 263/1000  Train Loss: 0.8223  Val Loss: 0.7665  [Train: 0.17s  Val: 0.12s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 264/1000  Train Loss: 0.8242  Val Loss: 0.7653  [Train: 0.16s  Val: 0.15s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 265/1000  Train Loss: 0.8221  Val Loss: 0.7643  [Train: 0.22s  Val: 0.14s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 266/1000  Train Loss: 0.8300  Val Loss: 0.7630  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 267/1000  Train Loss: 0.8122  Val Loss: 0.7618  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 268/1000  Train Loss: 0.8248  Val Loss: 0.7605  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 269/1000  Train Loss: 0.8065  Val Loss: 0.7593  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 270/1000  Train Loss: 0.8137  Val Loss: 0.7581  [Train: 0.18s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 271/1000  Train Loss: 0.8190  Val Loss: 0.7568  [Train: 0.17s  Val: 0.19s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 272/1000  Train Loss: 0.8049  Val Loss: 0.7555  [Train: 0.20s  Val: 0.14s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 273/1000  Train Loss: 0.8030  Val Loss: 0.7542  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 274/1000  Train Loss: 0.8058  Val Loss: 0.7527  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 275/1000  Train Loss: 0.8083  Val Loss: 0.7513  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 276/1000  Train Loss: 0.8085  Val Loss: 0.7498  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 277/1000  Train Loss: 0.7960  Val Loss: 0.7484  [Train: 0.18s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 278/1000  Train Loss: 0.8036  Val Loss: 0.7472  [Train: 0.17s  Val: 0.18s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 279/1000  Train Loss: 0.8081  Val Loss: 0.7458  [Train: 0.19s  Val: 0.14s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 280/1000  Train Loss: 0.8070  Val Loss: 0.7445  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 281/1000  Train Loss: 0.7956  Val Loss: 0.7433  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 282/1000  Train Loss: 0.8031  Val Loss: 0.7421  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 283/1000  Train Loss: 0.7986  Val Loss: 0.7409  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 284/1000  Train Loss: 0.7809  Val Loss: 0.7395  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 285/1000  Train Loss: 0.7857  Val Loss: 0.7381  [Train: 0.18s  Val: 0.15s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 286/1000  Train Loss: 0.7930  Val Loss: 0.7366  [Train: 0.18s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 287/1000  Train Loss: 0.7939  Val Loss: 0.7350  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 288/1000  Train Loss: 0.7921  Val Loss: 0.7335  [Train: 0.20s  Val: 0.13s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 289/1000  Train Loss: 0.7873  Val Loss: 0.7321  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 290/1000  Train Loss: 0.7871  Val Loss: 0.7306  [Train: 0.18s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 291/1000  Train Loss: 0.7995  Val Loss: 0.7292  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 292/1000  Train Loss: 0.7981  Val Loss: 0.7278  [Train: 0.17s  Val: 0.16s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 293/1000  Train Loss: 0.7843  Val Loss: 0.7267  [Train: 0.18s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 294/1000  Train Loss: 0.7979  Val Loss: 0.7253  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 295/1000  Train Loss: 0.7822  Val Loss: 0.7240  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 296/1000  Train Loss: 0.7856  Val Loss: 0.7227  [Train: 0.17s  Val: 0.12s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 297/1000  Train Loss: 0.7959  Val Loss: 0.7215  [Train: 0.18s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 298/1000  Train Loss: 0.7874  Val Loss: 0.7202  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 299/1000  Train Loss: 0.7839  Val Loss: 0.7190  [Train: 0.18s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 300/1000  Train Loss: 0.7675  Val Loss: 0.7178  [Train: 0.17s  Val: 0.15s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 301/1000  Train Loss: 0.7887  Val Loss: 0.7166  [Train: 0.21s  Val: 0.13s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 302/1000  Train Loss: 0.7807  Val Loss: 0.7154  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 303/1000  Train Loss: 0.7805  Val Loss: 0.7144  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 304/1000  Train Loss: 0.7870  Val Loss: 0.7135  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 305/1000  Train Loss: 0.7762  Val Loss: 0.7125  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 306/1000  Train Loss: 0.7600  Val Loss: 0.7115  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 307/1000  Train Loss: 0.7744  Val Loss: 0.7102  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 308/1000  Train Loss: 0.7721  Val Loss: 0.7089  [Train: 0.22s  Val: 0.18s  Total: 0.41s]\n",
      "Saved new best model.\n",
      "Epoch 309/1000  Train Loss: 0.7717  Val Loss: 0.7077  [Train: 0.18s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 310/1000  Train Loss: 0.7657  Val Loss: 0.7066  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 311/1000  Train Loss: 0.7695  Val Loss: 0.7054  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 312/1000  Train Loss: 0.7784  Val Loss: 0.7042  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 313/1000  Train Loss: 0.7550  Val Loss: 0.7032  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 314/1000  Train Loss: 0.7715  Val Loss: 0.7019  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 315/1000  Train Loss: 0.7723  Val Loss: 0.7006  [Train: 0.18s  Val: 0.17s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 316/1000  Train Loss: 0.7748  Val Loss: 0.6995  [Train: 0.20s  Val: 0.13s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 317/1000  Train Loss: 0.7595  Val Loss: 0.6983  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 318/1000  Train Loss: 0.7613  Val Loss: 0.6972  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 319/1000  Train Loss: 0.7579  Val Loss: 0.6961  [Train: 0.15s  Val: 0.14s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 320/1000  Train Loss: 0.7641  Val Loss: 0.6948  [Train: 0.16s  Val: 0.15s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 321/1000  Train Loss: 0.7626  Val Loss: 0.6936  [Train: 0.16s  Val: 0.15s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 322/1000  Train Loss: 0.7593  Val Loss: 0.6925  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 323/1000  Train Loss: 0.7519  Val Loss: 0.6913  [Train: 0.22s  Val: 0.16s  Total: 0.39s]\n",
      "Saved new best model.\n",
      "Epoch 324/1000  Train Loss: 0.7674  Val Loss: 0.6902  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 325/1000  Train Loss: 0.7561  Val Loss: 0.6891  [Train: 0.20s  Val: 0.15s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 326/1000  Train Loss: 0.7706  Val Loss: 0.6881  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 327/1000  Train Loss: 0.7708  Val Loss: 0.6872  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 328/1000  Train Loss: 0.7525  Val Loss: 0.6864  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 329/1000  Train Loss: 0.7587  Val Loss: 0.6855  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 330/1000  Train Loss: 0.7664  Val Loss: 0.6846  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 331/1000  Train Loss: 0.7612  Val Loss: 0.6837  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 332/1000  Train Loss: 0.7422  Val Loss: 0.6828  [Train: 0.15s  Val: 0.17s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 333/1000  Train Loss: 0.7572  Val Loss: 0.6818  [Train: 0.19s  Val: 0.12s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 334/1000  Train Loss: 0.7474  Val Loss: 0.6808  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 335/1000  Train Loss: 0.7582  Val Loss: 0.6799  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 336/1000  Train Loss: 0.7662  Val Loss: 0.6790  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 337/1000  Train Loss: 0.7568  Val Loss: 0.6781  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 338/1000  Train Loss: 0.7474  Val Loss: 0.6772  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 339/1000  Train Loss: 0.7493  Val Loss: 0.6763  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 340/1000  Train Loss: 0.7496  Val Loss: 0.6754  [Train: 0.20s  Val: 0.16s  Total: 0.38s]\n",
      "Saved new best model.\n",
      "Epoch 341/1000  Train Loss: 0.7551  Val Loss: 0.6747  [Train: 0.16s  Val: 0.16s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 342/1000  Train Loss: 0.7454  Val Loss: 0.6740  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 343/1000  Train Loss: 0.7334  Val Loss: 0.6732  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 344/1000  Train Loss: 0.7422  Val Loss: 0.6724  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 345/1000  Train Loss: 0.7248  Val Loss: 0.6716  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 346/1000  Train Loss: 0.7507  Val Loss: 0.6704  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 347/1000  Train Loss: 0.7473  Val Loss: 0.6693  [Train: 0.22s  Val: 0.15s  Total: 0.38s]\n",
      "Saved new best model.\n",
      "Epoch 348/1000  Train Loss: 0.7329  Val Loss: 0.6684  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 349/1000  Train Loss: 0.7379  Val Loss: 0.6673  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 350/1000  Train Loss: 0.7225  Val Loss: 0.6662  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 351/1000  Train Loss: 0.7309  Val Loss: 0.6651  [Train: 0.15s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 352/1000  Train Loss: 0.7469  Val Loss: 0.6641  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 353/1000  Train Loss: 0.7459  Val Loss: 0.6631  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 354/1000  Train Loss: 0.7489  Val Loss: 0.6622  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 355/1000  Train Loss: 0.7392  Val Loss: 0.6614  [Train: 0.20s  Val: 0.18s  Total: 0.39s]\n",
      "Saved new best model.\n",
      "Epoch 356/1000  Train Loss: 0.7262  Val Loss: 0.6606  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 357/1000  Train Loss: 0.7214  Val Loss: 0.6597  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 358/1000  Train Loss: 0.7218  Val Loss: 0.6588  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 359/1000  Train Loss: 0.7273  Val Loss: 0.6578  [Train: 0.18s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 360/1000  Train Loss: 0.7344  Val Loss: 0.6568  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 361/1000  Train Loss: 0.7258  Val Loss: 0.6558  [Train: 0.16s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 362/1000  Train Loss: 0.7459  Val Loss: 0.6547  [Train: 0.22s  Val: 0.15s  Total: 0.38s]\n",
      "Saved new best model.\n",
      "Epoch 363/1000  Train Loss: 0.7214  Val Loss: 0.6540  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 364/1000  Train Loss: 0.7156  Val Loss: 0.6529  [Train: 0.18s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 365/1000  Train Loss: 0.7322  Val Loss: 0.6520  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 366/1000  Train Loss: 0.7312  Val Loss: 0.6509  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 367/1000  Train Loss: 0.7387  Val Loss: 0.6498  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 368/1000  Train Loss: 0.7203  Val Loss: 0.6489  [Train: 0.17s  Val: 0.16s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 369/1000  Train Loss: 0.7188  Val Loss: 0.6479  [Train: 0.22s  Val: 0.12s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 370/1000  Train Loss: 0.7173  Val Loss: 0.6468  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 371/1000  Train Loss: 0.7255  Val Loss: 0.6458  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 372/1000  Train Loss: 0.7121  Val Loss: 0.6448  [Train: 0.15s  Val: 0.15s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 373/1000  Train Loss: 0.7443  Val Loss: 0.6438  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 374/1000  Train Loss: 0.7096  Val Loss: 0.6430  [Train: 0.17s  Val: 0.15s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 375/1000  Train Loss: 0.7209  Val Loss: 0.6422  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 376/1000  Train Loss: 0.7163  Val Loss: 0.6412  [Train: 0.21s  Val: 0.14s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 377/1000  Train Loss: 0.7158  Val Loss: 0.6403  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 378/1000  Train Loss: 0.7187  Val Loss: 0.6396  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 379/1000  Train Loss: 0.7186  Val Loss: 0.6389  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 380/1000  Train Loss: 0.7082  Val Loss: 0.6379  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 381/1000  Train Loss: 0.7153  Val Loss: 0.6370  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 382/1000  Train Loss: 0.7158  Val Loss: 0.6361  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 383/1000  Train Loss: 0.7233  Val Loss: 0.6351  [Train: 0.17s  Val: 0.17s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 384/1000  Train Loss: 0.7065  Val Loss: 0.6345  [Train: 0.19s  Val: 0.13s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 385/1000  Train Loss: 0.7062  Val Loss: 0.6336  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 386/1000  Train Loss: 0.7113  Val Loss: 0.6327  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 387/1000  Train Loss: 0.7148  Val Loss: 0.6318  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 388/1000  Train Loss: 0.7181  Val Loss: 0.6309  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 389/1000  Train Loss: 0.7256  Val Loss: 0.6300  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 390/1000  Train Loss: 0.7144  Val Loss: 0.6293  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 391/1000  Train Loss: 0.7084  Val Loss: 0.6285  [Train: 0.17s  Val: 0.14s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 392/1000  Train Loss: 0.6969  Val Loss: 0.6277  [Train: 0.18s  Val: 0.12s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 393/1000  Train Loss: 0.7047  Val Loss: 0.6268  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 394/1000  Train Loss: 0.7014  Val Loss: 0.6260  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 395/1000  Train Loss: 0.6985  Val Loss: 0.6252  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 396/1000  Train Loss: 0.6986  Val Loss: 0.6245  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 397/1000  Train Loss: 0.7018  Val Loss: 0.6238  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 398/1000  Train Loss: 0.6976  Val Loss: 0.6232  [Train: 0.16s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 399/1000  Train Loss: 0.7010  Val Loss: 0.6223  [Train: 0.19s  Val: 0.14s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 400/1000  Train Loss: 0.7026  Val Loss: 0.6215  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 401/1000  Train Loss: 0.6915  Val Loss: 0.6207  [Train: 0.15s  Val: 0.14s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 402/1000  Train Loss: 0.6933  Val Loss: 0.6198  [Train: 0.18s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 403/1000  Train Loss: 0.6996  Val Loss: 0.6189  [Train: 0.17s  Val: 0.12s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 404/1000  Train Loss: 0.7029  Val Loss: 0.6180  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 405/1000  Train Loss: 0.6878  Val Loss: 0.6172  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 406/1000  Train Loss: 0.6934  Val Loss: 0.6163  [Train: 0.17s  Val: 0.15s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 407/1000  Train Loss: 0.6972  Val Loss: 0.6153  [Train: 0.21s  Val: 0.15s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 408/1000  Train Loss: 0.6869  Val Loss: 0.6147  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 409/1000  Train Loss: 0.6921  Val Loss: 0.6138  [Train: 0.17s  Val: 0.12s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 410/1000  Train Loss: 0.6934  Val Loss: 0.6129  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 411/1000  Train Loss: 0.6863  Val Loss: 0.6121  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 412/1000  Train Loss: 0.6948  Val Loss: 0.6112  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 413/1000  Train Loss: 0.6958  Val Loss: 0.6103  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 414/1000  Train Loss: 0.6879  Val Loss: 0.6095  [Train: 0.18s  Val: 0.18s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 415/1000  Train Loss: 0.6742  Val Loss: 0.6087  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 416/1000  Train Loss: 0.6983  Val Loss: 0.6079  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 417/1000  Train Loss: 0.6931  Val Loss: 0.6072  [Train: 0.15s  Val: 0.15s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 418/1000  Train Loss: 0.6764  Val Loss: 0.6065  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 419/1000  Train Loss: 0.6712  Val Loss: 0.6057  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 420/1000  Train Loss: 0.6908  Val Loss: 0.6047  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 421/1000  Train Loss: 0.6907  Val Loss: 0.6039  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 422/1000  Train Loss: 0.6862  Val Loss: 0.6030  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 423/1000  Train Loss: 0.6802  Val Loss: 0.6021  [Train: 0.20s  Val: 0.16s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 424/1000  Train Loss: 0.6770  Val Loss: 0.6013  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 425/1000  Train Loss: 0.6896  Val Loss: 0.6004  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 426/1000  Train Loss: 0.6728  Val Loss: 0.5997  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 427/1000  Train Loss: 0.6840  Val Loss: 0.5989  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 428/1000  Train Loss: 0.6737  Val Loss: 0.5981  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 429/1000  Train Loss: 0.6792  Val Loss: 0.5973  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 430/1000  Train Loss: 0.6770  Val Loss: 0.5965  [Train: 0.16s  Val: 0.15s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 431/1000  Train Loss: 0.6628  Val Loss: 0.5957  [Train: 0.20s  Val: 0.14s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 432/1000  Train Loss: 0.6758  Val Loss: 0.5948  [Train: 0.16s  Val: 0.15s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 433/1000  Train Loss: 0.6806  Val Loss: 0.5939  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 434/1000  Train Loss: 0.6729  Val Loss: 0.5930  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 435/1000  Train Loss: 0.6766  Val Loss: 0.5922  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 436/1000  Train Loss: 0.6725  Val Loss: 0.5913  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 437/1000  Train Loss: 0.6852  Val Loss: 0.5905  [Train: 0.17s  Val: 0.12s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 438/1000  Train Loss: 0.6715  Val Loss: 0.5899  [Train: 0.17s  Val: 0.18s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 439/1000  Train Loss: 0.6746  Val Loss: 0.5893  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 440/1000  Train Loss: 0.6686  Val Loss: 0.5886  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 441/1000  Train Loss: 0.6601  Val Loss: 0.5880  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 442/1000  Train Loss: 0.6744  Val Loss: 0.5873  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 443/1000  Train Loss: 0.6728  Val Loss: 0.5866  [Train: 0.16s  Val: 0.16s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 444/1000  Train Loss: 0.6573  Val Loss: 0.5859  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 445/1000  Train Loss: 0.6736  Val Loss: 0.5850  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 446/1000  Train Loss: 0.6721  Val Loss: 0.5841  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 447/1000  Train Loss: 0.6723  Val Loss: 0.5835  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 448/1000  Train Loss: 0.6561  Val Loss: 0.5828  [Train: 0.18s  Val: 0.18s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 449/1000  Train Loss: 0.6480  Val Loss: 0.5821  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 450/1000  Train Loss: 0.6563  Val Loss: 0.5813  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 451/1000  Train Loss: 0.6652  Val Loss: 0.5805  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 452/1000  Train Loss: 0.6645  Val Loss: 0.5797  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 453/1000  Train Loss: 0.6752  Val Loss: 0.5788  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 454/1000  Train Loss: 0.6601  Val Loss: 0.5780  [Train: 0.15s  Val: 0.14s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 455/1000  Train Loss: 0.6553  Val Loss: 0.5772  [Train: 0.15s  Val: 0.15s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 456/1000  Train Loss: 0.6616  Val Loss: 0.5765  [Train: 0.21s  Val: 0.16s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 457/1000  Train Loss: 0.6617  Val Loss: 0.5758  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 458/1000  Train Loss: 0.6520  Val Loss: 0.5752  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 459/1000  Train Loss: 0.6544  Val Loss: 0.5744  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 460/1000  Train Loss: 0.6500  Val Loss: 0.5735  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 461/1000  Train Loss: 0.6675  Val Loss: 0.5727  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 462/1000  Train Loss: 0.6616  Val Loss: 0.5719  [Train: 0.17s  Val: 0.17s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 463/1000  Train Loss: 0.6604  Val Loss: 0.5713  [Train: 0.21s  Val: 0.15s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 464/1000  Train Loss: 0.6508  Val Loss: 0.5705  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 465/1000  Train Loss: 0.6501  Val Loss: 0.5697  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 466/1000  Train Loss: 0.6467  Val Loss: 0.5690  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 467/1000  Train Loss: 0.6615  Val Loss: 0.5682  [Train: 0.18s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 468/1000  Train Loss: 0.6603  Val Loss: 0.5675  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 469/1000  Train Loss: 0.6452  Val Loss: 0.5669  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 470/1000  Train Loss: 0.6500  Val Loss: 0.5661  [Train: 0.20s  Val: 0.15s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 471/1000  Train Loss: 0.6549  Val Loss: 0.5652  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 472/1000  Train Loss: 0.6402  Val Loss: 0.5644  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 473/1000  Train Loss: 0.6434  Val Loss: 0.5637  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 474/1000  Train Loss: 0.6564  Val Loss: 0.5629  [Train: 0.15s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 475/1000  Train Loss: 0.6372  Val Loss: 0.5623  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 476/1000  Train Loss: 0.6426  Val Loss: 0.5615  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 477/1000  Train Loss: 0.6475  Val Loss: 0.5607  [Train: 0.15s  Val: 0.17s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 478/1000  Train Loss: 0.6537  Val Loss: 0.5600  [Train: 0.21s  Val: 0.13s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 479/1000  Train Loss: 0.6415  Val Loss: 0.5593  [Train: 0.17s  Val: 0.12s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 480/1000  Train Loss: 0.6605  Val Loss: 0.5586  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 481/1000  Train Loss: 0.6492  Val Loss: 0.5580  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 482/1000  Train Loss: 0.6459  Val Loss: 0.5575  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 483/1000  Train Loss: 0.6360  Val Loss: 0.5569  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 484/1000  Train Loss: 0.6420  Val Loss: 0.5562  [Train: 0.17s  Val: 0.19s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 485/1000  Train Loss: 0.6437  Val Loss: 0.5555  [Train: 0.18s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 486/1000  Train Loss: 0.6473  Val Loss: 0.5549  [Train: 0.18s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 487/1000  Train Loss: 0.6448  Val Loss: 0.5542  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 488/1000  Train Loss: 0.6432  Val Loss: 0.5535  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 489/1000  Train Loss: 0.6402  Val Loss: 0.5529  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 490/1000  Train Loss: 0.6352  Val Loss: 0.5523  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 491/1000  Train Loss: 0.6338  Val Loss: 0.5517  [Train: 0.15s  Val: 0.15s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 492/1000  Train Loss: 0.6325  Val Loss: 0.5512  [Train: 0.22s  Val: 0.15s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 493/1000  Train Loss: 0.6348  Val Loss: 0.5507  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 494/1000  Train Loss: 0.6406  Val Loss: 0.5502  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 495/1000  Train Loss: 0.6506  Val Loss: 0.5496  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 496/1000  Train Loss: 0.6288  Val Loss: 0.5492  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 497/1000  Train Loss: 0.6287  Val Loss: 0.5488  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 498/1000  Train Loss: 0.6277  Val Loss: 0.5483  [Train: 0.18s  Val: 0.14s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 499/1000  Train Loss: 0.6347  Val Loss: 0.5477  [Train: 0.18s  Val: 0.14s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 500/1000  Train Loss: 0.6411  Val Loss: 0.5471  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 501/1000  Train Loss: 0.6236  Val Loss: 0.5465  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 502/1000  Train Loss: 0.6236  Val Loss: 0.5457  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 503/1000  Train Loss: 0.6281  Val Loss: 0.5449  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 504/1000  Train Loss: 0.6227  Val Loss: 0.5442  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 505/1000  Train Loss: 0.6137  Val Loss: 0.5434  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 506/1000  Train Loss: 0.6417  Val Loss: 0.5426  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 507/1000  Train Loss: 0.6259  Val Loss: 0.5421  [Train: 0.17s  Val: 0.17s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 508/1000  Train Loss: 0.6224  Val Loss: 0.5415  [Train: 0.20s  Val: 0.15s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 509/1000  Train Loss: 0.6169  Val Loss: 0.5410  [Train: 0.18s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 510/1000  Train Loss: 0.6367  Val Loss: 0.5402  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 511/1000  Train Loss: 0.6202  Val Loss: 0.5395  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 512/1000  Train Loss: 0.6167  Val Loss: 0.5388  [Train: 0.18s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 513/1000  Train Loss: 0.6393  Val Loss: 0.5380  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 514/1000  Train Loss: 0.6302  Val Loss: 0.5374  [Train: 0.16s  Val: 0.17s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 515/1000  Train Loss: 0.6185  Val Loss: 0.5367  [Train: 0.21s  Val: 0.13s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 516/1000  Train Loss: 0.6447  Val Loss: 0.5362  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 517/1000  Train Loss: 0.6369  Val Loss: 0.5356  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 518/1000  Train Loss: 0.6281  Val Loss: 0.5350  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 519/1000  Train Loss: 0.6153  Val Loss: 0.5345  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 520/1000  Train Loss: 0.6180  Val Loss: 0.5340  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 521/1000  Train Loss: 0.6247  Val Loss: 0.5334  [Train: 0.15s  Val: 0.15s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 522/1000  Train Loss: 0.6185  Val Loss: 0.5330  [Train: 0.19s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 523/1000  Train Loss: 0.6128  Val Loss: 0.5324  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 524/1000  Train Loss: 0.6085  Val Loss: 0.5317  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 525/1000  Train Loss: 0.6175  Val Loss: 0.5311  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 526/1000  Train Loss: 0.6153  Val Loss: 0.5304  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 527/1000  Train Loss: 0.6200  Val Loss: 0.5299  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 528/1000  Train Loss: 0.6211  Val Loss: 0.5292  [Train: 0.17s  Val: 0.15s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 529/1000  Train Loss: 0.6221  Val Loss: 0.5287  [Train: 0.21s  Val: 0.16s  Total: 0.38s]\n",
      "Saved new best model.\n",
      "Epoch 530/1000  Train Loss: 0.6265  Val Loss: 0.5282  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 531/1000  Train Loss: 0.6174  Val Loss: 0.5278  [Train: 0.18s  Val: 0.14s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 532/1000  Train Loss: 0.6082  Val Loss: 0.5273  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 533/1000  Train Loss: 0.6058  Val Loss: 0.5267  [Train: 0.16s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 534/1000  Train Loss: 0.6178  Val Loss: 0.5260  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 535/1000  Train Loss: 0.6152  Val Loss: 0.5253  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 536/1000  Train Loss: 0.6208  Val Loss: 0.5247  [Train: 0.17s  Val: 0.18s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 537/1000  Train Loss: 0.6097  Val Loss: 0.5241  [Train: 0.20s  Val: 0.14s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 538/1000  Train Loss: 0.6042  Val Loss: 0.5235  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 539/1000  Train Loss: 0.6162  Val Loss: 0.5227  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 540/1000  Train Loss: 0.6067  Val Loss: 0.5221  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 541/1000  Train Loss: 0.5893  Val Loss: 0.5217  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 542/1000  Train Loss: 0.5995  Val Loss: 0.5211  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 543/1000  Train Loss: 0.6054  Val Loss: 0.5204  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 544/1000  Train Loss: 0.6002  Val Loss: 0.5197  [Train: 0.20s  Val: 0.15s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 545/1000  Train Loss: 0.6014  Val Loss: 0.5190  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 546/1000  Train Loss: 0.6114  Val Loss: 0.5182  [Train: 0.17s  Val: 0.12s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 547/1000  Train Loss: 0.6153  Val Loss: 0.5176  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 548/1000  Train Loss: 0.6050  Val Loss: 0.5170  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 549/1000  Train Loss: 0.6013  Val Loss: 0.5165  [Train: 0.18s  Val: 0.14s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 550/1000  Train Loss: 0.6082  Val Loss: 0.5158  [Train: 0.16s  Val: 0.16s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 551/1000  Train Loss: 0.6058  Val Loss: 0.5152  [Train: 0.19s  Val: 0.13s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 552/1000  Train Loss: 0.6032  Val Loss: 0.5147  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 553/1000  Train Loss: 0.6003  Val Loss: 0.5141  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 554/1000  Train Loss: 0.6135  Val Loss: 0.5138  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 555/1000  Train Loss: 0.6024  Val Loss: 0.5134  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 556/1000  Train Loss: 0.5889  Val Loss: 0.5128  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 557/1000  Train Loss: 0.5996  Val Loss: 0.5122  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 558/1000  Train Loss: 0.6100  Val Loss: 0.5115  [Train: 0.18s  Val: 0.12s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 559/1000  Train Loss: 0.6079  Val Loss: 0.5110  [Train: 0.16s  Val: 0.15s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 560/1000  Train Loss: 0.6104  Val Loss: 0.5105  [Train: 0.18s  Val: 0.15s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 561/1000  Train Loss: 0.5915  Val Loss: 0.5099  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 562/1000  Train Loss: 0.5940  Val Loss: 0.5095  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 563/1000  Train Loss: 0.6007  Val Loss: 0.5089  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 564/1000  Train Loss: 0.6018  Val Loss: 0.5082  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 565/1000  Train Loss: 0.5904  Val Loss: 0.5076  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 566/1000  Train Loss: 0.5916  Val Loss: 0.5069  [Train: 0.17s  Val: 0.18s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 567/1000  Train Loss: 0.5997  Val Loss: 0.5062  [Train: 0.18s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 568/1000  Train Loss: 0.6011  Val Loss: 0.5056  [Train: 0.16s  Val: 0.15s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 569/1000  Train Loss: 0.5905  Val Loss: 0.5052  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 570/1000  Train Loss: 0.5793  Val Loss: 0.5048  [Train: 0.17s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 571/1000  Train Loss: 0.5916  Val Loss: 0.5043  [Train: 0.16s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 572/1000  Train Loss: 0.6018  Val Loss: 0.5039  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 573/1000  Train Loss: 0.5908  Val Loss: 0.5034  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 574/1000  Train Loss: 0.5826  Val Loss: 0.5030  [Train: 0.20s  Val: 0.15s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 575/1000  Train Loss: 0.5835  Val Loss: 0.5024  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 576/1000  Train Loss: 0.5788  Val Loss: 0.5019  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 577/1000  Train Loss: 0.5884  Val Loss: 0.5014  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 578/1000  Train Loss: 0.5916  Val Loss: 0.5006  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 579/1000  Train Loss: 0.6020  Val Loss: 0.4999  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 580/1000  Train Loss: 0.5985  Val Loss: 0.4992  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 581/1000  Train Loss: 0.5870  Val Loss: 0.4987  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 582/1000  Train Loss: 0.5910  Val Loss: 0.4981  [Train: 0.21s  Val: 0.15s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 583/1000  Train Loss: 0.5872  Val Loss: 0.4976  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 584/1000  Train Loss: 0.5962  Val Loss: 0.4971  [Train: 0.18s  Val: 0.12s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 585/1000  Train Loss: 0.5730  Val Loss: 0.4966  [Train: 0.20s  Val: 0.13s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 586/1000  Train Loss: 0.5824  Val Loss: 0.4961  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 587/1000  Train Loss: 0.5823  Val Loss: 0.4955  [Train: 0.16s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 588/1000  Train Loss: 0.5824  Val Loss: 0.4948  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 589/1000  Train Loss: 0.5947  Val Loss: 0.4942  [Train: 0.21s  Val: 0.16s  Total: 0.38s]\n",
      "Saved new best model.\n",
      "Epoch 590/1000  Train Loss: 0.5729  Val Loss: 0.4937  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 591/1000  Train Loss: 0.5926  Val Loss: 0.4931  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 592/1000  Train Loss: 0.5932  Val Loss: 0.4925  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 593/1000  Train Loss: 0.5758  Val Loss: 0.4920  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 594/1000  Train Loss: 0.5784  Val Loss: 0.4914  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 595/1000  Train Loss: 0.5835  Val Loss: 0.4908  [Train: 0.18s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 596/1000  Train Loss: 0.5773  Val Loss: 0.4903  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 597/1000  Train Loss: 0.5705  Val Loss: 0.4898  [Train: 0.20s  Val: 0.18s  Total: 0.39s]\n",
      "Saved new best model.\n",
      "Epoch 598/1000  Train Loss: 0.5719  Val Loss: 0.4894  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 599/1000  Train Loss: 0.5838  Val Loss: 0.4888  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 600/1000  Train Loss: 0.5780  Val Loss: 0.4883  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 601/1000  Train Loss: 0.5662  Val Loss: 0.4878  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 602/1000  Train Loss: 0.5743  Val Loss: 0.4873  [Train: 0.18s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 603/1000  Train Loss: 0.5842  Val Loss: 0.4866  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 604/1000  Train Loss: 0.5768  Val Loss: 0.4860  [Train: 0.19s  Val: 0.16s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 605/1000  Train Loss: 0.5961  Val Loss: 0.4854  [Train: 0.19s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 606/1000  Train Loss: 0.5858  Val Loss: 0.4848  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 607/1000  Train Loss: 0.5802  Val Loss: 0.4844  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 608/1000  Train Loss: 0.5874  Val Loss: 0.4839  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 609/1000  Train Loss: 0.5704  Val Loss: 0.4836  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 610/1000  Train Loss: 0.5793  Val Loss: 0.4832  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 611/1000  Train Loss: 0.5728  Val Loss: 0.4828  [Train: 0.18s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 612/1000  Train Loss: 0.5711  Val Loss: 0.4823  [Train: 0.21s  Val: 0.15s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 613/1000  Train Loss: 0.5786  Val Loss: 0.4818  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 614/1000  Train Loss: 0.5783  Val Loss: 0.4814  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 615/1000  Train Loss: 0.5781  Val Loss: 0.4809  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 616/1000  Train Loss: 0.5627  Val Loss: 0.4804  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 617/1000  Train Loss: 0.5773  Val Loss: 0.4799  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 618/1000  Train Loss: 0.5711  Val Loss: 0.4794  [Train: 0.16s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 619/1000  Train Loss: 0.5700  Val Loss: 0.4790  [Train: 0.17s  Val: 0.15s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 620/1000  Train Loss: 0.5587  Val Loss: 0.4786  [Train: 0.21s  Val: 0.13s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 621/1000  Train Loss: 0.5677  Val Loss: 0.4780  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 622/1000  Train Loss: 0.5720  Val Loss: 0.4775  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 623/1000  Train Loss: 0.5743  Val Loss: 0.4770  [Train: 0.16s  Val: 0.15s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 624/1000  Train Loss: 0.5803  Val Loss: 0.4765  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 625/1000  Train Loss: 0.5673  Val Loss: 0.4763  [Train: 0.16s  Val: 0.16s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 626/1000  Train Loss: 0.5592  Val Loss: 0.4760  [Train: 0.21s  Val: 0.15s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 627/1000  Train Loss: 0.5734  Val Loss: 0.4756  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 628/1000  Train Loss: 0.5753  Val Loss: 0.4751  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 629/1000  Train Loss: 0.5583  Val Loss: 0.4748  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 630/1000  Train Loss: 0.5578  Val Loss: 0.4744  [Train: 0.15s  Val: 0.14s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 631/1000  Train Loss: 0.5696  Val Loss: 0.4738  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 632/1000  Train Loss: 0.5623  Val Loss: 0.4734  [Train: 0.16s  Val: 0.15s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 633/1000  Train Loss: 0.5622  Val Loss: 0.4730  [Train: 0.18s  Val: 0.14s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 634/1000  Train Loss: 0.5562  Val Loss: 0.4726  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 635/1000  Train Loss: 0.5596  Val Loss: 0.4722  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 636/1000  Train Loss: 0.5656  Val Loss: 0.4717  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 637/1000  Train Loss: 0.5714  Val Loss: 0.4714  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 638/1000  Train Loss: 0.5484  Val Loss: 0.4711  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 639/1000  Train Loss: 0.5652  Val Loss: 0.4707  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 640/1000  Train Loss: 0.5650  Val Loss: 0.4702  [Train: 0.16s  Val: 0.17s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 641/1000  Train Loss: 0.5577  Val Loss: 0.4696  [Train: 0.18s  Val: 0.14s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 642/1000  Train Loss: 0.5620  Val Loss: 0.4690  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 643/1000  Train Loss: 0.5448  Val Loss: 0.4684  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 644/1000  Train Loss: 0.5512  Val Loss: 0.4678  [Train: 0.16s  Val: 0.15s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 645/1000  Train Loss: 0.5614  Val Loss: 0.4673  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 646/1000  Train Loss: 0.5497  Val Loss: 0.4666  [Train: 0.14s  Val: 0.14s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 647/1000  Train Loss: 0.5581  Val Loss: 0.4660  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 648/1000  Train Loss: 0.5573  Val Loss: 0.4653  [Train: 0.17s  Val: 0.19s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 649/1000  Train Loss: 0.5512  Val Loss: 0.4648  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 650/1000  Train Loss: 0.5506  Val Loss: 0.4642  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 651/1000  Train Loss: 0.5615  Val Loss: 0.4638  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 652/1000  Train Loss: 0.5534  Val Loss: 0.4633  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 653/1000  Train Loss: 0.5557  Val Loss: 0.4629  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 654/1000  Train Loss: 0.5523  Val Loss: 0.4622  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 655/1000  Train Loss: 0.5669  Val Loss: 0.4615  [Train: 0.17s  Val: 0.14s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 656/1000  Train Loss: 0.5559  Val Loss: 0.4608  [Train: 0.19s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 657/1000  Train Loss: 0.5654  Val Loss: 0.4602  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 658/1000  Train Loss: 0.5506  Val Loss: 0.4597  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 659/1000  Train Loss: 0.5544  Val Loss: 0.4591  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 660/1000  Train Loss: 0.5670  Val Loss: 0.4585  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 661/1000  Train Loss: 0.5449  Val Loss: 0.4580  [Train: 0.16s  Val: 0.15s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 662/1000  Train Loss: 0.5467  Val Loss: 0.4575  [Train: 0.19s  Val: 0.15s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 663/1000  Train Loss: 0.5532  Val Loss: 0.4570  [Train: 0.20s  Val: 0.13s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 664/1000  Train Loss: 0.5621  Val Loss: 0.4564  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 665/1000  Train Loss: 0.5591  Val Loss: 0.4561  [Train: 0.18s  Val: 0.15s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 666/1000  Train Loss: 0.5535  Val Loss: 0.4558  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 667/1000  Train Loss: 0.5458  Val Loss: 0.4554  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 668/1000  Train Loss: 0.5537  Val Loss: 0.4550  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 669/1000  Train Loss: 0.5541  Val Loss: 0.4546  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 670/1000  Train Loss: 0.5359  Val Loss: 0.4542  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 671/1000  Train Loss: 0.5454  Val Loss: 0.4538  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 672/1000  Train Loss: 0.5538  Val Loss: 0.4533  [Train: 0.18s  Val: 0.19s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 673/1000  Train Loss: 0.5405  Val Loss: 0.4528  [Train: 0.20s  Val: 0.13s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 674/1000  Train Loss: 0.5528  Val Loss: 0.4523  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 675/1000  Train Loss: 0.5488  Val Loss: 0.4517  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 676/1000  Train Loss: 0.5451  Val Loss: 0.4513  [Train: 0.17s  Val: 0.15s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 677/1000  Train Loss: 0.5505  Val Loss: 0.4508  [Train: 0.18s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 678/1000  Train Loss: 0.5409  Val Loss: 0.4503  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 679/1000  Train Loss: 0.5518  Val Loss: 0.4498  [Train: 0.16s  Val: 0.15s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 680/1000  Train Loss: 0.5498  Val Loss: 0.4494  [Train: 0.21s  Val: 0.15s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 681/1000  Train Loss: 0.5651  Val Loss: 0.4490  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 682/1000  Train Loss: 0.5459  Val Loss: 0.4488  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 683/1000  Train Loss: 0.5451  Val Loss: 0.4485  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 684/1000  Train Loss: 0.5360  Val Loss: 0.4481  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 685/1000  Train Loss: 0.5446  Val Loss: 0.4478  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 686/1000  Train Loss: 0.5309  Val Loss: 0.4475  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 687/1000  Train Loss: 0.5390  Val Loss: 0.4471  [Train: 0.20s  Val: 0.14s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 688/1000  Train Loss: 0.5353  Val Loss: 0.4467  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 689/1000  Train Loss: 0.5270  Val Loss: 0.4461  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 690/1000  Train Loss: 0.5374  Val Loss: 0.4456  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 691/1000  Train Loss: 0.5525  Val Loss: 0.4450  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 692/1000  Train Loss: 0.5454  Val Loss: 0.4445  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 693/1000  Train Loss: 0.5300  Val Loss: 0.4442  [Train: 0.21s  Val: 0.13s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 694/1000  Train Loss: 0.5504  Val Loss: 0.4437  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 695/1000  Train Loss: 0.5382  Val Loss: 0.4435  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 696/1000  Train Loss: 0.5337  Val Loss: 0.4432  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 697/1000  Train Loss: 0.5432  Val Loss: 0.4429  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 698/1000  Train Loss: 0.5438  Val Loss: 0.4427  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 699/1000  Train Loss: 0.5330  Val Loss: 0.4424  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 700/1000  Train Loss: 0.5444  Val Loss: 0.4422  [Train: 0.20s  Val: 0.19s  Total: 0.40s]\n",
      "Saved new best model.\n",
      "Epoch 701/1000  Train Loss: 0.5318  Val Loss: 0.4418  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 702/1000  Train Loss: 0.5391  Val Loss: 0.4415  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 703/1000  Train Loss: 0.5354  Val Loss: 0.4412  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 704/1000  Train Loss: 0.5306  Val Loss: 0.4408  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 705/1000  Train Loss: 0.5308  Val Loss: 0.4405  [Train: 0.18s  Val: 0.12s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 706/1000  Train Loss: 0.5377  Val Loss: 0.4402  [Train: 0.18s  Val: 0.14s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 707/1000  Train Loss: 0.5459  Val Loss: 0.4399  [Train: 0.18s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 708/1000  Train Loss: 0.5296  Val Loss: 0.4395  [Train: 0.18s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 709/1000  Train Loss: 0.5315  Val Loss: 0.4390  [Train: 0.17s  Val: 0.15s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 710/1000  Train Loss: 0.5154  Val Loss: 0.4384  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 711/1000  Train Loss: 0.5306  Val Loss: 0.4379  [Train: 0.15s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 712/1000  Train Loss: 0.5262  Val Loss: 0.4374  [Train: 0.15s  Val: 0.14s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 713/1000  Train Loss: 0.5440  Val Loss: 0.4370  [Train: 0.18s  Val: 0.17s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 714/1000  Train Loss: 0.5365  Val Loss: 0.4367  [Train: 0.20s  Val: 0.14s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 715/1000  Train Loss: 0.5199  Val Loss: 0.4364  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 716/1000  Train Loss: 0.5291  Val Loss: 0.4360  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 717/1000  Train Loss: 0.5316  Val Loss: 0.4357  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 718/1000  Train Loss: 0.5254  Val Loss: 0.4354  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 719/1000  Train Loss: 0.5343  Val Loss: 0.4350  [Train: 0.18s  Val: 0.15s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 720/1000  Train Loss: 0.5240  Val Loss: 0.4347  [Train: 0.18s  Val: 0.17s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 721/1000  Train Loss: 0.5210  Val Loss: 0.4343  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 722/1000  Train Loss: 0.5239  Val Loss: 0.4339  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 723/1000  Train Loss: 0.5080  Val Loss: 0.4334  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 724/1000  Train Loss: 0.5283  Val Loss: 0.4328  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 725/1000  Train Loss: 0.5257  Val Loss: 0.4323  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 726/1000  Train Loss: 0.5257  Val Loss: 0.4318  [Train: 0.17s  Val: 0.18s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 727/1000  Train Loss: 0.5195  Val Loss: 0.4314  [Train: 0.18s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 728/1000  Train Loss: 0.5204  Val Loss: 0.4312  [Train: 0.16s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 729/1000  Train Loss: 0.5178  Val Loss: 0.4307  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 730/1000  Train Loss: 0.5219  Val Loss: 0.4302  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 731/1000  Train Loss: 0.5255  Val Loss: 0.4296  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 732/1000  Train Loss: 0.5180  Val Loss: 0.4291  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 733/1000  Train Loss: 0.5256  Val Loss: 0.4286  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 734/1000  Train Loss: 0.5291  Val Loss: 0.4283  [Train: 0.17s  Val: 0.12s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 735/1000  Train Loss: 0.5212  Val Loss: 0.4279  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 736/1000  Train Loss: 0.5191  Val Loss: 0.4274  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 737/1000  Train Loss: 0.5263  Val Loss: 0.4269  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 738/1000  Train Loss: 0.5195  Val Loss: 0.4265  [Train: 0.16s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 739/1000  Train Loss: 0.5152  Val Loss: 0.4261  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 740/1000  Train Loss: 0.5226  Val Loss: 0.4258  [Train: 0.17s  Val: 0.16s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 741/1000  Train Loss: 0.5293  Val Loss: 0.4255  [Train: 0.24s  Val: 0.13s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 742/1000  Train Loss: 0.5094  Val Loss: 0.4253  [Train: 0.18s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 743/1000  Train Loss: 0.5151  Val Loss: 0.4250  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 744/1000  Train Loss: 0.5197  Val Loss: 0.4247  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 745/1000  Train Loss: 0.5135  Val Loss: 0.4244  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 746/1000  Train Loss: 0.5183  Val Loss: 0.4241  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 747/1000  Train Loss: 0.5134  Val Loss: 0.4238  [Train: 0.15s  Val: 0.15s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 748/1000  Train Loss: 0.5237  Val Loss: 0.4234  [Train: 0.20s  Val: 0.15s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 749/1000  Train Loss: 0.5116  Val Loss: 0.4231  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 750/1000  Train Loss: 0.5114  Val Loss: 0.4228  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 751/1000  Train Loss: 0.5219  Val Loss: 0.4225  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 752/1000  Train Loss: 0.5034  Val Loss: 0.4222  [Train: 0.18s  Val: 0.14s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 753/1000  Train Loss: 0.5177  Val Loss: 0.4219  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 754/1000  Train Loss: 0.5215  Val Loss: 0.4215  [Train: 0.17s  Val: 0.12s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 755/1000  Train Loss: 0.5235  Val Loss: 0.4212  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 756/1000  Train Loss: 0.5206  Val Loss: 0.4208  [Train: 0.20s  Val: 0.16s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 757/1000  Train Loss: 0.5145  Val Loss: 0.4203  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 758/1000  Train Loss: 0.5203  Val Loss: 0.4196  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 759/1000  Train Loss: 0.5200  Val Loss: 0.4191  [Train: 0.16s  Val: 0.15s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 760/1000  Train Loss: 0.5092  Val Loss: 0.4187  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 761/1000  Train Loss: 0.5075  Val Loss: 0.4182  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 762/1000  Train Loss: 0.4998  Val Loss: 0.4177  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 763/1000  Train Loss: 0.5063  Val Loss: 0.4174  [Train: 0.18s  Val: 0.15s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 764/1000  Train Loss: 0.5152  Val Loss: 0.4169  [Train: 0.22s  Val: 0.15s  Total: 0.38s]\n",
      "Saved new best model.\n",
      "Epoch 765/1000  Train Loss: 0.5139  Val Loss: 0.4166  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 766/1000  Train Loss: 0.5199  Val Loss: 0.4162  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 767/1000  Train Loss: 0.4994  Val Loss: 0.4157  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 768/1000  Train Loss: 0.5161  Val Loss: 0.4152  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 769/1000  Train Loss: 0.5053  Val Loss: 0.4147  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 770/1000  Train Loss: 0.5096  Val Loss: 0.4143  [Train: 0.20s  Val: 0.13s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 771/1000  Train Loss: 0.5157  Val Loss: 0.4139  [Train: 0.15s  Val: 0.14s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 772/1000  Train Loss: 0.5136  Val Loss: 0.4136  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 773/1000  Train Loss: 0.5201  Val Loss: 0.4131  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 774/1000  Train Loss: 0.4981  Val Loss: 0.4127  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 775/1000  Train Loss: 0.5087  Val Loss: 0.4123  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 776/1000  Train Loss: 0.4973  Val Loss: 0.4118  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 777/1000  Train Loss: 0.5052  Val Loss: 0.4114  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 778/1000  Train Loss: 0.5053  Val Loss: 0.4112  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 779/1000  Train Loss: 0.5014  Val Loss: 0.4108  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 780/1000  Train Loss: 0.5177  Val Loss: 0.4105  [Train: 0.21s  Val: 0.18s  Total: 0.40s]\n",
      "Saved new best model.\n",
      "Epoch 781/1000  Train Loss: 0.5047  Val Loss: 0.4101  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 782/1000  Train Loss: 0.4900  Val Loss: 0.4097  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 783/1000  Train Loss: 0.5072  Val Loss: 0.4094  [Train: 0.16s  Val: 0.15s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 784/1000  Train Loss: 0.4876  Val Loss: 0.4092  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 785/1000  Train Loss: 0.5092  Val Loss: 0.4088  [Train: 0.18s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 786/1000  Train Loss: 0.5047  Val Loss: 0.4085  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 787/1000  Train Loss: 0.5015  Val Loss: 0.4082  [Train: 0.16s  Val: 0.15s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 788/1000  Train Loss: 0.5138  Val Loss: 0.4078  [Train: 0.20s  Val: 0.13s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 789/1000  Train Loss: 0.4935  Val Loss: 0.4075  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 790/1000  Train Loss: 0.5045  Val Loss: 0.4073  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 791/1000  Train Loss: 0.5012  Val Loss: 0.4071  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 792/1000  Train Loss: 0.5027  Val Loss: 0.4068  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 793/1000  Train Loss: 0.5083  Val Loss: 0.4066  [Train: 0.16s  Val: 0.15s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 794/1000  Train Loss: 0.4982  Val Loss: 0.4063  [Train: 0.18s  Val: 0.14s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 795/1000  Train Loss: 0.5096  Val Loss: 0.4060  [Train: 0.20s  Val: 0.16s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 796/1000  Train Loss: 0.4912  Val Loss: 0.4056  [Train: 0.18s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 797/1000  Train Loss: 0.4928  Val Loss: 0.4052  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 798/1000  Train Loss: 0.4934  Val Loss: 0.4047  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 799/1000  Train Loss: 0.4833  Val Loss: 0.4043  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 800/1000  Train Loss: 0.4866  Val Loss: 0.4038  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 801/1000  Train Loss: 0.4852  Val Loss: 0.4033  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 802/1000  Train Loss: 0.5074  Val Loss: 0.4028  [Train: 0.21s  Val: 0.14s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 803/1000  Train Loss: 0.4924  Val Loss: 0.4024  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 804/1000  Train Loss: 0.4944  Val Loss: 0.4020  [Train: 0.15s  Val: 0.16s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 805/1000  Train Loss: 0.4857  Val Loss: 0.4016  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 806/1000  Train Loss: 0.4861  Val Loss: 0.4012  [Train: 0.17s  Val: 0.12s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 807/1000  Train Loss: 0.4995  Val Loss: 0.4008  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 808/1000  Train Loss: 0.4967  Val Loss: 0.4004  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 809/1000  Train Loss: 0.5035  Val Loss: 0.3999  [Train: 0.19s  Val: 0.16s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 810/1000  Train Loss: 0.4928  Val Loss: 0.3995  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 811/1000  Train Loss: 0.4814  Val Loss: 0.3992  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 812/1000  Train Loss: 0.4898  Val Loss: 0.3989  [Train: 0.16s  Val: 0.12s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 813/1000  Train Loss: 0.4858  Val Loss: 0.3985  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 814/1000  Train Loss: 0.4925  Val Loss: 0.3982  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 815/1000  Train Loss: 0.4950  Val Loss: 0.3977  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 816/1000  Train Loss: 0.4783  Val Loss: 0.3973  [Train: 0.16s  Val: 0.17s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 817/1000  Train Loss: 0.4939  Val Loss: 0.3969  [Train: 0.19s  Val: 0.14s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 818/1000  Train Loss: 0.4892  Val Loss: 0.3966  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 819/1000  Train Loss: 0.4869  Val Loss: 0.3962  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 820/1000  Train Loss: 0.4914  Val Loss: 0.3958  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 821/1000  Train Loss: 0.4859  Val Loss: 0.3955  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 822/1000  Train Loss: 0.5057  Val Loss: 0.3952  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 823/1000  Train Loss: 0.4816  Val Loss: 0.3951  [Train: 0.16s  Val: 0.16s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 824/1000  Train Loss: 0.4907  Val Loss: 0.3949  [Train: 0.21s  Val: 0.14s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 825/1000  Train Loss: 0.4920  Val Loss: 0.3945  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 826/1000  Train Loss: 0.4837  Val Loss: 0.3941  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 827/1000  Train Loss: 0.4819  Val Loss: 0.3938  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 828/1000  Train Loss: 0.4903  Val Loss: 0.3934  [Train: 0.18s  Val: 0.14s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 829/1000  Train Loss: 0.4951  Val Loss: 0.3931  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 830/1000  Train Loss: 0.4855  Val Loss: 0.3929  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 831/1000  Train Loss: 0.4996  Val Loss: 0.3927  [Train: 0.19s  Val: 0.14s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 832/1000  Train Loss: 0.4923  Val Loss: 0.3926  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 833/1000  Train Loss: 0.4915  Val Loss: 0.3925  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 834/1000  Train Loss: 0.4812  Val Loss: 0.3924  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 835/1000  Train Loss: 0.4875  Val Loss: 0.3923  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 836/1000  Train Loss: 0.4981  Val Loss: 0.3921  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 837/1000  Train Loss: 0.4761  Val Loss: 0.3920  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 838/1000  Train Loss: 0.4927  Val Loss: 0.3918  [Train: 0.19s  Val: 0.19s  Total: 0.39s]\n",
      "Saved new best model.\n",
      "Epoch 839/1000  Train Loss: 0.4854  Val Loss: 0.3916  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 840/1000  Train Loss: 0.4762  Val Loss: 0.3914  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 841/1000  Train Loss: 0.4946  Val Loss: 0.3911  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 842/1000  Train Loss: 0.4799  Val Loss: 0.3908  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 843/1000  Train Loss: 0.4679  Val Loss: 0.3906  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 844/1000  Train Loss: 0.4826  Val Loss: 0.3903  [Train: 0.16s  Val: 0.15s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 845/1000  Train Loss: 0.4737  Val Loss: 0.3901  [Train: 0.19s  Val: 0.15s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 846/1000  Train Loss: 0.4841  Val Loss: 0.3898  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 847/1000  Train Loss: 0.4882  Val Loss: 0.3894  [Train: 0.14s  Val: 0.14s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 848/1000  Train Loss: 0.4858  Val Loss: 0.3890  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 849/1000  Train Loss: 0.4788  Val Loss: 0.3886  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 850/1000  Train Loss: 0.4722  Val Loss: 0.3882  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 851/1000  Train Loss: 0.4723  Val Loss: 0.3879  [Train: 0.15s  Val: 0.18s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 852/1000  Train Loss: 0.4780  Val Loss: 0.3876  [Train: 0.21s  Val: 0.13s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 853/1000  Train Loss: 0.4770  Val Loss: 0.3873  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 854/1000  Train Loss: 0.4819  Val Loss: 0.3869  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 855/1000  Train Loss: 0.4764  Val Loss: 0.3867  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 856/1000  Train Loss: 0.4688  Val Loss: 0.3864  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 857/1000  Train Loss: 0.4760  Val Loss: 0.3863  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 858/1000  Train Loss: 0.4666  Val Loss: 0.3860  [Train: 0.16s  Val: 0.16s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 859/1000  Train Loss: 0.4903  Val Loss: 0.3857  [Train: 0.22s  Val: 0.13s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 860/1000  Train Loss: 0.4714  Val Loss: 0.3852  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 861/1000  Train Loss: 0.4972  Val Loss: 0.3849  [Train: 0.15s  Val: 0.14s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 862/1000  Train Loss: 0.4629  Val Loss: 0.3845  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 863/1000  Train Loss: 0.4814  Val Loss: 0.3842  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 864/1000  Train Loss: 0.4656  Val Loss: 0.3838  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 865/1000  Train Loss: 0.4712  Val Loss: 0.3835  [Train: 0.16s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 866/1000  Train Loss: 0.4813  Val Loss: 0.3831  [Train: 0.19s  Val: 0.14s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 867/1000  Train Loss: 0.4793  Val Loss: 0.3826  [Train: 0.18s  Val: 0.12s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 868/1000  Train Loss: 0.4736  Val Loss: 0.3822  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 869/1000  Train Loss: 0.4849  Val Loss: 0.3818  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 870/1000  Train Loss: 0.4660  Val Loss: 0.3815  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 871/1000  Train Loss: 0.4711  Val Loss: 0.3811  [Train: 0.17s  Val: 0.15s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 872/1000  Train Loss: 0.4869  Val Loss: 0.3807  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 873/1000  Train Loss: 0.4720  Val Loss: 0.3803  [Train: 0.20s  Val: 0.14s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 874/1000  Train Loss: 0.4807  Val Loss: 0.3799  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 875/1000  Train Loss: 0.4799  Val Loss: 0.3795  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 876/1000  Train Loss: 0.4765  Val Loss: 0.3792  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 877/1000  Train Loss: 0.4787  Val Loss: 0.3789  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 878/1000  Train Loss: 0.4670  Val Loss: 0.3787  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 879/1000  Train Loss: 0.4763  Val Loss: 0.3786  [Train: 0.17s  Val: 0.16s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 880/1000  Train Loss: 0.4746  Val Loss: 0.3783  [Train: 0.21s  Val: 0.14s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 881/1000  Train Loss: 0.4826  Val Loss: 0.3781  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 882/1000  Train Loss: 0.4660  Val Loss: 0.3778  [Train: 0.18s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 883/1000  Train Loss: 0.4796  Val Loss: 0.3774  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 884/1000  Train Loss: 0.4837  Val Loss: 0.3772  [Train: 0.16s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 885/1000  Train Loss: 0.4738  Val Loss: 0.3768  [Train: 0.17s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 886/1000  Train Loss: 0.4636  Val Loss: 0.3766  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 887/1000  Train Loss: 0.4676  Val Loss: 0.3762  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 888/1000  Train Loss: 0.4614  Val Loss: 0.3759  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 889/1000  Train Loss: 0.4714  Val Loss: 0.3756  [Train: 0.18s  Val: 0.18s  Total: 0.38s]\n",
      "Saved new best model.\n",
      "Epoch 890/1000  Train Loss: 0.4667  Val Loss: 0.3753  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 891/1000  Train Loss: 0.4796  Val Loss: 0.3751  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 892/1000  Train Loss: 0.4715  Val Loss: 0.3749  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 893/1000  Train Loss: 0.4695  Val Loss: 0.3747  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 894/1000  Train Loss: 0.4533  Val Loss: 0.3744  [Train: 0.15s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 895/1000  Train Loss: 0.4609  Val Loss: 0.3742  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 896/1000  Train Loss: 0.4639  Val Loss: 0.3739  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 897/1000  Train Loss: 0.4723  Val Loss: 0.3737  [Train: 0.18s  Val: 0.17s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 898/1000  Train Loss: 0.4641  Val Loss: 0.3734  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 899/1000  Train Loss: 0.4555  Val Loss: 0.3731  [Train: 0.15s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 900/1000  Train Loss: 0.4732  Val Loss: 0.3728  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 901/1000  Train Loss: 0.4573  Val Loss: 0.3726  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 902/1000  Train Loss: 0.4655  Val Loss: 0.3724  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 903/1000  Train Loss: 0.4808  Val Loss: 0.3721  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 904/1000  Train Loss: 0.4659  Val Loss: 0.3718  [Train: 0.17s  Val: 0.16s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 905/1000  Train Loss: 0.4602  Val Loss: 0.3717  [Train: 0.18s  Val: 0.12s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 906/1000  Train Loss: 0.4557  Val Loss: 0.3714  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 907/1000  Train Loss: 0.4590  Val Loss: 0.3711  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 908/1000  Train Loss: 0.4546  Val Loss: 0.3708  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 909/1000  Train Loss: 0.4613  Val Loss: 0.3704  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 910/1000  Train Loss: 0.4731  Val Loss: 0.3701  [Train: 0.16s  Val: 0.15s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 911/1000  Train Loss: 0.4642  Val Loss: 0.3698  [Train: 0.21s  Val: 0.14s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 912/1000  Train Loss: 0.4651  Val Loss: 0.3695  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 913/1000  Train Loss: 0.4526  Val Loss: 0.3692  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 914/1000  Train Loss: 0.4535  Val Loss: 0.3689  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 915/1000  Train Loss: 0.4544  Val Loss: 0.3686  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 916/1000  Train Loss: 0.4512  Val Loss: 0.3682  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 917/1000  Train Loss: 0.4620  Val Loss: 0.3680  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 918/1000  Train Loss: 0.4533  Val Loss: 0.3677  [Train: 0.21s  Val: 0.14s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 919/1000  Train Loss: 0.4593  Val Loss: 0.3674  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 920/1000  Train Loss: 0.4544  Val Loss: 0.3671  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 921/1000  Train Loss: 0.4600  Val Loss: 0.3668  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 922/1000  Train Loss: 0.4491  Val Loss: 0.3666  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 923/1000  Train Loss: 0.4623  Val Loss: 0.3663  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 924/1000  Train Loss: 0.4532  Val Loss: 0.3660  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 925/1000  Train Loss: 0.4466  Val Loss: 0.3657  [Train: 0.22s  Val: 0.13s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 926/1000  Train Loss: 0.4506  Val Loss: 0.3653  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 927/1000  Train Loss: 0.4662  Val Loss: 0.3650  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 928/1000  Train Loss: 0.4495  Val Loss: 0.3648  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 929/1000  Train Loss: 0.4593  Val Loss: 0.3645  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 930/1000  Train Loss: 0.4625  Val Loss: 0.3643  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 931/1000  Train Loss: 0.4548  Val Loss: 0.3642  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 932/1000  Train Loss: 0.4566  Val Loss: 0.3641  [Train: 0.20s  Val: 0.17s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 933/1000  Train Loss: 0.4556  Val Loss: 0.3639  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 934/1000  Train Loss: 0.4717  Val Loss: 0.3637  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 935/1000  Train Loss: 0.4502  Val Loss: 0.3635  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 936/1000  Train Loss: 0.4375  Val Loss: 0.3632  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 937/1000  Train Loss: 0.4561  Val Loss: 0.3629  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 938/1000  Train Loss: 0.4578  Val Loss: 0.3626  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 939/1000  Train Loss: 0.4521  Val Loss: 0.3624  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 940/1000  Train Loss: 0.4469  Val Loss: 0.3622  [Train: 0.21s  Val: 0.16s  Total: 0.37s]\n",
      "Saved new best model.\n",
      "Epoch 941/1000  Train Loss: 0.4538  Val Loss: 0.3620  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 942/1000  Train Loss: 0.4574  Val Loss: 0.3618  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 943/1000  Train Loss: 0.4389  Val Loss: 0.3615  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 944/1000  Train Loss: 0.4454  Val Loss: 0.3612  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 945/1000  Train Loss: 0.4365  Val Loss: 0.3608  [Train: 0.17s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 946/1000  Train Loss: 0.4446  Val Loss: 0.3605  [Train: 0.16s  Val: 0.16s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 947/1000  Train Loss: 0.4455  Val Loss: 0.3602  [Train: 0.20s  Val: 0.13s  Total: 0.34s]\n",
      "Saved new best model.\n",
      "Epoch 948/1000  Train Loss: 0.4435  Val Loss: 0.3599  [Train: 0.16s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 949/1000  Train Loss: 0.4460  Val Loss: 0.3596  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 950/1000  Train Loss: 0.4436  Val Loss: 0.3593  [Train: 0.18s  Val: 0.14s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 951/1000  Train Loss: 0.4634  Val Loss: 0.3590  [Train: 0.18s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 952/1000  Train Loss: 0.4625  Val Loss: 0.3588  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 953/1000  Train Loss: 0.4546  Val Loss: 0.3586  [Train: 0.18s  Val: 0.17s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 954/1000  Train Loss: 0.4462  Val Loss: 0.3584  [Train: 0.19s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 955/1000  Train Loss: 0.4553  Val Loss: 0.3581  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 956/1000  Train Loss: 0.4521  Val Loss: 0.3579  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 957/1000  Train Loss: 0.4448  Val Loss: 0.3576  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 958/1000  Train Loss: 0.4543  Val Loss: 0.3573  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 959/1000  Train Loss: 0.4462  Val Loss: 0.3571  [Train: 0.19s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 960/1000  Train Loss: 0.4559  Val Loss: 0.3568  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 961/1000  Train Loss: 0.4463  Val Loss: 0.3564  [Train: 0.20s  Val: 0.14s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 962/1000  Train Loss: 0.4389  Val Loss: 0.3561  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 963/1000  Train Loss: 0.4443  Val Loss: 0.3557  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 964/1000  Train Loss: 0.4426  Val Loss: 0.3554  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 965/1000  Train Loss: 0.4429  Val Loss: 0.3550  [Train: 0.17s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 966/1000  Train Loss: 0.4531  Val Loss: 0.3545  [Train: 0.16s  Val: 0.15s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 967/1000  Train Loss: 0.4291  Val Loss: 0.3542  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 968/1000  Train Loss: 0.4500  Val Loss: 0.3538  [Train: 0.16s  Val: 0.19s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 969/1000  Train Loss: 0.4527  Val Loss: 0.3535  [Train: 0.22s  Val: 0.13s  Total: 0.35s]\n",
      "Saved new best model.\n",
      "Epoch 970/1000  Train Loss: 0.4545  Val Loss: 0.3532  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 971/1000  Train Loss: 0.4537  Val Loss: 0.3530  [Train: 0.18s  Val: 0.14s  Total: 0.33s]\n",
      "Saved new best model.\n",
      "Epoch 972/1000  Train Loss: 0.4550  Val Loss: 0.3529  [Train: 0.16s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 973/1000  Train Loss: 0.4306  Val Loss: 0.3528  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 974/1000  Train Loss: 0.4389  Val Loss: 0.3527  [Train: 0.16s  Val: 0.14s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 975/1000  Train Loss: 0.4395  Val Loss: 0.3526  [Train: 0.21s  Val: 0.14s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 976/1000  Train Loss: 0.4396  Val Loss: 0.3525  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 977/1000  Train Loss: 0.4386  Val Loss: 0.3523  [Train: 0.17s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 978/1000  Train Loss: 0.4425  Val Loss: 0.3521  [Train: 0.16s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 979/1000  Train Loss: 0.4428  Val Loss: 0.3519  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 980/1000  Train Loss: 0.4452  Val Loss: 0.3516  [Train: 0.17s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 981/1000  Train Loss: 0.4371  Val Loss: 0.3514  [Train: 0.19s  Val: 0.18s  Total: 0.38s]\n",
      "Saved new best model.\n",
      "Epoch 982/1000  Train Loss: 0.4516  Val Loss: 0.3511  [Train: 0.18s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 983/1000  Train Loss: 0.4330  Val Loss: 0.3509  [Train: 0.16s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 984/1000  Train Loss: 0.4420  Val Loss: 0.3507  [Train: 0.15s  Val: 0.14s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 985/1000  Train Loss: 0.4374  Val Loss: 0.3505  [Train: 0.18s  Val: 0.13s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 986/1000  Train Loss: 0.4450  Val Loss: 0.3503  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 987/1000  Train Loss: 0.4267  Val Loss: 0.3501  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 988/1000  Train Loss: 0.4324  Val Loss: 0.3499  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 989/1000  Train Loss: 0.4349  Val Loss: 0.3495  [Train: 0.20s  Val: 0.16s  Total: 0.36s]\n",
      "Saved new best model.\n",
      "Epoch 990/1000  Train Loss: 0.4301  Val Loss: 0.3493  [Train: 0.15s  Val: 0.13s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 991/1000  Train Loss: 0.4373  Val Loss: 0.3490  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 992/1000  Train Loss: 0.4366  Val Loss: 0.3488  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 993/1000  Train Loss: 0.4379  Val Loss: 0.3484  [Train: 0.16s  Val: 0.12s  Total: 0.29s]\n",
      "Saved new best model.\n",
      "Epoch 994/1000  Train Loss: 0.4406  Val Loss: 0.3481  [Train: 0.15s  Val: 0.13s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 995/1000  Train Loss: 0.4384  Val Loss: 0.3478  [Train: 0.16s  Val: 0.13s  Total: 0.30s]\n",
      "Saved new best model.\n",
      "Epoch 996/1000  Train Loss: 0.4468  Val Loss: 0.3475  [Train: 0.15s  Val: 0.12s  Total: 0.28s]\n",
      "Saved new best model.\n",
      "Epoch 997/1000  Train Loss: 0.4298  Val Loss: 0.3472  [Train: 0.18s  Val: 0.13s  Total: 0.31s]\n",
      "Saved new best model.\n",
      "Epoch 998/1000  Train Loss: 0.4345  Val Loss: 0.3469  [Train: 0.17s  Val: 0.14s  Total: 0.32s]\n",
      "Saved new best model.\n",
      "Epoch 999/1000  Train Loss: 0.4316  Val Loss: 0.3466  [Train: 0.23s  Val: 0.15s  Total: 0.39s]\n",
      "Saved new best model.\n",
      "Epoch 1000/1000  Train Loss: 0.4318  Val Loss: 0.3463  [Train: 0.17s  Val: 0.16s  Total: 0.34s]\n"
     ]
    }
   ],
   "source": [
    "# TODO initialize with correct weights\n",
    "epochs =1000\n",
    "train_val_loop(\n",
    "            node,\n",
    "            target_nodes,\n",
    "            NODE_DIR,\n",
    "            tram_model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            epochs,\n",
    "            optimizer,\n",
    "            use_scheduler,\n",
    "            scheduler,\n",
    "            save_linear_shifts=False,\n",
    "            verbose=1,\n",
    "            device=device,\n",
    "            debug=True) # TODO startvalues for bernsteinpols\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a57f405",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "def7e535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading weights and history...\n"
     ]
    }
   ],
   "source": [
    "## laoding the best model \n",
    "MODEL_PATH,LAST_MODEL_PATH,TRAIN_HIST_PATH,VAL_HIST_PATH=model_train_val_paths(NODE_DIR)\n",
    "\n",
    "if os.path.exists(MODEL_PATH) and os.path.exists(TRAIN_HIST_PATH) and os.path.exists(VAL_HIST_PATH):\n",
    "    print(\"Existing model found. Loading weights and history...\")\n",
    "    tram_model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c789baa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.1%\n",
      "Accuracy: 94.6%\n",
      "Accuracy: 80.0%\n",
      "Accuracy: 94.4%\n"
     ]
    }
   ],
   "source": [
    "## evaluate on testdata\n",
    "tram_model.eval()\n",
    "\n",
    "_, ordered_transformation_terms_in_h, _=ordered_parents(node, target_nodes)\n",
    "\n",
    "min_vals = torch.tensor(target_nodes[node]['min'], dtype=torch.float32).to(device)\n",
    "max_vals = torch.tensor(target_nodes[node]['max'], dtype=torch.float32).to(device)\n",
    "min_max = torch.stack([min_vals, max_vals], dim=0)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (int_input, shift_list), y in train_loader:\n",
    "        int_input = int_input.to(device)\n",
    "        shift_list = [s.to(device) for s in shift_list]\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred = tram_model(int_input=int_input, shift_input=shift_list)\n",
    "        # loss = contram_nll(y_pred, y, min_max=min_max)\n",
    "        pred_labels = get_pdf_ordinal(get_cdf_ordinal(y_pred)).argmax(dim=1)\n",
    "        true_labels = y.argmax(dim=1)\n",
    "        accuracy = (pred_labels == true_labels).float().mean().item()\n",
    "\n",
    "        print(f\"Accuracy: {accuracy*100:.1f}%\")  # → 100.0%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (int_input, shift_list), y in val_loader:\n",
    "        int_input = int_input.to(device)\n",
    "        shift_list = [s.to(device) for s in shift_list]\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred = tram_model(int_input=int_input, shift_input=shift_list)\n",
    "        # loss = contram_nll(y_pred, y, min_max=min_max)\n",
    "        pred_labels = get_pdf_ordinal(get_cdf_ordinal(y_pred)).argmax(dim=1)\n",
    "        true_labels = y.argmax(dim=1)\n",
    "        accuracy = (pred_labels == true_labels).float().mean().item()\n",
    "\n",
    "        print(f\"Accuracy: {accuracy*100:.1f}%\")  # → 100.0%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
