{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32793ca7",
   "metadata": {},
   "source": [
    "# minimal example of ontram implementation\n",
    "- anlogous to https://github.com/liherz/ontram_pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee5f4e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with GPU support.\n"
     ]
    }
   ],
   "source": [
    "# Load dependencies\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from utils.configuration import *\n",
    "from utils.loss_ordinal import *\n",
    "from utils.tram_model_helpers import *\n",
    "from utils.tram_models import *\n",
    "from utils.tram_data import *\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Train with GPU support.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU found, train with CPU support.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ada57",
   "metadata": {},
   "source": [
    "adjustet funcitnos for ordinal outcomes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8678f4f3",
   "metadata": {},
   "source": [
    "dev ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd61311",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"ordinal_wine_example_7\"   ## <--- set experiment name\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "\n",
    "LOG_DIR=\"/home/bule/TramDag/dev_experiment_logs\"\n",
    "EXPERIMENT_DIR = os.path.join(LOG_DIR, experiment_name)\n",
    "DATA_PATH = EXPERIMENT_DIR # <----------- change to different source if needed\n",
    "CONF_DICT_PATH = os.path.join(EXPERIMENT_DIR, f\"configuration.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64449eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
       "         1.065e+03],\n",
       "        [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
       "         1.050e+03],\n",
       "        [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
       "         1.185e+03],\n",
       "        ...,\n",
       "        [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
       "         8.350e+02],\n",
       "        [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
       "         8.400e+02],\n",
       "        [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
       "         5.600e+02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['class_0', 'class_1', 'class_2'], dtype='<U7'),\n",
       " 'DESCR': '.. _wine_dataset:\\n\\nWine recognition dataset\\n------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 178\\n:Number of Attributes: 13 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - Alcohol\\n    - Malic acid\\n    - Ash\\n    - Alcalinity of ash\\n    - Magnesium\\n    - Total phenols\\n    - Flavanoids\\n    - Nonflavanoid phenols\\n    - Proanthocyanins\\n    - Color intensity\\n    - Hue\\n    - OD280/OD315 of diluted wines\\n    - Proline\\n    - class:\\n        - class_0\\n        - class_1\\n        - class_2\\n\\n:Summary Statistics:\\n\\n============================= ==== ===== ======= =====\\n                                Min   Max   Mean     SD\\n============================= ==== ===== ======= =====\\nAlcohol:                      11.0  14.8    13.0   0.8\\nMalic Acid:                   0.74  5.80    2.34  1.12\\nAsh:                          1.36  3.23    2.36  0.27\\nAlcalinity of Ash:            10.6  30.0    19.5   3.3\\nMagnesium:                    70.0 162.0    99.7  14.3\\nTotal Phenols:                0.98  3.88    2.29  0.63\\nFlavanoids:                   0.34  5.08    2.03  1.00\\nNonflavanoid Phenols:         0.13  0.66    0.36  0.12\\nProanthocyanins:              0.41  3.58    1.59  0.57\\nColour Intensity:              1.3  13.0     5.1   2.3\\nHue:                          0.48  1.71    0.96  0.23\\nOD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\\nProline:                       278  1680     746   315\\n============================= ==== ===== ======= =====\\n\\n:Missing Attribute Values: None\\n:Class Distribution: class_0 (59), class_1 (71), class_2 (48)\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThis is a copy of UCI ML Wine recognition datasets.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\\n\\nThe data is the results of a chemical analysis of wines grown in the same\\nregion in Italy by three different cultivators. There are thirteen different\\nmeasurements taken for different constituents found in the three types of\\nwine.\\n\\nOriginal Owners:\\n\\nForina, M. et al, PARVUS -\\nAn Extendible Package for Data Exploration, Classification and Correlation.\\nInstitute of Pharmaceutical and Food Analysis and Technologies,\\nVia Brigata Salerno, 16147 Genoa, Italy.\\n\\nCitation:\\n\\nLichman, M. (2013). UCI Machine Learning Repository\\n[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\\nSchool of Information and Computer Science.\\n\\n.. dropdown:: References\\n\\n    (1) S. Aeberhard, D. Coomans and O. de Vel,\\n    Comparison of Classifiers in High Dimensional Settings,\\n    Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of\\n    Mathematics and Statistics, James Cook University of North Queensland.\\n    (Also submitted to Technometrics).\\n\\n    The data was used with many others for comparing various\\n    classifiers. The classes are separable, though only RDA\\n    has achieved 100% correct classification.\\n    (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data))\\n    (All results using the leave-one-out technique)\\n\\n    (2) S. Aeberhard, D. Coomans and O. de Vel,\\n    \"THE CLASSIFICATION PERFORMANCE OF RDA\"\\n    Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of\\n    Mathematics and Statistics, James Cook University of North Queensland.\\n    (Also submitted to Journal of Chemometrics).\\n',\n",
       " 'feature_names': ['alcohol',\n",
       "  'malic_acid',\n",
       "  'ash',\n",
       "  'alcalinity_of_ash',\n",
       "  'magnesium',\n",
       "  'total_phenols',\n",
       "  'flavanoids',\n",
       "  'nonflavanoid_phenols',\n",
       "  'proanthocyanins',\n",
       "  'color_intensity',\n",
       "  'hue',\n",
       "  'od280/od315_of_diluted_wines',\n",
       "  'proline']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "wine = load_wine()\n",
    "wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9a21723",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(wine['data'], columns=wine['feature_names'])\n",
    "df['target']=wine['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1851b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "quantiles = train_df.quantile([0.05, 0.95])\n",
    "min_vals = quantiles.loc[0.05]\n",
    "max_vals = quantiles.loc[0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a156cd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 142 entries, 158 to 102\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   alcohol                       142 non-null    float64\n",
      " 1   malic_acid                    142 non-null    float64\n",
      " 2   ash                           142 non-null    float64\n",
      " 3   alcalinity_of_ash             142 non-null    float64\n",
      " 4   magnesium                     142 non-null    float64\n",
      " 5   total_phenols                 142 non-null    float64\n",
      " 6   flavanoids                    142 non-null    float64\n",
      " 7   nonflavanoid_phenols          142 non-null    float64\n",
      " 8   proanthocyanins               142 non-null    float64\n",
      " 9   color_intensity               142 non-null    float64\n",
      " 10  hue                           142 non-null    float64\n",
      " 11  od280/od315_of_diluted_wines  142 non-null    float64\n",
      " 12  proline                       142 non-null    float64\n",
      " 13  target                        142 non-null    int64  \n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 16.6 KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f49ea145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "target",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "96ffb5ae-1a6a-4e4b-a17d-f1cc59451147",
       "rows": [
        [
         "158",
         "2"
        ],
        [
         "137",
         "2"
        ],
        [
         "98",
         "1"
        ],
        [
         "159",
         "2"
        ],
        [
         "38",
         "0"
        ],
        [
         "108",
         "1"
        ],
        [
         "85",
         "1"
        ],
        [
         "68",
         "1"
        ],
        [
         "143",
         "2"
        ],
        [
         "2",
         "0"
        ],
        [
         "100",
         "1"
        ],
        [
         "122",
         "1"
        ],
        [
         "154",
         "2"
        ],
        [
         "51",
         "0"
        ],
        [
         "76",
         "1"
        ],
        [
         "56",
         "0"
        ],
        [
         "26",
         "0"
        ],
        [
         "153",
         "2"
        ],
        [
         "138",
         "2"
        ],
        [
         "104",
         "1"
        ],
        [
         "78",
         "1"
        ],
        [
         "36",
         "0"
        ],
        [
         "93",
         "1"
        ],
        [
         "22",
         "0"
        ],
        [
         "146",
         "2"
        ],
        [
         "97",
         "1"
        ],
        [
         "69",
         "1"
        ],
        [
         "167",
         "2"
        ],
        [
         "11",
         "0"
        ],
        [
         "6",
         "0"
        ],
        [
         "27",
         "0"
        ],
        [
         "144",
         "2"
        ],
        [
         "4",
         "0"
        ],
        [
         "32",
         "0"
        ],
        [
         "95",
         "1"
        ],
        [
         "170",
         "2"
        ],
        [
         "75",
         "1"
        ],
        [
         "10",
         "0"
        ],
        [
         "147",
         "2"
        ],
        [
         "123",
         "1"
        ],
        [
         "0",
         "0"
        ],
        [
         "142",
         "2"
        ],
        [
         "126",
         "1"
        ],
        [
         "64",
         "1"
        ],
        [
         "44",
         "0"
        ],
        [
         "96",
         "1"
        ],
        [
         "28",
         "0"
        ],
        [
         "40",
         "0"
        ],
        [
         "127",
         "1"
        ],
        [
         "25",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 142
       }
      },
      "text/plain": [
       "158    2\n",
       "137    2\n",
       "98     1\n",
       "159    2\n",
       "38     0\n",
       "      ..\n",
       "71     1\n",
       "106    1\n",
       "14     0\n",
       "92     1\n",
       "102    1\n",
       "Name: target, Length: 142, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cdc6fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "alcohol",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "malic_acid",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ash",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "alcalinity_of_ash",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "magnesium",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_phenols",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "flavanoids",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nonflavanoid_phenols",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "proanthocyanins",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "color_intensity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "od280/od315_of_diluted_wines",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "proline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "target",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "63ffb38f-28df-49dd-ad8f-a7bdfb462e46",
       "rows": [
        [
         "158",
         "14.34",
         "1.68",
         "2.7",
         "25.0",
         "98.0",
         "2.8",
         "1.31",
         "0.53",
         "2.7",
         "13.0",
         "0.57",
         "1.96",
         "660.0",
         "2"
        ],
        [
         "137",
         "12.53",
         "5.51",
         "2.64",
         "25.0",
         "96.0",
         "1.79",
         "0.6",
         "0.63",
         "1.1",
         "5.0",
         "0.82",
         "1.69",
         "515.0",
         "2"
        ],
        [
         "98",
         "12.37",
         "1.07",
         "2.1",
         "18.5",
         "88.0",
         "3.52",
         "3.75",
         "0.24",
         "1.95",
         "4.5",
         "1.04",
         "2.77",
         "660.0",
         "1"
        ],
        [
         "159",
         "13.48",
         "1.67",
         "2.64",
         "22.5",
         "89.0",
         "2.6",
         "1.1",
         "0.52",
         "2.29",
         "11.75",
         "0.57",
         "1.78",
         "620.0",
         "2"
        ],
        [
         "38",
         "13.07",
         "1.5",
         "2.1",
         "15.5",
         "98.0",
         "2.4",
         "2.64",
         "0.28",
         "1.37",
         "3.7",
         "1.18",
         "2.69",
         "1020.0",
         "0"
        ],
        [
         "108",
         "12.22",
         "1.29",
         "1.94",
         "19.0",
         "92.0",
         "2.36",
         "2.04",
         "0.39",
         "2.08",
         "2.7",
         "0.86",
         "3.02",
         "312.0",
         "1"
        ],
        [
         "85",
         "12.67",
         "0.98",
         "2.24",
         "18.0",
         "99.0",
         "2.2",
         "1.94",
         "0.3",
         "1.46",
         "2.62",
         "1.23",
         "3.16",
         "450.0",
         "1"
        ],
        [
         "68",
         "13.34",
         "0.94",
         "2.36",
         "17.0",
         "110.0",
         "2.53",
         "1.3",
         "0.55",
         "0.42",
         "3.17",
         "1.02",
         "1.93",
         "750.0",
         "1"
        ],
        [
         "143",
         "13.62",
         "4.95",
         "2.35",
         "20.0",
         "92.0",
         "2.0",
         "0.8",
         "0.47",
         "1.02",
         "4.4",
         "0.91",
         "2.05",
         "550.0",
         "2"
        ],
        [
         "2",
         "13.16",
         "2.36",
         "2.67",
         "18.6",
         "101.0",
         "2.8",
         "3.24",
         "0.3",
         "2.81",
         "5.68",
         "1.03",
         "3.17",
         "1185.0",
         "0"
        ],
        [
         "100",
         "12.08",
         "2.08",
         "1.7",
         "17.5",
         "97.0",
         "2.23",
         "2.17",
         "0.26",
         "1.4",
         "3.3",
         "1.27",
         "2.96",
         "710.0",
         "1"
        ],
        [
         "122",
         "12.42",
         "4.43",
         "2.73",
         "26.5",
         "102.0",
         "2.2",
         "2.13",
         "0.43",
         "1.71",
         "2.08",
         "0.92",
         "3.12",
         "365.0",
         "1"
        ],
        [
         "154",
         "12.58",
         "1.29",
         "2.1",
         "20.0",
         "103.0",
         "1.48",
         "0.58",
         "0.53",
         "1.4",
         "7.6",
         "0.58",
         "1.55",
         "640.0",
         "2"
        ],
        [
         "51",
         "13.83",
         "1.65",
         "2.6",
         "17.2",
         "94.0",
         "2.45",
         "2.99",
         "0.22",
         "2.29",
         "5.6",
         "1.24",
         "3.37",
         "1265.0",
         "0"
        ],
        [
         "76",
         "13.03",
         "0.9",
         "1.71",
         "16.0",
         "86.0",
         "1.95",
         "2.03",
         "0.24",
         "1.46",
         "4.6",
         "1.19",
         "2.48",
         "392.0",
         "1"
        ],
        [
         "56",
         "14.22",
         "1.7",
         "2.3",
         "16.3",
         "118.0",
         "3.2",
         "3.0",
         "0.26",
         "2.03",
         "6.38",
         "0.94",
         "3.31",
         "970.0",
         "0"
        ],
        [
         "26",
         "13.39",
         "1.77",
         "2.62",
         "16.1",
         "93.0",
         "2.85",
         "2.94",
         "0.34",
         "1.45",
         "4.8",
         "0.92",
         "3.22",
         "1195.0",
         "0"
        ],
        [
         "153",
         "13.23",
         "3.3",
         "2.28",
         "18.5",
         "98.0",
         "1.8",
         "0.83",
         "0.61",
         "1.87",
         "10.52",
         "0.56",
         "1.51",
         "675.0",
         "2"
        ],
        [
         "138",
         "13.49",
         "3.59",
         "2.19",
         "19.5",
         "88.0",
         "1.62",
         "0.48",
         "0.58",
         "0.88",
         "5.7",
         "0.81",
         "1.82",
         "580.0",
         "2"
        ],
        [
         "104",
         "12.51",
         "1.73",
         "1.98",
         "20.5",
         "85.0",
         "2.2",
         "1.92",
         "0.32",
         "1.48",
         "2.94",
         "1.04",
         "3.57",
         "672.0",
         "1"
        ],
        [
         "78",
         "12.33",
         "0.99",
         "1.95",
         "14.8",
         "136.0",
         "1.9",
         "1.85",
         "0.35",
         "2.76",
         "3.4",
         "1.06",
         "2.31",
         "750.0",
         "1"
        ],
        [
         "36",
         "13.28",
         "1.64",
         "2.84",
         "15.5",
         "110.0",
         "2.6",
         "2.68",
         "0.34",
         "1.36",
         "4.6",
         "1.09",
         "2.78",
         "880.0",
         "0"
        ],
        [
         "93",
         "12.29",
         "2.83",
         "2.22",
         "18.0",
         "88.0",
         "2.45",
         "2.25",
         "0.25",
         "1.99",
         "2.15",
         "1.15",
         "3.3",
         "290.0",
         "1"
        ],
        [
         "22",
         "13.71",
         "1.86",
         "2.36",
         "16.6",
         "101.0",
         "2.61",
         "2.88",
         "0.27",
         "1.69",
         "3.8",
         "1.11",
         "4.0",
         "1035.0",
         "0"
        ],
        [
         "146",
         "13.88",
         "5.04",
         "2.23",
         "20.0",
         "80.0",
         "0.98",
         "0.34",
         "0.4",
         "0.68",
         "4.9",
         "0.58",
         "1.33",
         "415.0",
         "2"
        ],
        [
         "97",
         "12.29",
         "1.41",
         "1.98",
         "16.0",
         "85.0",
         "2.55",
         "2.5",
         "0.29",
         "1.77",
         "2.9",
         "1.23",
         "2.74",
         "428.0",
         "1"
        ],
        [
         "69",
         "12.21",
         "1.19",
         "1.75",
         "16.8",
         "151.0",
         "1.85",
         "1.28",
         "0.14",
         "2.5",
         "2.85",
         "1.28",
         "3.07",
         "718.0",
         "1"
        ],
        [
         "167",
         "12.82",
         "3.37",
         "2.3",
         "19.5",
         "88.0",
         "1.48",
         "0.66",
         "0.4",
         "0.97",
         "10.26",
         "0.72",
         "1.75",
         "685.0",
         "2"
        ],
        [
         "11",
         "14.12",
         "1.48",
         "2.32",
         "16.8",
         "95.0",
         "2.2",
         "2.43",
         "0.26",
         "1.57",
         "5.0",
         "1.17",
         "2.82",
         "1280.0",
         "0"
        ],
        [
         "6",
         "14.39",
         "1.87",
         "2.45",
         "14.6",
         "96.0",
         "2.5",
         "2.52",
         "0.3",
         "1.98",
         "5.25",
         "1.02",
         "3.58",
         "1290.0",
         "0"
        ],
        [
         "27",
         "13.3",
         "1.72",
         "2.14",
         "17.0",
         "94.0",
         "2.4",
         "2.19",
         "0.27",
         "1.35",
         "3.95",
         "1.02",
         "2.77",
         "1285.0",
         "0"
        ],
        [
         "144",
         "12.25",
         "3.88",
         "2.2",
         "18.5",
         "112.0",
         "1.38",
         "0.78",
         "0.29",
         "1.14",
         "8.21",
         "0.65",
         "2.0",
         "855.0",
         "2"
        ],
        [
         "4",
         "13.24",
         "2.59",
         "2.87",
         "21.0",
         "118.0",
         "2.8",
         "2.69",
         "0.39",
         "1.82",
         "4.32",
         "1.04",
         "2.93",
         "735.0",
         "0"
        ],
        [
         "32",
         "13.68",
         "1.83",
         "2.36",
         "17.2",
         "104.0",
         "2.42",
         "2.69",
         "0.42",
         "1.97",
         "3.84",
         "1.23",
         "2.87",
         "990.0",
         "0"
        ],
        [
         "95",
         "12.47",
         "1.52",
         "2.2",
         "19.0",
         "162.0",
         "2.5",
         "2.27",
         "0.32",
         "3.28",
         "2.6",
         "1.16",
         "2.63",
         "937.0",
         "1"
        ],
        [
         "170",
         "12.2",
         "3.03",
         "2.32",
         "19.0",
         "96.0",
         "1.25",
         "0.49",
         "0.4",
         "0.73",
         "5.5",
         "0.66",
         "1.83",
         "510.0",
         "2"
        ],
        [
         "75",
         "11.66",
         "1.88",
         "1.92",
         "16.0",
         "97.0",
         "1.61",
         "1.57",
         "0.34",
         "1.15",
         "3.8",
         "1.23",
         "2.14",
         "428.0",
         "1"
        ],
        [
         "10",
         "14.1",
         "2.16",
         "2.3",
         "18.0",
         "105.0",
         "2.95",
         "3.32",
         "0.22",
         "2.38",
         "5.75",
         "1.25",
         "3.17",
         "1510.0",
         "0"
        ],
        [
         "147",
         "12.87",
         "4.61",
         "2.48",
         "21.5",
         "86.0",
         "1.7",
         "0.65",
         "0.47",
         "0.86",
         "7.65",
         "0.54",
         "1.86",
         "625.0",
         "2"
        ],
        [
         "123",
         "13.05",
         "5.8",
         "2.13",
         "21.5",
         "86.0",
         "2.62",
         "2.65",
         "0.3",
         "2.01",
         "2.6",
         "0.73",
         "3.1",
         "380.0",
         "1"
        ],
        [
         "0",
         "14.23",
         "1.71",
         "2.43",
         "15.6",
         "127.0",
         "2.8",
         "3.06",
         "0.28",
         "2.29",
         "5.64",
         "1.04",
         "3.92",
         "1065.0",
         "0"
        ],
        [
         "142",
         "13.52",
         "3.17",
         "2.72",
         "23.5",
         "97.0",
         "1.55",
         "0.52",
         "0.5",
         "0.55",
         "4.35",
         "0.89",
         "2.06",
         "520.0",
         "2"
        ],
        [
         "126",
         "12.43",
         "1.53",
         "2.29",
         "21.5",
         "86.0",
         "2.74",
         "3.15",
         "0.39",
         "1.77",
         "3.94",
         "0.69",
         "2.84",
         "352.0",
         "1"
        ],
        [
         "64",
         "12.17",
         "1.45",
         "2.53",
         "19.0",
         "104.0",
         "1.89",
         "1.75",
         "0.45",
         "1.03",
         "2.95",
         "1.45",
         "2.23",
         "355.0",
         "1"
        ],
        [
         "44",
         "13.05",
         "1.77",
         "2.1",
         "17.0",
         "107.0",
         "3.0",
         "3.0",
         "0.28",
         "2.03",
         "5.04",
         "0.88",
         "3.35",
         "885.0",
         "0"
        ],
        [
         "96",
         "11.81",
         "2.12",
         "2.74",
         "21.5",
         "134.0",
         "1.6",
         "0.99",
         "0.14",
         "1.56",
         "2.5",
         "0.95",
         "2.26",
         "625.0",
         "1"
        ],
        [
         "28",
         "13.87",
         "1.9",
         "2.8",
         "19.4",
         "107.0",
         "2.95",
         "2.97",
         "0.37",
         "1.76",
         "4.5",
         "1.25",
         "3.4",
         "915.0",
         "0"
        ],
        [
         "40",
         "13.56",
         "1.71",
         "2.31",
         "16.2",
         "117.0",
         "3.15",
         "3.29",
         "0.34",
         "2.34",
         "6.13",
         "0.95",
         "3.38",
         "795.0",
         "0"
        ],
        [
         "127",
         "11.79",
         "2.13",
         "2.78",
         "28.5",
         "92.0",
         "2.13",
         "2.24",
         "0.58",
         "1.76",
         "3.0",
         "0.97",
         "2.44",
         "466.0",
         "1"
        ],
        [
         "25",
         "13.05",
         "2.05",
         "3.22",
         "25.0",
         "124.0",
         "2.63",
         "2.68",
         "0.47",
         "1.92",
         "3.58",
         "1.13",
         "3.2",
         "830.0",
         "0"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 142
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>14.34</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.70</td>\n",
       "      <td>25.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2.70</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.96</td>\n",
       "      <td>660.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>12.53</td>\n",
       "      <td>5.51</td>\n",
       "      <td>2.64</td>\n",
       "      <td>25.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.10</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.69</td>\n",
       "      <td>515.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>12.37</td>\n",
       "      <td>1.07</td>\n",
       "      <td>2.10</td>\n",
       "      <td>18.5</td>\n",
       "      <td>88.0</td>\n",
       "      <td>3.52</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.95</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.77</td>\n",
       "      <td>660.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>13.48</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.64</td>\n",
       "      <td>22.5</td>\n",
       "      <td>89.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2.29</td>\n",
       "      <td>11.75</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.78</td>\n",
       "      <td>620.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>13.07</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.10</td>\n",
       "      <td>15.5</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.37</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.18</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>13.86</td>\n",
       "      <td>1.51</td>\n",
       "      <td>2.67</td>\n",
       "      <td>25.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.87</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.36</td>\n",
       "      <td>3.16</td>\n",
       "      <td>410.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>12.25</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.12</td>\n",
       "      <td>19.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.63</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.17</td>\n",
       "      <td>510.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.38</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.96</td>\n",
       "      <td>7.50</td>\n",
       "      <td>1.20</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1547.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>12.69</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.7</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2.06</td>\n",
       "      <td>495.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>12.34</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.46</td>\n",
       "      <td>21.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3.38</td>\n",
       "      <td>438.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "158    14.34        1.68  2.70               25.0       98.0           2.80   \n",
       "137    12.53        5.51  2.64               25.0       96.0           1.79   \n",
       "98     12.37        1.07  2.10               18.5       88.0           3.52   \n",
       "159    13.48        1.67  2.64               22.5       89.0           2.60   \n",
       "38     13.07        1.50  2.10               15.5       98.0           2.40   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "71     13.86        1.51  2.67               25.0       86.0           2.95   \n",
       "106    12.25        1.73  2.12               19.0       80.0           1.65   \n",
       "14     14.38        1.87  2.38               12.0      102.0           3.30   \n",
       "92     12.69        1.53  2.26               20.7       80.0           1.38   \n",
       "102    12.34        2.45  2.46               21.0       98.0           2.56   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "158        1.31                  0.53             2.70            13.00  0.57   \n",
       "137        0.60                  0.63             1.10             5.00  0.82   \n",
       "98         3.75                  0.24             1.95             4.50  1.04   \n",
       "159        1.10                  0.52             2.29            11.75  0.57   \n",
       "38         2.64                  0.28             1.37             3.70  1.18   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "71         2.86                  0.21             1.87             3.38  1.36   \n",
       "106        2.03                  0.37             1.63             3.40  1.00   \n",
       "14         3.64                  0.29             2.96             7.50  1.20   \n",
       "92         1.46                  0.58             1.62             3.05  0.96   \n",
       "102        2.11                  0.34             1.31             2.80  0.80   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  target  \n",
       "158                          1.96    660.0       2  \n",
       "137                          1.69    515.0       2  \n",
       "98                           2.77    660.0       1  \n",
       "159                          1.78    620.0       2  \n",
       "38                           2.69   1020.0       0  \n",
       "..                            ...      ...     ...  \n",
       "71                           3.16    410.0       1  \n",
       "106                          3.17    510.0       1  \n",
       "14                           3.00   1547.0       0  \n",
       "92                           2.06    495.0       1  \n",
       "102                          3.38    438.0       1  \n",
       "\n",
       "[142 rows x 14 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0327150",
   "metadata": {},
   "source": [
    "data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90e93c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alcohol': 'continous',\n",
       " 'malic_acid': 'continous',\n",
       " 'ash': 'continous',\n",
       " 'alcalinity_of_ash': 'continous',\n",
       " 'magnesium': 'continous',\n",
       " 'total_phenols': 'continous',\n",
       " 'flavanoids': 'continous',\n",
       " 'nonflavanoid_phenols': 'continous',\n",
       " 'proanthocyanins': 'continous',\n",
       " 'color_intensity': 'continous',\n",
       " 'hue': 'continous',\n",
       " 'od280/od315_of_diluted_wines': 'continous',\n",
       " 'proline': 'continous',\n",
       " 'target': 'ordinal_Xc_Yo'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type={key:value for key, value in zip(train_df.columns, ['continous']*13+['ordinal_Xc_Yo'])}\n",
    "data_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31eb799",
   "metadata": {},
   "source": [
    "configartion dicitonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18adb654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date_of_creation': '2025-07-31 14:50:52',\n",
       " 'experiment_name': 'ordinal_wine_example_7',\n",
       " 'PATHS': {'DATA_PATH': '/home/bule/TramDag/dev_experiment_logs/ordinal_wine_example_7',\n",
       "  'LOG_DIR': '/home/bule/TramDag/dev_experiment_logs',\n",
       "  'EXPERIMENT_DIR': '/home/bule/TramDag/dev_experiment_logs/ordinal_wine_example_7'},\n",
       " 'data_type': None,\n",
       " 'adj_matrix': None,\n",
       " 'model_names': None,\n",
       " 'seed': None,\n",
       " 'nodes': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration_dict=new_conf_dict(experiment_name,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)\n",
    "configuration_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a49ae0",
   "metadata": {},
   "source": [
    "modeloling it as a graph with one sink node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6a166ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        'LinearShift'],\n",
       "       ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        '0']], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type={key:value for key, value in zip(train_df.columns, ['continous']*13+['ordinal_Xc_Yo'])}\n",
    "\n",
    "\n",
    "columns = [\n",
    "    'alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium',\n",
    "    'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins',\n",
    "    'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline', 'target'\n",
    "]\n",
    "\n",
    "adj_matrix = np.full((len(columns), len(columns)), \"0\", dtype=object)\n",
    "\n",
    "# Set last column (edges *to* 'target') as \"ls\", excluding self-loop\n",
    "for i in range(len(columns) - 1):\n",
    "    adj_matrix[i, -1] = \"ls\"\n",
    "    \n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "nn_names_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6ece2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alcohol': {'Modelnr': 0,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 11.665000000000001,\n",
       "  'max': 14.2295,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'malic_acid': {'Modelnr': 1,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 1.0710000000000002,\n",
       "  'max': 4.600999999999998,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'ash': {'Modelnr': 2,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 1.92,\n",
       "  'max': 2.7495,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'alcalinity_of_ash': {'Modelnr': 3,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 14.030000000000001,\n",
       "  'max': 25.0,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'magnesium': {'Modelnr': 4,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 84.05,\n",
       "  'max': 126.94999999999999,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'total_phenols': {'Modelnr': 5,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 1.38,\n",
       "  'max': 3.2984999999999993,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'flavanoids': {'Modelnr': 6,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 0.5705,\n",
       "  'max': 3.485499999999999,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'nonflavanoid_phenols': {'Modelnr': 7,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 0.1905,\n",
       "  'max': 0.6,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'proanthocyanins': {'Modelnr': 8,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 0.8,\n",
       "  'max': 2.8074999999999997,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'color_intensity': {'Modelnr': 9,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 2.3075,\n",
       "  'max': 9.684999999999995,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'hue': {'Modelnr': 10,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 0.57,\n",
       "  'max': 1.3084999999999998,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'od280/od315_of_diluted_wines': {'Modelnr': 11,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 1.4224999999999999,\n",
       "  'max': 3.5795,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'proline': {'Modelnr': 12,\n",
       "  'data_type': 'continous',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 365.65000000000003,\n",
       "  'max': 1285.0,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'target': {'Modelnr': 13,\n",
       "  'data_type': 'ordinal_Xc_Yo',\n",
       "  'levels': 3,\n",
       "  'node_type': 'sink',\n",
       "  'parents': ['alcohol',\n",
       "   'malic_acid',\n",
       "   'ash',\n",
       "   'alcalinity_of_ash',\n",
       "   'magnesium',\n",
       "   'total_phenols',\n",
       "   'flavanoids',\n",
       "   'nonflavanoid_phenols',\n",
       "   'proanthocyanins',\n",
       "   'color_intensity',\n",
       "   'hue',\n",
       "   'od280/od315_of_diluted_wines',\n",
       "   'proline'],\n",
       "  'parents_datatype': {'alcohol': 'continous',\n",
       "   'malic_acid': 'continous',\n",
       "   'ash': 'continous',\n",
       "   'alcalinity_of_ash': 'continous',\n",
       "   'magnesium': 'continous',\n",
       "   'total_phenols': 'continous',\n",
       "   'flavanoids': 'continous',\n",
       "   'nonflavanoid_phenols': 'continous',\n",
       "   'proanthocyanins': 'continous',\n",
       "   'color_intensity': 'continous',\n",
       "   'hue': 'continous',\n",
       "   'od280/od315_of_diluted_wines': 'continous',\n",
       "   'proline': 'continous'},\n",
       "  'transformation_terms_in_h()': {'alcohol': 'ls',\n",
       "   'malic_acid': 'ls',\n",
       "   'ash': 'ls',\n",
       "   'alcalinity_of_ash': 'ls',\n",
       "   'magnesium': 'ls',\n",
       "   'total_phenols': 'ls',\n",
       "   'flavanoids': 'ls',\n",
       "   'nonflavanoid_phenols': 'ls',\n",
       "   'proanthocyanins': 'ls',\n",
       "   'color_intensity': 'ls',\n",
       "   'hue': 'ls',\n",
       "   'od280/od315_of_diluted_wines': 'ls',\n",
       "   'proline': 'ls'},\n",
       "  'min': 0.0,\n",
       "  'max': 2.0,\n",
       "  'transformation_term_nn_models_in_h()': {'alcohol': 'LinearShift',\n",
       "   'malic_acid': 'LinearShift',\n",
       "   'ash': 'LinearShift',\n",
       "   'alcalinity_of_ash': 'LinearShift',\n",
       "   'magnesium': 'LinearShift',\n",
       "   'total_phenols': 'LinearShift',\n",
       "   'flavanoids': 'LinearShift',\n",
       "   'nonflavanoid_phenols': 'LinearShift',\n",
       "   'proanthocyanins': 'LinearShift',\n",
       "   'color_intensity': 'LinearShift',\n",
       "   'hue': 'LinearShift',\n",
       "   'od280/od315_of_diluted_wines': 'LinearShift',\n",
       "   'proline': 'LinearShift'}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levels_dict=create_levels_dict(df,data_type)\n",
    "target_nodes=create_node_dict(adj_matrix, nn_names_matrix, data_type, min_vals, max_vals,levels_dict)\n",
    "target_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6cee33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TramModel(\n",
       "  (nn_int): SimpleIntercept(\n",
       "    (fc): Linear(in_features=1, out_features=2, bias=False)\n",
       "  )\n",
       "  (nn_shift): ModuleList(\n",
       "    (0-12): 13 x LinearShift(\n",
       "      (fc): Linear(in_features=1, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node='target'\n",
    "tram_model=get_fully_specified_tram_model(node, target_nodes, verbose=True)\n",
    "tram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfde5c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.tram_data import get_dataloader\n",
    "\n",
    "learning_rate=0.001\n",
    "use_scheduler=False\n",
    "\n",
    "\n",
    "NODE_DIR = os.path.join(EXPERIMENT_DIR, f'{node}')\n",
    "os.makedirs(NODE_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_PATH,LAST_MODEL_PATH,TRAIN_HIST_PATH,VAL_HIST_PATH=model_train_val_paths(NODE_DIR)\n",
    "\n",
    "\n",
    "optimizer =torch.optim.Adam(tram_model.parameters(), lr=learning_rate)\n",
    "\n",
    "if use_scheduler:\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "train_loader, val_loader = get_dataloader(node, target_nodes, train_df, val_df, batch_size=56,return_intercept_shift=True, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e48da1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Device: cuda\n",
      "[DEBUG] Model paths: /home/bule/TramDag/dev_experiment_logs/ordinal_wine_example_7/target/best_model.pt /home/bule/TramDag/dev_experiment_logs/ordinal_wine_example_7/target/last_model.pt\n",
      "[DEBUG] min_max shape: torch.Size([2])\n",
      "No existing model found. Starting fresh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n",
      "Epoch 1/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.56s  Val: 0.16s  Total: 0.72s]\n",
      "Epoch 2/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 3/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.21s  Val: 0.16s  Total: 0.38s]\n",
      "Epoch 4/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 5/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.21s  Val: 0.16s  Total: 0.37s]\n",
      "Epoch 6/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.20s  Val: 0.18s  Total: 0.39s]\n",
      "Epoch 7/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.19s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 8/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.22s  Val: 0.20s  Total: 0.43s]\n",
      "Epoch 9/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.22s  Val: 0.21s  Total: 0.44s]\n",
      "Epoch 10/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.22s  Val: 0.16s  Total: 0.38s]\n",
      "Epoch 11/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.21s  Val: 0.16s  Total: 0.38s]\n",
      "Epoch 12/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.24s  Val: 0.16s  Total: 0.40s]\n",
      "Epoch 13/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.21s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 14/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.21s  Val: 0.21s  Total: 0.43s]\n",
      "Epoch 15/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.24s  Val: 0.20s  Total: 0.44s]\n",
      "Epoch 16/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.21s  Val: 0.16s  Total: 0.37s]\n",
      "Epoch 17/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.21s  Val: 0.21s  Total: 0.43s]\n",
      "Epoch 18/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.25s  Val: 0.18s  Total: 0.42s]\n",
      "Epoch 19/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 20/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.19s  Val: 0.21s  Total: 0.40s]\n",
      "Epoch 21/1000  Train Loss: 25.4088  Val Loss: 22.5142  [Train: 0.20s  Val: 0.19s  Total: 0.40s]\n",
      "Epoch 22/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.24s  Val: 0.18s  Total: 0.42s]\n",
      "Epoch 23/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.24s  Val: 0.17s  Total: 0.41s]\n",
      "Epoch 24/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.21s  Val: 0.21s  Total: 0.42s]\n",
      "Epoch 25/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.22s  Val: 0.18s  Total: 0.40s]\n",
      "Epoch 26/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.24s  Val: 0.17s  Total: 0.41s]\n",
      "Epoch 27/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 28/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.21s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 29/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 30/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.20s  Val: 0.15s  Total: 0.36s]\n",
      "Epoch 31/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.21s  Val: 0.20s  Total: 0.41s]\n",
      "Epoch 32/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.23s  Val: 0.20s  Total: 0.43s]\n",
      "Epoch 33/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.24s  Val: 0.19s  Total: 0.43s]\n",
      "Epoch 34/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.43s  Val: 0.17s  Total: 0.61s]\n",
      "Epoch 35/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.44s  Val: 0.16s  Total: 0.60s]\n",
      "Epoch 36/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.22s  Val: 0.20s  Total: 0.42s]\n",
      "Epoch 37/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.26s  Val: 0.17s  Total: 0.44s]\n",
      "Epoch 38/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 39/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.23s  Val: 0.17s  Total: 0.40s]\n",
      "Epoch 40/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.37s]\n",
      "Epoch 41/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.25s  Val: 0.19s  Total: 0.44s]\n",
      "Epoch 42/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.43s]\n",
      "Epoch 43/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.24s  Val: 0.15s  Total: 0.39s]\n",
      "Epoch 44/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.21s  Val: 0.18s  Total: 0.39s]\n",
      "Epoch 45/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.43s]\n",
      "Epoch 46/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 47/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.23s  Val: 0.16s  Total: 0.39s]\n",
      "Epoch 48/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.18s  Val: 0.17s  Total: 0.35s]\n",
      "Epoch 49/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.20s  Val: 0.18s  Total: 0.38s]\n",
      "Epoch 50/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.20s  Val: 0.21s  Total: 0.41s]\n",
      "Epoch 51/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 52/1000  Train Loss: 27.3094  Val Loss: 22.5142  [Train: 0.23s  Val: 0.18s  Total: 0.41s]\n",
      "Epoch 53/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.23s  Val: 0.20s  Total: 0.44s]\n",
      "Epoch 54/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.22s  Val: 0.18s  Total: 0.40s]\n",
      "Epoch 55/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.23s  Val: 0.18s  Total: 0.42s]\n",
      "Epoch 56/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.25s  Val: 0.23s  Total: 0.49s]\n",
      "Epoch 57/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.26s  Val: 0.18s  Total: 0.44s]\n",
      "Epoch 58/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.44s  Val: 0.17s  Total: 0.62s]\n",
      "Epoch 59/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.42s  Val: 0.18s  Total: 0.61s]\n",
      "Epoch 60/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.43s  Val: 0.19s  Total: 0.62s]\n",
      "Epoch 61/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.42s  Val: 0.39s  Total: 0.81s]\n",
      "Epoch 62/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.41s  Val: 0.37s  Total: 0.78s]\n",
      "Epoch 63/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.59s  Val: 0.17s  Total: 0.77s]\n",
      "Epoch 64/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.23s  Val: 0.17s  Total: 0.41s]\n",
      "Epoch 65/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.22s  Val: 0.18s  Total: 0.40s]\n",
      "Epoch 66/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.22s  Val: 0.18s  Total: 0.41s]\n",
      "Epoch 67/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.24s  Val: 0.20s  Total: 0.44s]\n",
      "Epoch 68/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.24s  Val: 0.21s  Total: 0.46s]\n",
      "Epoch 69/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 70/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.22s  Val: 0.18s  Total: 0.41s]\n",
      "Epoch 71/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.24s  Val: 0.16s  Total: 0.40s]\n",
      "Epoch 72/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.22s  Val: 0.15s  Total: 0.38s]\n",
      "Epoch 73/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.23s  Val: 0.16s  Total: 0.39s]\n",
      "Epoch 74/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.21s  Val: 0.15s  Total: 0.36s]\n",
      "Epoch 75/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.23s  Val: 0.17s  Total: 0.40s]\n",
      "Epoch 76/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 77/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.25s  Val: 0.17s  Total: 0.43s]\n",
      "Epoch 78/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.24s  Val: 0.17s  Total: 0.41s]\n",
      "Epoch 79/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.22s  Val: 0.18s  Total: 0.41s]\n",
      "Epoch 80/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.21s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 81/1000  Train Loss: 27.3094  Val Loss: 22.5142  [Train: 0.21s  Val: 0.16s  Total: 0.37s]\n",
      "Epoch 82/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.22s  Val: 0.16s  Total: 0.38s]\n",
      "Epoch 83/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.24s  Val: 0.16s  Total: 0.40s]\n",
      "Epoch 84/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.20s  Val: 0.19s  Total: 0.40s]\n",
      "Epoch 85/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.21s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 86/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.19s  Val: 0.15s  Total: 0.35s]\n",
      "Epoch 87/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.37s]\n",
      "Epoch 88/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.22s  Val: 0.15s  Total: 0.38s]\n",
      "Epoch 89/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 90/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.37s]\n",
      "Epoch 91/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.19s  Val: 0.17s  Total: 0.36s]\n",
      "Epoch 92/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.24s  Val: 0.19s  Total: 0.44s]\n",
      "Epoch 93/1000  Train Loss: 25.4088  Val Loss: 22.5142  [Train: 0.21s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 94/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.19s  Val: 0.18s  Total: 0.38s]\n",
      "Epoch 95/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.24s  Val: 0.18s  Total: 0.43s]\n",
      "Epoch 96/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 97/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.21s  Val: 0.15s  Total: 0.37s]\n",
      "Epoch 98/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.23s  Val: 0.17s  Total: 0.41s]\n",
      "Epoch 99/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.24s  Val: 0.14s  Total: 0.38s]\n",
      "Epoch 100/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.23s  Val: 0.16s  Total: 0.39s]\n",
      "Epoch 101/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.21s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 102/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.21s  Val: 0.16s  Total: 0.37s]\n",
      "Epoch 103/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.18s  Val: 0.16s  Total: 0.34s]\n",
      "Epoch 104/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.20s  Val: 0.14s  Total: 0.34s]\n",
      "Epoch 105/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.20s  Val: 0.15s  Total: 0.36s]\n",
      "Epoch 106/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.37s]\n",
      "Epoch 107/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.40s]\n",
      "Epoch 108/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.21s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 109/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 110/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.21s  Val: 0.16s  Total: 0.37s]\n",
      "Epoch 111/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 112/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.21s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 113/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 114/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 115/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.20s  Val: 0.15s  Total: 0.36s]\n",
      "Epoch 116/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.21s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 117/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.21s  Val: 0.18s  Total: 0.40s]\n",
      "Epoch 118/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 119/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.21s  Val: 0.18s  Total: 0.39s]\n",
      "Epoch 120/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.19s  Val: 0.19s  Total: 0.39s]\n",
      "Epoch 121/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.25s  Val: 0.19s  Total: 0.44s]\n",
      "Epoch 122/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.22s  Val: 0.15s  Total: 0.38s]\n",
      "Epoch 123/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.37s]\n",
      "Epoch 124/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.21s  Val: 0.20s  Total: 0.41s]\n",
      "Epoch 125/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.24s  Val: 0.19s  Total: 0.44s]\n",
      "Epoch 126/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.23s  Val: 0.18s  Total: 0.41s]\n",
      "Epoch 127/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.22s  Val: 0.18s  Total: 0.41s]\n",
      "Epoch 128/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.24s  Val: 0.22s  Total: 0.47s]\n",
      "Epoch 129/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.22s  Val: 0.16s  Total: 0.38s]\n",
      "Epoch 130/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 131/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.23s  Val: 0.16s  Total: 0.40s]\n",
      "Epoch 132/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.21s  Val: 0.18s  Total: 0.40s]\n",
      "Epoch 133/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.24s  Val: 0.21s  Total: 0.45s]\n",
      "Epoch 134/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.23s  Val: 0.18s  Total: 0.42s]\n",
      "Epoch 135/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.21s  Val: 0.19s  Total: 0.40s]\n",
      "Epoch 136/1000  Train Loss: 25.5989  Val Loss: 22.5142  [Train: 0.23s  Val: 0.18s  Total: 0.42s]\n",
      "Epoch 137/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.22s  Val: 0.18s  Total: 0.41s]\n",
      "Epoch 138/1000  Train Loss: 25.5989  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 139/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.24s  Val: 0.18s  Total: 0.43s]\n",
      "Epoch 140/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.24s  Val: 0.16s  Total: 0.40s]\n",
      "Epoch 141/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.22s  Val: 0.18s  Total: 0.40s]\n",
      "Epoch 142/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.23s  Val: 0.20s  Total: 0.44s]\n",
      "Epoch 143/1000  Train Loss: 27.3094  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 144/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.21s  Val: 0.19s  Total: 0.40s]\n",
      "Epoch 145/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.21s  Val: 0.20s  Total: 0.41s]\n",
      "Epoch 146/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.26s  Val: 0.18s  Total: 0.44s]\n",
      "Epoch 147/1000  Train Loss: 27.3094  Val Loss: 22.5142  [Train: 0.20s  Val: 0.19s  Total: 0.40s]\n",
      "Epoch 148/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.21s  Val: 0.16s  Total: 0.38s]\n",
      "Epoch 149/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.21s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 150/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.26s  Val: 0.16s  Total: 0.41s]\n",
      "Epoch 151/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.19s  Val: 0.17s  Total: 0.36s]\n",
      "Epoch 152/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.19s  Val: 0.18s  Total: 0.37s]\n",
      "Epoch 153/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 154/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.24s  Val: 0.21s  Total: 0.46s]\n",
      "Epoch 155/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.24s  Val: 0.20s  Total: 0.45s]\n",
      "Epoch 156/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.24s  Val: 0.19s  Total: 0.44s]\n",
      "Epoch 157/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 158/1000  Train Loss: 27.3094  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.40s]\n",
      "Epoch 159/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.26s  Val: 0.20s  Total: 0.46s]\n",
      "Epoch 160/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 161/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.24s  Val: 0.18s  Total: 0.42s]\n",
      "Epoch 162/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.21s  Val: 0.18s  Total: 0.39s]\n",
      "Epoch 163/1000  Train Loss: 25.5989  Val Loss: 22.5142  [Train: 0.20s  Val: 0.19s  Total: 0.40s]\n",
      "Epoch 164/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.24s  Val: 0.18s  Total: 0.43s]\n",
      "Epoch 165/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 166/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 167/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 168/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.25s  Val: 0.18s  Total: 0.43s]\n",
      "Epoch 169/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.21s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 170/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.24s  Val: 0.19s  Total: 0.44s]\n",
      "Epoch 171/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.24s  Val: 0.21s  Total: 0.45s]\n",
      "Epoch 172/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.23s  Val: 0.20s  Total: 0.44s]\n",
      "Epoch 173/1000  Train Loss: 25.5989  Val Loss: 22.5142  [Train: 0.21s  Val: 0.18s  Total: 0.39s]\n",
      "Epoch 174/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.43s]\n",
      "Epoch 175/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.37s]\n",
      "Epoch 176/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.21s  Val: 0.21s  Total: 0.43s]\n",
      "Epoch 177/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 178/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.24s  Val: 0.17s  Total: 0.42s]\n",
      "Epoch 179/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.20s  Val: 0.18s  Total: 0.39s]\n",
      "Epoch 180/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.21s  Val: 0.20s  Total: 0.42s]\n",
      "Epoch 181/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 182/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 183/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.20s  Val: 0.15s  Total: 0.36s]\n",
      "Epoch 184/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.19s  Val: 0.16s  Total: 0.35s]\n",
      "Epoch 185/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 186/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 187/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 188/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.23s  Val: 0.18s  Total: 0.42s]\n",
      "Epoch 189/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.21s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 190/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 191/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.19s  Val: 0.14s  Total: 0.33s]\n",
      "Epoch 192/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.37s]\n",
      "Epoch 193/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.37s]\n",
      "Epoch 194/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.20s  Val: 0.20s  Total: 0.41s]\n",
      "Epoch 195/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.40s]\n",
      "Epoch 196/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.21s  Val: 0.18s  Total: 0.40s]\n",
      "Epoch 197/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.23s  Val: 0.17s  Total: 0.40s]\n",
      "Epoch 198/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.24s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 199/1000  Train Loss: 27.3094  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 200/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.23s  Val: 0.18s  Total: 0.42s]\n",
      "Epoch 201/1000  Train Loss: 25.5989  Val Loss: 22.5142  [Train: 0.20s  Val: 0.20s  Total: 0.41s]\n",
      "Epoch 202/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.21s  Val: 0.18s  Total: 0.40s]\n",
      "Epoch 203/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.23s  Val: 0.17s  Total: 0.41s]\n",
      "Epoch 204/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 205/1000  Train Loss: 27.4994  Val Loss: 22.5142  [Train: 0.20s  Val: 0.13s  Total: 0.34s]\n",
      "Epoch 206/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.19s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 207/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.18s  Val: 0.15s  Total: 0.34s]\n",
      "Epoch 208/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 209/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.22s  Val: 0.18s  Total: 0.40s]\n",
      "Epoch 210/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.24s  Val: 0.15s  Total: 0.38s]\n",
      "Epoch 211/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.21s  Val: 0.20s  Total: 0.42s]\n",
      "Epoch 212/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.21s  Val: 0.15s  Total: 0.37s]\n",
      "Epoch 213/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 214/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.19s  Val: 0.17s  Total: 0.36s]\n",
      "Epoch 215/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.19s  Val: 0.15s  Total: 0.35s]\n",
      "Epoch 216/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.20s  Val: 0.18s  Total: 0.38s]\n",
      "Epoch 217/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 218/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.21s  Val: 0.19s  Total: 0.39s]\n",
      "Epoch 219/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.21s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 220/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.23s  Val: 0.16s  Total: 0.38s]\n",
      "Epoch 221/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.22s  Val: 0.20s  Total: 0.42s]\n",
      "Epoch 222/1000  Train Loss: 25.5989  Val Loss: 22.5142  [Train: 0.21s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 223/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.23s  Val: 0.15s  Total: 0.39s]\n",
      "Epoch 224/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 225/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.21s  Val: 0.16s  Total: 0.38s]\n",
      "Epoch 226/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.23s  Val: 0.16s  Total: 0.41s]\n",
      "Epoch 227/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.24s  Val: 0.14s  Total: 0.38s]\n",
      "Epoch 228/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.20s  Val: 0.20s  Total: 0.41s]\n",
      "Epoch 229/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.23s  Val: 0.17s  Total: 0.40s]\n",
      "Epoch 230/1000  Train Loss: 27.3094  Val Loss: 22.5142  [Train: 0.20s  Val: 0.15s  Total: 0.35s]\n",
      "Epoch 231/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.19s  Val: 0.15s  Total: 0.35s]\n",
      "Epoch 232/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.37s]\n",
      "Epoch 233/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.19s  Val: 0.17s  Total: 0.37s]\n",
      "Epoch 234/1000  Train Loss: 27.6895  Val Loss: 22.5142  [Train: 0.19s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 235/1000  Train Loss: 25.5989  Val Loss: 22.5142  [Train: 0.19s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 236/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.37s]\n",
      "Epoch 237/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.18s  Val: 0.14s  Total: 0.32s]\n",
      "Epoch 238/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.19s  Val: 0.17s  Total: 0.36s]\n",
      "Epoch 239/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.19s  Val: 0.20s  Total: 0.39s]\n",
      "Epoch 240/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.21s  Val: 0.16s  Total: 0.38s]\n",
      "Epoch 241/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.21s  Val: 0.16s  Total: 0.38s]\n",
      "Epoch 242/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.20s  Val: 0.18s  Total: 0.39s]\n",
      "Epoch 243/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.21s  Val: 0.15s  Total: 0.37s]\n",
      "Epoch 244/1000  Train Loss: 25.4088  Val Loss: 22.5142  [Train: 0.21s  Val: 0.16s  Total: 0.37s]\n",
      "Epoch 245/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 246/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.21s  Val: 0.18s  Total: 0.39s]\n",
      "Epoch 247/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.23s  Val: 0.18s  Total: 0.42s]\n",
      "Epoch 248/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.21s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 249/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.23s  Val: 0.18s  Total: 0.41s]\n",
      "Epoch 250/1000  Train Loss: 25.5989  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 251/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.37s]\n",
      "Epoch 252/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.20s  Val: 0.19s  Total: 0.40s]\n",
      "Epoch 253/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.26s  Val: 0.16s  Total: 0.43s]\n",
      "Epoch 254/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.18s  Val: 0.20s  Total: 0.39s]\n",
      "Epoch 255/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.21s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 256/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.18s  Val: 0.21s  Total: 0.40s]\n",
      "Epoch 257/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.23s  Val: 0.18s  Total: 0.42s]\n",
      "Epoch 258/1000  Train Loss: 25.4088  Val Loss: 22.5142  [Train: 0.24s  Val: 0.18s  Total: 0.43s]\n",
      "Epoch 259/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.43s]\n",
      "Epoch 260/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 261/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.37s]\n",
      "Epoch 262/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.22s  Val: 0.18s  Total: 0.40s]\n",
      "Epoch 263/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.23s  Val: 0.17s  Total: 0.41s]\n",
      "Epoch 264/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 265/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.23s  Val: 0.18s  Total: 0.42s]\n",
      "Epoch 266/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 267/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.25s  Val: 0.21s  Total: 0.47s]\n",
      "Epoch 268/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.24s  Val: 0.17s  Total: 0.41s]\n",
      "Epoch 269/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.21s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 270/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.22s  Val: 0.16s  Total: 0.39s]\n",
      "Epoch 271/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.24s  Val: 0.16s  Total: 0.41s]\n",
      "Epoch 272/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.37s]\n",
      "Epoch 273/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.18s  Val: 0.19s  Total: 0.37s]\n",
      "Epoch 274/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.20s  Val: 0.19s  Total: 0.39s]\n",
      "Epoch 275/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.19s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 276/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 277/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 278/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.23s  Val: 0.18s  Total: 0.41s]\n",
      "Epoch 279/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.23s  Val: 0.20s  Total: 0.43s]\n",
      "Epoch 280/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.24s  Val: 0.17s  Total: 0.42s]\n",
      "Epoch 281/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.21s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 282/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.24s  Val: 0.16s  Total: 0.41s]\n",
      "Epoch 283/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.22s  Val: 0.18s  Total: 0.40s]\n",
      "Epoch 284/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.21s  Val: 0.19s  Total: 0.40s]\n",
      "Epoch 285/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 286/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.26s  Val: 0.19s  Total: 0.45s]\n",
      "Epoch 287/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.22s  Val: 0.18s  Total: 0.41s]\n",
      "Epoch 288/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.24s  Val: 0.19s  Total: 0.44s]\n",
      "Epoch 289/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.24s  Val: 0.18s  Total: 0.42s]\n",
      "Epoch 290/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 291/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 292/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.19s  Val: 0.15s  Total: 0.35s]\n",
      "Epoch 293/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.19s  Val: 0.15s  Total: 0.34s]\n",
      "Epoch 294/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.21s  Val: 0.17s  Total: 0.39s]\n",
      "Epoch 295/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.37s]\n",
      "Epoch 296/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.21s  Val: 0.16s  Total: 0.38s]\n",
      "Epoch 297/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.24s  Val: 0.17s  Total: 0.42s]\n",
      "Epoch 298/1000  Train Loss: 25.5989  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.37s]\n",
      "Epoch 299/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.21s  Val: 0.20s  Total: 0.42s]\n",
      "Epoch 300/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.22s  Val: 0.16s  Total: 0.39s]\n",
      "Epoch 301/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.19s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 302/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.37s]\n",
      "Epoch 303/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 304/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.43s]\n",
      "Epoch 305/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.20s  Val: 0.18s  Total: 0.38s]\n",
      "Epoch 306/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 307/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.21s  Val: 0.15s  Total: 0.37s]\n",
      "Epoch 308/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.19s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 309/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.21s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 310/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.23s  Val: 0.17s  Total: 0.41s]\n",
      "Epoch 311/1000  Train Loss: 25.5989  Val Loss: 22.5142  [Train: 0.20s  Val: 0.19s  Total: 0.39s]\n",
      "Epoch 312/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.24s  Val: 0.18s  Total: 0.43s]\n",
      "Epoch 313/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.25s  Val: 0.19s  Total: 0.44s]\n",
      "Epoch 314/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.43s]\n",
      "Epoch 315/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.25s  Val: 0.18s  Total: 0.45s]\n",
      "Epoch 316/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.24s  Val: 0.16s  Total: 0.40s]\n",
      "Epoch 317/1000  Train Loss: 27.4994  Val Loss: 22.5142  [Train: 0.21s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 318/1000  Train Loss: 25.5989  Val Loss: 22.5142  [Train: 0.27s  Val: 0.18s  Total: 0.45s]\n",
      "Epoch 319/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.19s  Val: 0.17s  Total: 0.37s]\n",
      "Epoch 320/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.21s  Val: 0.16s  Total: 0.37s]\n",
      "Epoch 321/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.19s  Val: 0.15s  Total: 0.35s]\n",
      "Epoch 322/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.18s  Val: 0.15s  Total: 0.33s]\n",
      "Epoch 323/1000  Train Loss: 27.3094  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 324/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 325/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.21s  Val: 0.16s  Total: 0.38s]\n",
      "Epoch 326/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.18s  Val: 0.18s  Total: 0.36s]\n",
      "Epoch 327/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.20s  Val: 0.20s  Total: 0.41s]\n",
      "Epoch 328/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.26s  Val: 0.16s  Total: 0.42s]\n",
      "Epoch 329/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.21s  Val: 0.18s  Total: 0.39s]\n",
      "Epoch 330/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.23s  Val: 0.16s  Total: 0.39s]\n",
      "Epoch 331/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.20s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 332/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.18s  Val: 0.19s  Total: 0.38s]\n",
      "Epoch 333/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 334/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.21s  Val: 0.17s  Total: 0.38s]\n",
      "Epoch 335/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.19s  Val: 0.17s  Total: 0.36s]\n",
      "Epoch 336/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 337/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.18s  Val: 0.17s  Total: 0.36s]\n",
      "Epoch 338/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.20s  Val: 0.16s  Total: 0.36s]\n",
      "Epoch 339/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.21s  Val: 0.18s  Total: 0.39s]\n",
      "Epoch 340/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.23s  Val: 0.18s  Total: 0.41s]\n",
      "Epoch 341/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.22s  Val: 0.18s  Total: 0.40s]\n",
      "Epoch 342/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.42s]\n",
      "Epoch 343/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.21s  Val: 0.18s  Total: 0.39s]\n",
      "Epoch 344/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.20s  Val: 0.18s  Total: 0.39s]\n",
      "Epoch 345/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.20s  Val: 0.20s  Total: 0.41s]\n",
      "Epoch 346/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.43s]\n",
      "Epoch 347/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.23s  Val: 0.17s  Total: 0.41s]\n",
      "Epoch 348/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.24s  Val: 0.19s  Total: 0.44s]\n",
      "Epoch 349/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.43s]\n",
      "Epoch 350/1000  Train Loss: 25.5989  Val Loss: 22.5142  [Train: 0.24s  Val: 0.19s  Total: 0.44s]\n",
      "Epoch 351/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.24s  Val: 0.19s  Total: 0.44s]\n",
      "Epoch 352/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.22s  Val: 0.21s  Total: 0.43s]\n",
      "Epoch 353/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.23s  Val: 0.18s  Total: 0.41s]\n",
      "Epoch 354/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.43s]\n",
      "Epoch 355/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.25s  Val: 0.17s  Total: 0.42s]\n",
      "Epoch 356/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.21s  Val: 0.18s  Total: 0.39s]\n",
      "Epoch 357/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.21s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 358/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.21s  Val: 0.20s  Total: 0.42s]\n",
      "Epoch 359/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.23s  Val: 0.17s  Total: 0.41s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x767eb5926310>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bule/anaconda3/envs/tramdag/lib/python3.9/logging/__init__.py\", line 227, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.20s  Val: 0.15s  Total: 0.36s]\n",
      "Epoch 361/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.24s  Val: 0.21s  Total: 0.46s]\n",
      "Epoch 362/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.21s  Val: 0.16s  Total: 0.37s]\n",
      "Epoch 363/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.23s  Val: 0.19s  Total: 0.43s]\n",
      "Epoch 364/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.25s  Val: 0.20s  Total: 0.45s]\n",
      "Epoch 365/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 366/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.23s  Val: 0.21s  Total: 0.45s]\n",
      "Epoch 367/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.24s  Val: 0.20s  Total: 0.45s]\n",
      "Epoch 368/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.22s  Val: 0.20s  Total: 0.43s]\n",
      "Epoch 369/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.22s  Val: 0.20s  Total: 0.42s]\n",
      "Epoch 370/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.24s  Val: 0.20s  Total: 0.45s]\n",
      "Epoch 371/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.20s  Val: 0.19s  Total: 0.40s]\n",
      "Epoch 372/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.21s  Val: 0.20s  Total: 0.41s]\n",
      "Epoch 373/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.22s  Val: 0.19s  Total: 0.41s]\n",
      "Epoch 374/1000  Train Loss: 25.7890  Val Loss: 22.5142  [Train: 0.22s  Val: 0.22s  Total: 0.45s]\n",
      "Epoch 375/1000  Train Loss: 27.3094  Val Loss: 22.5142  [Train: 0.23s  Val: 0.20s  Total: 0.44s]\n",
      "Epoch 376/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.24s  Val: 0.17s  Total: 0.41s]\n",
      "Epoch 377/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.26s  Val: 0.16s  Total: 0.43s]\n",
      "Epoch 378/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.20s  Val: 0.18s  Total: 0.38s]\n",
      "Epoch 379/1000  Train Loss: 26.9293  Val Loss: 22.5142  [Train: 0.22s  Val: 0.20s  Total: 0.43s]\n",
      "Epoch 380/1000  Train Loss: 26.1691  Val Loss: 22.5142  [Train: 0.23s  Val: 0.20s  Total: 0.44s]\n",
      "Epoch 381/1000  Train Loss: 27.1193  Val Loss: 22.5142  [Train: 0.22s  Val: 0.16s  Total: 0.38s]\n",
      "Epoch 382/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.21s  Val: 0.18s  Total: 0.39s]\n",
      "Epoch 383/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.24s  Val: 0.16s  Total: 0.41s]\n",
      "Epoch 384/1000  Train Loss: 25.9790  Val Loss: 22.5142  [Train: 0.20s  Val: 0.19s  Total: 0.40s]\n",
      "Epoch 385/1000  Train Loss: 26.3591  Val Loss: 22.5142  [Train: 0.22s  Val: 0.18s  Total: 0.40s]\n",
      "Epoch 386/1000  Train Loss: 26.7392  Val Loss: 22.5142  [Train: 0.20s  Val: 0.18s  Total: 0.39s]\n",
      "Epoch 387/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.22s  Val: 0.17s  Total: 0.40s]\n",
      "Epoch 388/1000  Train Loss: 26.5492  Val Loss: 22.5142  [Train: 0.21s  Val: 0.20s  Total: 0.42s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# TODO initialize with correct weights\n",
    "epochs =1000\n",
    "train_val_loop(\n",
    "            node,\n",
    "            target_nodes,\n",
    "            NODE_DIR,\n",
    "            tram_model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            epochs,\n",
    "            optimizer,\n",
    "            use_scheduler,\n",
    "            scheduler,\n",
    "            save_linear_shifts=False,\n",
    "            verbose=1,\n",
    "            device=device,\n",
    "            debug=True) # TODO startvalues for bernsteinpols\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a57f405",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def7e535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing model found. Loading weights and history...\n"
     ]
    }
   ],
   "source": [
    "## laoding the best model \n",
    "MODEL_PATH,LAST_MODEL_PATH,TRAIN_HIST_PATH,VAL_HIST_PATH=model_train_val_paths(NODE_DIR)\n",
    "\n",
    "if os.path.exists(MODEL_PATH) and os.path.exists(TRAIN_HIST_PATH) and os.path.exists(VAL_HIST_PATH):\n",
    "    print(\"Existing model found. Loading weights and history...\")\n",
    "    tram_model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789baa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 28.6%\n",
      "Accuracy: 35.7%\n",
      "Accuracy: 30.0%\n",
      "Accuracy: 38.9%\n"
     ]
    }
   ],
   "source": [
    "## evaluate on testdata\n",
    "tram_model.eval()\n",
    "\n",
    "_, ordered_transformation_terms_in_h, _=ordered_parents(node, target_nodes)\n",
    "\n",
    "min_vals = torch.tensor(target_nodes[node]['min'], dtype=torch.float32).to(device)\n",
    "max_vals = torch.tensor(target_nodes[node]['max'], dtype=torch.float32).to(device)\n",
    "min_max = torch.stack([min_vals, max_vals], dim=0)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (int_input, shift_list), y in train_loader:\n",
    "        int_input = int_input.to(device)\n",
    "        shift_list = [s.to(device) for s in shift_list]\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred = tram_model(int_input=int_input, shift_input=shift_list)\n",
    "        # loss = contram_nll(y_pred, y, min_max=min_max)\n",
    "        pred_labels = get_pdf_ordinal(get_cdf_ordinal(y_pred)).argmax(dim=1)\n",
    "        true_labels = y.argmax(dim=1)\n",
    "        accuracy = (pred_labels == true_labels).float().mean().item()\n",
    "\n",
    "        print(f\"Accuracy: {accuracy*100:.1f}%\")  # â†’ 100.0%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (int_input, shift_list), y in val_loader:\n",
    "        int_input = int_input.to(device)\n",
    "        shift_list = [s.to(device) for s in shift_list]\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred = tram_model(int_input=int_input, shift_input=shift_list)\n",
    "        # loss = contram_nll(y_pred, y, min_max=min_max)\n",
    "        pred_labels = get_pdf_ordinal(get_cdf_ordinal(y_pred)).argmax(dim=1)\n",
    "        true_labels = y.argmax(dim=1)\n",
    "        accuracy = (pred_labels == true_labels).float().mean().item()\n",
    "\n",
    "        print(f\"Accuracy: {accuracy*100:.1f}%\")  # â†’ 100.0%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
