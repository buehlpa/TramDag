{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32793ca7",
   "metadata": {},
   "source": [
    "## tesing dataloader V4\n",
    "\n",
    "new ideas with ordinal c and yc \n",
    "has ordinal binary and continous outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee5f4e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with GPU support.\n"
     ]
    }
   ],
   "source": [
    "# Load dependencies\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from utils.graph import *\n",
    "from utils.loss_ordinal import *\n",
    "from utils.tram_model_helpers import *\n",
    "from utils.tram_models import *\n",
    "from utils.tram_data import *\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Train with GPU support.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU found, train with CPU support.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ada57",
   "metadata": {},
   "source": [
    "adjustet funcitnos for ordinal outcomes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8678f4f3",
   "metadata": {},
   "source": [
    "dev ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd61311",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"testing_v4_dataloader\"   ## <--- set experiment name\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "\n",
    "LOG_DIR=\"/home/bule/TramDag/dev_experiment_logs\"\n",
    "EXPERIMENT_DIR = os.path.join(LOG_DIR, experiment_name)\n",
    "DATA_PATH = EXPERIMENT_DIR # <----------- change to different source if needed\n",
    "CONF_DICT_PATH = os.path.join(EXPERIMENT_DIR, f\"configuration.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeac2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### New Functions and classes\n",
    "\n",
    "\n",
    "def create_levels_dict(df:pd.DataFrame,data_type:dict):\n",
    "    # creates the levels dictionary for variables which should be modelled ordinaly \n",
    "    levels_dict={}\n",
    "    for variable,datatype in data_type.items():\n",
    "            if \"ordinal\" in datatype.lower():\n",
    "                unique_vals = set(df[variable].dropna().unique())\n",
    "                num_classes = len(unique_vals)\n",
    "\n",
    "                expected_vals = set(range(num_classes))\n",
    "                if unique_vals != expected_vals:\n",
    "                    raise ValueError(\n",
    "                        f\"Variable '{variable}' has values {sorted(unique_vals)}, \"\n",
    "                        f\"but expected values are {sorted(expected_vals)} (0 to {num_classes - 1}). \"\n",
    "                        \"Multiclass ordinal variables must be zero-indexed and contiguous.\"\n",
    "                )\n",
    "                levels_dict[variable]=len(np.unique(df[variable]))\n",
    "    return levels_dict   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_node_dict_v3(adj_matrix, nn_names_matrix, data_type, min_vals, max_vals,levels_dict=None):\n",
    "    \"\"\"\n",
    "    Creates a configuration dictionary for TRAMADAG based on an adjacency matrix,\n",
    "    a neural network names matrix, and a data type dictionary.\n",
    "    \"\"\"\n",
    "    if not validate_adj_matrix(adj_matrix):\n",
    "        raise ValueError(\"Invalid adjacency matrix. Please check the criteria.\")\n",
    "    \n",
    "    if len(data_type) != adj_matrix.shape[0]:\n",
    "        raise ValueError(\"Data type dictionary should have the same length as the adjacency matrix.\")\n",
    "    \n",
    "    target_nodes = {}\n",
    "    G, edge_labels = create_nx_graph(adj_matrix, node_labels=list(data_type.keys()))\n",
    "    \n",
    "    sources = [node for node in G.nodes if G.in_degree(node) == 0]\n",
    "    sinks = [node for node in G.nodes if G.out_degree(node) == 0]\n",
    "    \n",
    "    for i, node in enumerate(G.nodes):\n",
    "        parents = list(G.predecessors(node))\n",
    "        target_nodes[node] = {}\n",
    "        target_nodes[node]['Modelnr'] = i\n",
    "        target_nodes[node]['data_type'] = data_type[node]\n",
    "        \n",
    "        # write the levels of the ordinal outcome\n",
    "        if 'ordinal' in data_type[node]:\n",
    "            if levels_dict is None:\n",
    "                raise ValueError(\n",
    "                    \"levels_dict must be provided for ordinal nodes; \"\n",
    "                    \"e.g. levels_dict={'x3': 3}\"\n",
    "                )\n",
    "            if node not in levels_dict:\n",
    "                raise KeyError(\n",
    "                    f\"levels_dict is missing an entry for node '{node}'. \"\n",
    "                    f\"Expected something like levels_dict['{node}'] = <num_levels>\"\n",
    "                )\n",
    "            target_nodes[node]['levels'] = levels_dict[node]\n",
    "    \n",
    "        target_nodes[node]['node_type'] = \"source\" if node in sources else \"sink\" if node in sinks else \"internal\"\n",
    "        target_nodes[node]['parents'] = parents\n",
    "        target_nodes[node]['parents_datatype'] = {parent:data_type[parent] for parent in parents}\n",
    "        target_nodes[node]['transformation_terms_in_h()'] = {parent: edge_labels[(parent, node)] for parent in parents if (parent, node) in edge_labels}\n",
    "        target_nodes[node]['min'] = min_vals.iloc[i].tolist()   \n",
    "        target_nodes[node]['max'] = max_vals.iloc[i].tolist()\n",
    "\n",
    "        \n",
    "        transformation_term_nn_models = {}\n",
    "        for parent in parents:\n",
    "            parent_idx = list(data_type.keys()).index(parent)  \n",
    "            child_idx = list(data_type.keys()).index(node) \n",
    "            \n",
    "            if nn_names_matrix[parent_idx, child_idx] != \"0\":\n",
    "                transformation_term_nn_models[parent] = nn_names_matrix[parent_idx, child_idx]\n",
    "        target_nodes[node]['transformation_term_nn_models_in_h()'] = transformation_term_nn_models\n",
    "    return target_nodes\n",
    "\n",
    "\n",
    "class GenericDataset_v4(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        target_col,\n",
    "        target_nodes=None,\n",
    "        parents_dataype_dict=None,\n",
    "        transformation_terms_in_h=None,\n",
    "        transform=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        df: pd.DataFrame\n",
    "        target_col: str\n",
    "        target_nodes: dict mapping each node → metadata (including 'data_type')\n",
    "        parents_dataype_dict: dict var_name → \"cont\"|\"ord\"|\"other\"\n",
    "        transform: torchvision transform for images\n",
    "        transformation_terms_in_h: dict for intercept logic\n",
    "        \"\"\"\n",
    "                \n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.target_col = target_col\n",
    "        self.target_nodes = target_nodes or {}\n",
    "        self.parents_dataype_dict = parents_dataype_dict or {}\n",
    "        self.predictors = list(self.parents_dataype_dict.keys())\n",
    "        self.transform = transform\n",
    "        self.transformation_terms_in_h = transformation_terms_in_h or {}\n",
    "        \n",
    "        self.target_is_source= True if target_nodes[self.target_col].get('node_type').lower() == \"source\" else False\n",
    "        self.h_needs_simple_intercept=True if all('i' not in str(v) for v in self.transformation_terms_in_h.values()) else False\n",
    "        self.ordinal_num_classes ={\n",
    "                        var: self.df[var].nunique() for var in self.predictors\n",
    "                        if \"ordinal\" in self.parents_dataype_dict[var].lower() and \"Xn\".lower() in self.parents_dataype_dict[var].lower()\n",
    "                        }\n",
    "        \n",
    "        self.target_data_type=self.target_nodes[self.target_col].get('data_type').lower()\n",
    "        self.target_num_classes=self.target_nodes[self.target_col].get('levels') or None # should be none anywasy if levels not exits\n",
    "        \n",
    "        #checks\n",
    "        self._check_multiclass_predictors_of_df()\n",
    "        self._check_ordinal_levels()\n",
    "    \n",
    "    def _transform_y(self,row):\n",
    "        #returns continous or onehot encoded target\n",
    "        if  self.target_data_type==\"continous\" or \"Yc\".lower() in self.target_data_type:\n",
    "                y = torch.tensor(row[self.target_col], dtype=torch.float32)\n",
    "                return y\n",
    "        elif self.target_num_classes is not None:\n",
    "                        raw = row[self.target_col]\n",
    "                        y_int = int(raw)\n",
    "                        y = F.one_hot(torch.tensor(y_int, dtype=torch.long), num_classes=self.target_num_classes).float().squeeze()\n",
    "                        return y    \n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Could not determine how to encode target '{self.target_col}'.\\n\"\n",
    "                f\"target_data_type: {self.target_data_type}, target_num_classes: {self.target_num_classes}\"\n",
    "            )\n",
    "\n",
    "        \n",
    "    def _check_multiclass_predictors_of_df(self):\n",
    "        # checks whether the predictors(predictors) have star\n",
    "        for var in self.predictors:\n",
    "            dtype = self.parents_dataype_dict[var]\n",
    "            # continous predictors or ordinal predictors modelled as continous are skippped\n",
    "            if \"ordinal\" not in dtype:\n",
    "                continue\n",
    "            if \"Xc\" in dtype:\n",
    "                continue\n",
    "            \n",
    "            # if ordinal varibale is nominally modelled -> check wheter the levels of the varibales start at 0 , minimal level must be 0\n",
    "            elif \"Xn\".lower() in dtype.lower():\n",
    "                unique_vals = set(self.df[var].dropna().unique())\n",
    "                num_classes = len(unique_vals)\n",
    "\n",
    "                expected_vals = set(range(num_classes))\n",
    "                if unique_vals != expected_vals:\n",
    "                    raise ValueError(\n",
    "                        f\"Variable '{var}' has values {sorted(unique_vals)}, \"\n",
    "                        f\"but expected values are {sorted(expected_vals)} (0 to {num_classes - 1}). \"\n",
    "                        \"Multiclass ordinal predictors must be zero-indexed and contiguous.\"\n",
    "                )\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    def _check_ordinal_levels(self):\n",
    "        \"\"\"\n",
    "        Ensures all ordinal variables (including the target) have a 'levels' key in target_nodes,\n",
    "        and that the values in the DataFrame are zero-indexed and contiguous.\n",
    "        \"\"\"\n",
    "        all_ordinal_vars = [self.target_col] if \"ordinal\" in self.target_nodes[self.target_col]['data_type'] else []\n",
    "\n",
    "        all_ordinal_vars+=[\n",
    "            var for var in self.predictors\n",
    "            if \"ordinal\" in self.parents_dataype_dict[var].lower()\n",
    "            and \"xn\" in self.parents_dataype_dict[var].lower()\n",
    "        ]\n",
    "        \n",
    "        for var in all_ordinal_vars:\n",
    "            node_info = self.target_nodes.get(var, {})\n",
    "            levels = node_info.get(\"levels\")\n",
    "            if levels is None:\n",
    "                raise ValueError(\n",
    "                    f\"[Ordinal Check] Variable '{var}' is marked ordinal but has no 'levels' entry in target_nodes.\"\n",
    "                )\n",
    "\n",
    "            unique_vals = sorted(self.df[var].dropna().unique())\n",
    "            expected_vals = list(range(levels))\n",
    "\n",
    "            if unique_vals != expected_vals:\n",
    "                raise ValueError(\n",
    "                    f\"[Ordinal Check] Variable '{var}' has values {unique_vals}, \"\n",
    "                    f\"but expected values {expected_vals} (0 to {levels - 1}).\\n\"\n",
    "                    f\"Fix this in the DataFrame or correct 'levels' in target_nodes.\"\n",
    "                )\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # ---------------------------------------- SOURCE NODE: no parents → x = [1.0] ---\n",
    "        if  self.target_is_source:\n",
    "            x_data = [torch.tensor(1.0)]\n",
    "            y=self._transform_y(row)\n",
    "            return tuple(x_data), y\n",
    "\n",
    "        # ---------------------------------------- if not source prepare data  ---\n",
    "        x_data = []\n",
    "\n",
    "        # --- SIMPLE INTERCEPT if needed  first term in x is x = [1.0]---\n",
    "        if self.h_needs_simple_intercept:\n",
    "            x_data.append(torch.tensor(1.0))\n",
    "\n",
    "        # --- BUILD FEATURES ---\n",
    "        for var in self.predictors:\n",
    "            dtype = self.parents_dataype_dict[var].lower()\n",
    "            ## Continous  feature\n",
    "            if dtype == \"continous\" or \"Xc\".lower() in dtype:\n",
    "                x_data.append(torch.tensor(row[var], dtype=torch.float32))\n",
    "                \n",
    "            ## Ordinal feature , if it has more thatn 2 classes it uses onehotencodig, if binary use just 0 and 1\n",
    "            elif \"ordinal\" in dtype and \"Xn\".lower() in dtype:\n",
    "                x_ord = int(row[var])\n",
    "                var_num_classes = self.ordinal_num_classes[var]\n",
    "                x_ord_onehot = F.one_hot(torch.tensor(x_ord, dtype=torch.long),num_classes=var_num_classes).float()\n",
    "                \n",
    "                x_data.append(x_ord_onehot.squeeze())\n",
    "\n",
    "            else:  # \"other\"\n",
    "                img = Image.open(row[var]).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                x_data.append(img)\n",
    "\n",
    "        # --- BUILD TARGET ---\n",
    "        y=self._transform_y(row)\n",
    "            \n",
    "            \n",
    "        return tuple(x_data), y\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloader_v4(node, target_nodes, train_df, val_df, batch_size=32, verbose=False):\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    ordered_parents_dataype_dict, ordered_transformation_terms_in_h, _ = ordered_parents(node, target_nodes)\n",
    "    if verbose:\n",
    "        print(f\"Parents dtype: {ordered_parents_dataype_dict}\")\n",
    "    train_ds = GenericDataset_v4(train_df,target_col=node,target_nodes=target_nodes,parents_dataype_dict=ordered_parents_dataype_dict,transform=transform,transformation_terms_in_h=ordered_transformation_terms_in_h)\n",
    "    val_ds = GenericDataset_v4(val_df,target_col=node,target_nodes=target_nodes,parents_dataype_dict=ordered_parents_dataype_dict,transform=transform,transformation_terms_in_h=ordered_transformation_terms_in_h)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be112e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# datatype= \"ordinal_Xc_Yc\" # as X its continous aswell as Y continous\n",
    "# datatype= \"ordinal_Xc_Yo\"\n",
    "# datatype= \"ordinal_Xn_Yc\"\n",
    "# datatype= \"ordinal_Xn_Yo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc1965b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------x1-------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1.])]\n",
      "tensor([-1.5148])\n",
      "-----------------ord_bin1-------------------\n",
      "[tensor([1.])]\n",
      "tensor([[1., 0.]])\n",
      "-----------------ord_multi-------------------\n",
      "[tensor([1.])]\n",
      "tensor([[0., 0., 0., 1.]])\n",
      "-----------------ord_bin2-------------------\n",
      "[tensor([1.]), tensor([-0.9815]), tensor([[1., 0.]]), tensor([[0., 1., 0., 0.]])]\n",
      "tensor([[1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"x1\": np.random.normal(loc=0, scale=1, size=1000),\n",
    "    # \"x2\": np.random.uniform(low=0, high=10, size=1000),\n",
    "    \"ord_bin1\":  np.random.binomial(1, p=0.4, size=1000),\n",
    "    \"ord_multi\": np.random.choice([0, 1, 2, 3], size=1000, p=[0.2, 0.3, 0.3, 0.2]),\n",
    "    \"ord_bin2\":  np.random.binomial(1, p=0.4, size=1000),\n",
    "\n",
    "})\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "quantiles = train_df.quantile([0.05, 0.95])\n",
    "min_vals = quantiles.loc[0.05]\n",
    "max_vals = quantiles.loc[0.95]\n",
    "\n",
    "\n",
    "data_type={key:value for key, value in zip(train_df.columns, ['continous']*1+['ordinal_Xn_Yo']*2+['ordinal_Xn_Yo'])}\n",
    "\n",
    "configuration_dict=new_conf_dict(experiment_name,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)\n",
    "\n",
    "columns = list(data_type.keys())\n",
    "adj_matrix = np.full((len(columns), len(columns)), \"0\", dtype=object)\n",
    "for i in range(len(columns)-1):\n",
    "    adj_matrix[i, -1] = \"ls\"\n",
    "            \n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "levels_dict=create_levels_dict(df,data_type)\n",
    "target_nodes=create_node_dict_v3(adj_matrix, nn_names_matrix, data_type, min_vals, max_vals,levels_dict)\n",
    "\n",
    "for node in target_nodes:\n",
    "    print(f'-----------------{node}-------------------')\n",
    "    train_loader, _ = get_dataloader_v4(node, target_nodes, train_df, val_df, batch_size=1, verbose=False)\n",
    "    x,y =next(iter(train_loader))\n",
    "\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40693e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': 'continous', 'ord_bin1': 'ordinal_Xc_Yc', 'ord_multi': 'ordinal_Xn_Yc', 'ord_bin2': 'ordinal_Xn_Yc'}\n",
      "-----------------x1-------------------\n",
      "[tensor([1.])]\n",
      "tensor([-1.0777])\n",
      "-----------------ord_bin1-------------------\n",
      "[tensor([1.])]\n",
      "tensor([1.])\n",
      "-----------------ord_multi-------------------\n",
      "[tensor([1.])]\n",
      "tensor([2.])\n",
      "-----------------ord_bin2-------------------\n",
      "[tensor([1.]), tensor([-0.6270]), tensor([0.]), tensor([[0., 0., 1., 0.]])]\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "data_type={key:value for key, value in zip(train_df.columns, ['continous']+['ordinal_Xc_Yc']+['ordinal_Xn_Yc']+['ordinal_Xn_Yc'])}\n",
    "print(data_type)\n",
    "configuration_dict=new_conf_dict(experiment_name,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)\n",
    "\n",
    "columns = list(data_type.keys())\n",
    "adj_matrix = np.full((len(columns), len(columns)), \"0\", dtype=object)\n",
    "for i in range(len(columns)-1):\n",
    "    adj_matrix[i, -1] = \"ls\"\n",
    "            \n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "levels_dict=create_levels_dict(df,data_type)\n",
    "target_nodes=create_node_dict_v3(adj_matrix, nn_names_matrix, data_type, min_vals, max_vals,levels_dict)\n",
    "\n",
    "for node in target_nodes:\n",
    "    print(f'-----------------{node}-------------------')\n",
    "    train_loader, _ = get_dataloader_v4(node, target_nodes, train_df, val_df, batch_size=1, verbose=False)\n",
    "    x,y =next(iter(train_loader))\n",
    "\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f339636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': 'continous', 'ord_bin1': 'ordinal_Xn_Yo', 'ord_multi': 'ordinal_Xn_Yo', 'ord_bin2': 'ordinal_Xn_Yo'}\n",
      "-----------------x1-------------------\n",
      "[tensor([1.])]\n",
      "tensor([0.1981])\n",
      "-----------------ord_bin1-------------------\n",
      "[tensor([1.])]\n",
      "tensor([[1., 0.]])\n",
      "-----------------ord_multi-------------------\n",
      "[tensor([1.])]\n",
      "tensor([[0., 0., 0., 1.]])\n",
      "-----------------ord_bin2-------------------\n",
      "[tensor([1.]), tensor([0.1504]), tensor([[0., 1.]]), tensor([[0., 0., 0., 1.]])]\n",
      "tensor([[1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "data_type={key:value for key, value in zip(train_df.columns, ['continous']+['ordinal_Xn_Yo']+['ordinal_Xn_Yo']+['ordinal_Xn_Yo'])}\n",
    "print(data_type)\n",
    "configuration_dict=new_conf_dict(experiment_name,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)\n",
    "\n",
    "columns = list(data_type.keys())\n",
    "adj_matrix = np.full((len(columns), len(columns)), \"0\", dtype=object)\n",
    "for i in range(len(columns)-1):\n",
    "    adj_matrix[i, -1] = \"ls\"\n",
    "            \n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "levels_dict=create_levels_dict(df,data_type)\n",
    "target_nodes=create_node_dict_v3(adj_matrix, nn_names_matrix, data_type, min_vals, max_vals,levels_dict)\n",
    "\n",
    "for node in target_nodes:\n",
    "    print(f'-----------------{node}-------------------')\n",
    "    train_loader, _ = get_dataloader_v4(node, target_nodes, train_df, val_df, batch_size=1, verbose=False)\n",
    "    x,y =next(iter(train_loader))\n",
    "\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30489b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': 'continous', 'ord_bin1': 'ordinal_Xc_Yo', 'ord_multi': 'ordinal_Xc_Yc', 'ord_bin2': 'ordinal_Xc_Yo'}\n",
      "-----------------x1-------------------\n",
      "[tensor([1.])]\n",
      "tensor([-1.7630])\n",
      "-----------------ord_bin1-------------------\n",
      "[tensor([1.])]\n",
      "tensor([[0., 1.]])\n",
      "-----------------ord_multi-------------------\n",
      "[tensor([1.])]\n",
      "tensor([2.])\n",
      "-----------------ord_bin2-------------------\n",
      "[tensor([1.]), tensor([1.8862]), tensor([0.]), tensor([1.])]\n",
      "tensor([[0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "data_type={key:value for key, value in zip(train_df.columns, ['continous']+['ordinal_Xc_Yo']+['ordinal_Xc_Yc']+['ordinal_Xc_Yo'])}\n",
    "print(data_type)\n",
    "configuration_dict=new_conf_dict(experiment_name,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)\n",
    "\n",
    "columns = list(data_type.keys())\n",
    "adj_matrix = np.full((len(columns), len(columns)), \"0\", dtype=object)\n",
    "for i in range(len(columns)-1):\n",
    "    adj_matrix[i, -1] = \"ls\"\n",
    "            \n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "levels_dict=create_levels_dict(df,data_type)\n",
    "target_nodes=create_node_dict_v3(adj_matrix, nn_names_matrix, data_type, min_vals, max_vals,levels_dict)\n",
    "\n",
    "for node in target_nodes:\n",
    "    print(f'-----------------{node}-------------------')\n",
    "    train_loader, _ = get_dataloader_v4(node, target_nodes, train_df, val_df, batch_size=1, verbose=False)\n",
    "    x,y =next(iter(train_loader))\n",
    "\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8c97b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': 'continous', 'ord_bin1': 'ordinal_Xc_Yc', 'ord_multi': 'ordinal_Xc_Yc', 'ord_bin2': 'ordinal_Xc_Yc'}\n",
      "-----------------x1-------------------\n",
      "[tensor([1.])]\n",
      "tensor([0.1504])\n",
      "-----------------ord_bin1-------------------\n",
      "[tensor([1.])]\n",
      "tensor([0.])\n",
      "-----------------ord_multi-------------------\n",
      "[tensor([1.])]\n",
      "tensor([2.])\n",
      "-----------------ord_bin2-------------------\n",
      "[tensor([1.]), tensor([1.1356]), tensor([0.]), tensor([1.])]\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "data_type={key:value for key, value in zip(train_df.columns, ['continous']+['ordinal_Xc_Yc']+['ordinal_Xc_Yc']+['ordinal_Xc_Yc'])}\n",
    "print(data_type)\n",
    "configuration_dict=new_conf_dict(experiment_name,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)\n",
    "\n",
    "columns = list(data_type.keys())\n",
    "adj_matrix = np.full((len(columns), len(columns)), \"0\", dtype=object)\n",
    "for i in range(len(columns)-1):\n",
    "    adj_matrix[i, -1] = \"ls\"\n",
    "            \n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "levels_dict=create_levels_dict(df,data_type)\n",
    "target_nodes=create_node_dict_v3(adj_matrix, nn_names_matrix, data_type, min_vals, max_vals,levels_dict)\n",
    "\n",
    "for node in target_nodes:\n",
    "    print(f'-----------------{node}-------------------')\n",
    "    train_loader, _ = get_dataloader_v4(node, target_nodes, train_df, val_df, batch_size=1, verbose=False)\n",
    "    x,y =next(iter(train_loader))\n",
    "\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb0fe767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs_v2(x, transformation_terms, device='cuda'):\n",
    "    \"\"\"\n",
    "    Prepares model input by grouping features by transformation term base:\n",
    "      - ci11, ci12 → 'ci1' (intercept)\n",
    "      - cs11, cs12 → 'cs1' (shift)\n",
    "      - cs21 → 'cs2' (another shift group)\n",
    "      - cs, ls → treated as full group keys\n",
    "    Returns:\n",
    "      - int_inputs: Tensor of shape (B, n_features) for intercept model\n",
    "      - shift_list: List of tensors for each shift model, shape (B, group_features)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    transformation_terms=list(transformation_terms)\n",
    "    \n",
    "    print(transformation_terms)\n",
    "    ## if there is only a source so transforamtion terms is 0:\n",
    "    x = [xi.to(device, non_blocking=True) for xi in x]\n",
    "    if len(transformation_terms)== 0:\n",
    "        x = [xi.unsqueeze(1) for xi in x] \n",
    "        int_inputs= x[0]\n",
    "        return int_inputs, None\n",
    "\n",
    "    # Always ensure there's an intercept term\n",
    "    if not any('ci' in str(value) for value in transformation_terms):\n",
    "        transformation_terms.insert(0, 'si')\n",
    "\n",
    "    # Lists to collect intercept tensors and shift‐groups\n",
    "    int_tensors = []\n",
    "    shift_groups = []\n",
    "\n",
    "    # Helpers to track the “current” shift‐group for numbered suffixes\n",
    "    current_group = None\n",
    "    current_key = None\n",
    "\n",
    "    for tensor, term in zip(x, transformation_terms):\n",
    "        # 1) INTERCEPT terms (si*, ci*)\n",
    "        if term.startswith(('si','ci')):\n",
    "            int_tensors.append(tensor)\n",
    "\n",
    "        # 2) SHIFT terms (cs*, ls*)\n",
    "        elif term.startswith(('cs','ls')):\n",
    "            # numbered suffix → group by the first 3 chars (e.g. 'cs11'/'cs12' → 'cs1')\n",
    "            if len(term) > 2 and term[2].isdigit():\n",
    "                key = term[:3]\n",
    "                # start a new group if key changed\n",
    "                if current_group is None or current_key != key:\n",
    "                    current_group = []\n",
    "                    shift_groups.append(current_group)\n",
    "                    current_key = key\n",
    "                current_group.append(tensor)\n",
    "\n",
    "            # lone 'cs' or 'ls' → always its own group\n",
    "            else:\n",
    "                current_group = [tensor]\n",
    "                shift_groups.append(current_group)\n",
    "                current_key = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown transformation term: {term}\")\n",
    "\n",
    "    # Intercept: should be exactly one group\n",
    "    if len(int_tensors) == 0:\n",
    "        raise ValueError(\"No intercept tensors found!\")\n",
    "    int_inputs = torch.cat(\n",
    "        [t.to(device, non_blocking=True).view(t.shape[0], -1) for t in int_tensors],\n",
    "        dim=1\n",
    "    )\n",
    "\n",
    "    shift_list = [\n",
    "        torch.cat([t.to(device, non_blocking=True).view(t.shape[0], -1) for t in group], dim=1)\n",
    "        for group in shift_groups\n",
    "    ]\n",
    "\n",
    "    return int_inputs, shift_list if shift_list else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3822274a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': 'continous', 'ord_bin1': 'ordinal_Xn_Yo', 'ord_multi': 'ordinal_Xn_Yo', 'ord_bin2': 'ordinal_Xn_Yo'}\n",
      "*************\n",
      " Model has Complex intercepts and Complex shifts, please add your Model to the modelzoo \n",
      "*************\n",
      "-----------------x1-------------------\n",
      "[tensor([1.])]\n",
      "tensor([0.0675])\n",
      "[]\n",
      "-----------------ord_bin1-------------------\n",
      "[tensor([1.])]\n",
      "tensor([[0., 1.]])\n",
      "[]\n",
      "-----------------ord_multi-------------------\n",
      "[tensor([1.]), tensor([-0.2258]), tensor([[0., 1.]])]\n",
      "tensor([[0., 1., 0., 0.]])\n",
      "[np.str_('cs11'), np.str_('cs12')]\n",
      "-----------------ord_bin2-------------------\n",
      "[tensor([-0.0543]), tensor([[0., 1.]]), tensor([[0., 0., 1., 0.]])]\n",
      "tensor([[1., 0.]])\n",
      "[np.str_('ci11'), np.str_('ci12'), np.str_('cs')]\n"
     ]
    }
   ],
   "source": [
    "data_type={key:value for key, value in zip(train_df.columns, ['continous']+['ordinal_Xn_Yo']+['ordinal_Xn_Yo']+['ordinal_Xn_Yo'])}\n",
    "print(data_type)\n",
    "configuration_dict=new_conf_dict(experiment_name,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)\n",
    "\n",
    "adj_matrix=np.array([['0', '0', 'cs11', 'ci11'],\n",
    "                     ['0', '0', 'cs12', 'ci12'],\n",
    "                     ['0', '0', '0', 'cs'],\n",
    "                     ['0', '0', '0', '0']])\n",
    "            \n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "levels_dict=create_levels_dict(df,data_type)\n",
    "target_nodes=create_node_dict_v3(adj_matrix, nn_names_matrix, data_type, min_vals, max_vals,levels_dict)\n",
    "\n",
    "for node in target_nodes:\n",
    "    print(f'-----------------{node}-------------------')\n",
    "    train_loader, _ = get_dataloader_v4(node, target_nodes, train_df, val_df, batch_size=1, verbose=False)\n",
    "    x,y =next(iter(train_loader))\n",
    "\n",
    "    print(x)\n",
    "    print(y)\n",
    "    \n",
    "    _, ordered_transformation_terms_in_h, _=ordered_parents(node, target_nodes)\n",
    "\n",
    "\n",
    "    int_input, shift_list = preprocess_inputs_v2(x, ordered_transformation_terms_in_h.values(), device=device)\n",
    "\n",
    "    # print(f'int_input {int_input}')\n",
    "    # print(f'shift_list {shift_list}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36743ab",
   "metadata": {},
   "source": [
    "# refract preprocess inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f3a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': 'continous', 'ord_bin1': 'ordinal_Xn_Yo', 'ord_multi': 'ordinal_Xn_Yo', 'ord_bin2': 'ordinal_Xn_Yo'}\n",
      "*************\n",
      " Model has Complex intercepts and Complex shifts, please add your Model to the modelzoo \n",
      "*************\n",
      "-----------------x1-------------------\n",
      "[tensor([1.])]\n",
      "tensor([-0.4081])\n",
      "int_input tensor([[1.]], device='cuda:0')\n",
      "shift_list None\n",
      "-----------------ord_bin1-------------------\n",
      "[tensor([1.])]\n",
      "tensor([[1., 0.]])\n",
      "int_input tensor([[1.]], device='cuda:0')\n",
      "shift_list None\n",
      "-----------------ord_multi-------------------\n",
      "[tensor([1.]), tensor([0.4732]), tensor([[1., 0.]])]\n",
      "tensor([[0., 0., 0., 1.]])\n",
      "int_input tensor([[1.]], device='cuda:0')\n",
      "shift_list [tensor([[0.4732, 1.0000, 0.0000]], device='cuda:0')]\n",
      "-----------------ord_bin2-------------------\n",
      "[tensor([-0.2490]), tensor([[0., 1.]]), tensor([[0., 1., 0., 0.]])]\n",
      "tensor([[0., 1.]])\n",
      "int_input tensor([[-0.2490,  0.0000,  1.0000]], device='cuda:0')\n",
      "shift_list [tensor([[0., 1., 0., 0.]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "data_type={key:value for key, value in zip(train_df.columns, ['continous']+['ordinal_Xn_Yo']+['ordinal_Xn_Yo']+['ordinal_Xn_Yo'])}\n",
    "print(data_type)\n",
    "configuration_dict=new_conf_dict(experiment_name,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)\n",
    "\n",
    "adj_matrix=np.array([['0', '0', 'cs11', 'ci11'],\n",
    "                     ['0', '0', 'cs12', 'ci12'],\n",
    "                     ['0', '0', '0', 'cs'],\n",
    "                     ['0', '0', '0', '0']])\n",
    "            \n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "levels_dict=create_levels_dict(df,data_type)\n",
    "target_nodes=create_node_dict_v3(adj_matrix, nn_names_matrix, data_type, min_vals, max_vals,levels_dict)\n",
    "\n",
    "\n",
    "def preprocess_inputs_v3(x,transformation_terms_preprocessing,intercept_indices,shift_groups_indices):\n",
    "    \"\"\"\n",
    "    Prepares model input by grouping features:\n",
    "      - Intercepts: concatenated from intercept_indices\n",
    "      - Shifts: list of concatenated tensors per group\n",
    "\n",
    "    Args:\n",
    "      x: List of input tensors matching transformation_terms_preprocessing\n",
    "      device: 'cuda' or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "      int_inputs: Tensor of shape (B, n_intercept_features)\n",
    "      shift_list: List of tensors for each shift group or None if empty\n",
    "    \"\"\"\n",
    "    # Select device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Move inputs to device\n",
    "    x = [xi.to(device, non_blocking=True) for xi in x]\n",
    "\n",
    "    # Handle case with no transformation terms (unlikely after intercept insert)\n",
    "    if not transformation_terms_preprocessing:\n",
    "        # treat single input as intercept\n",
    "        return x[0].unsqueeze(1), None\n",
    "\n",
    "    # Build intercept inputs\n",
    "    int_tensors = [x[idx] for idx in intercept_indices]\n",
    "    if not int_tensors:\n",
    "        raise ValueError(\"No intercept tensors found!\")\n",
    "    # Flatten and concatenate\n",
    "    int_inputs = torch.cat(\n",
    "        [t.view(t.shape[0], -1) for t in int_tensors], dim=1\n",
    "    )\n",
    "\n",
    "    # Build shift groups\n",
    "    shift_list = []\n",
    "    for group_idxs in shift_groups_indices:\n",
    "        tensors = [x[idx] for idx in group_idxs]\n",
    "        group_tensor = torch.cat(\n",
    "            [t.view(t.shape[0], -1) for t in tensors], dim=1\n",
    "        )\n",
    "        shift_list.append(group_tensor)\n",
    "\n",
    "    return int_inputs, (shift_list if shift_list else None)\n",
    "\n",
    "\n",
    "def intercept_shift_indexes(transformation_terms_preprocessing):\n",
    "         # ensure it's a list\n",
    "        # Always ensure there's an intercept term\n",
    "        if not any('ci' in str(value) for value in transformation_terms_preprocessing):\n",
    "            transformation_terms_preprocessing.insert(0, 'si')\n",
    "            \n",
    "        intercept_indices = [i for i, term in enumerate(transformation_terms_preprocessing)if term.startswith(('si', 'ci'))]\n",
    "\n",
    "        shift_groups_indices = []\n",
    "        current_key = None\n",
    "        for i, term in enumerate(transformation_terms_preprocessing):\n",
    "            if term.startswith(('cs', 'ls')):\n",
    "                # numbered suffix → group by the first 3 chars\n",
    "                if len(term) > 2 and term[2].isdigit():\n",
    "                    key = term[:3]\n",
    "                    if not shift_groups_indices or current_key != key:\n",
    "                        shift_groups_indices.append([i])\n",
    "                        current_key = key\n",
    "                    else:\n",
    "                        shift_groups_indices[-1].append(i)\n",
    "                else:\n",
    "                    # lone 'cs' or 'ls'\n",
    "                    shift_groups_indices.append([i])\n",
    "                    current_key = None\n",
    "        return intercept_indices,shift_groups_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for node in target_nodes:\n",
    "    print(f'-----------------{node}-------------------')\n",
    "    train_loader, _ = get_dataloader_v4(node, target_nodes, train_df, val_df, batch_size=1, verbose=False)\n",
    "    x,y =next(iter(train_loader))\n",
    "\n",
    "    print(x)\n",
    "    print(y)\n",
    "    \n",
    "    _, ordered_transformation_terms_in_h, _=ordered_parents(node, target_nodes)\n",
    "\n",
    "    transformation_terms_preprocessing = list(ordered_transformation_terms_in_h.values())\n",
    "\n",
    "    intercept_indices,shift_groups_indices =intercept_shift_indexes(transformation_terms_preprocessing)\n",
    "\n",
    "    int_input, shift_list = preprocess_inputs_v3(x,transformation_terms_preprocessing,intercept_indices,shift_groups_indices)\n",
    "\n",
    "    print(f'int_input {int_input}')\n",
    "    print(f'shift_list {shift_list}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
