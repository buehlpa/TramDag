{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32793ca7",
   "metadata": {},
   "source": [
    "## tesing dataloader V4\n",
    "\n",
    "new ideas with ordinal c and yc \n",
    "has ordinal binary and continous outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5f4e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with GPU support.\n"
     ]
    }
   ],
   "source": [
    "# Load dependencies\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from utils.graph import *\n",
    "from utils.loss_ordinal import *\n",
    "from utils.tram_model_helpers import *\n",
    "from utils.tram_models import *\n",
    "from utils.tram_data import *\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Train with GPU support.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU found, train with CPU support.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ada57",
   "metadata": {},
   "source": [
    "adjustet funcitnos for ordinal outcomes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8678f4f3",
   "metadata": {},
   "source": [
    "dev ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd61311",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"testing_v4_dataloader\"   ## <--- set experiment name\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "\n",
    "LOG_DIR=\"/home/bule/TramDag/dev_experiment_logs\"\n",
    "EXPERIMENT_DIR = os.path.join(LOG_DIR, experiment_name)\n",
    "DATA_PATH = EXPERIMENT_DIR # <----------- change to different source if needed\n",
    "CONF_DICT_PATH = os.path.join(EXPERIMENT_DIR, f\"configuration.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0caa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make  ordinal variables ned new coding since the can funciton as targets and predictors and can be encoded as contionus and categorical \n",
    "\n",
    "datatype= \"ordinal_Xc_Yc\" # as X its continous aswell as Y continous\n",
    "datatype= \"ordinal_Xc_Yn\"\n",
    "datatype= \"ordinal_Xn_Yc\"\n",
    "datatype= \"ordinal_Xn_Yn\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc1965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"x1\": np.random.normal(loc=0, scale=1, size=1000),\n",
    "    # \"x2\": np.random.uniform(low=0, high=10, size=1000),\n",
    "    \"ord_bin1\":  np.random.binomial(1, p=0.4, size=1000),\n",
    "    \"ord_multi\": np.random.choice([0, 1, 2, 3], size=1000, p=[0.2, 0.3, 0.3, 0.2]),\n",
    "    \"ord_bin2\":  np.random.binomial(1, p=0.4, size=1000),\n",
    "\n",
    "})\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "quantiles = train_df.quantile([0.05, 0.95])\n",
    "min_vals = quantiles.loc[0.05]\n",
    "max_vals = quantiles.loc[0.95]\n",
    "\n",
    "\n",
    "\n",
    "data_type={key:value for key, value in zip(train_df.columns, ['cont']*1+['ord']*2+['ord'])}\n",
    "\n",
    "configuration_dict=new_conf_dict(experiment_name,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeac2eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ord_bin1': 2, 'ord_multi': 4, 'ord_bin2': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_levels_dict(df:pd.DataFrame,data_type:dict):\n",
    "    levels_dict={}\n",
    "    for key,val in data_type.items():\n",
    "        if val=='ord':\n",
    "            check_ordinal_variable_values(df,key)\n",
    "            levels_dict[key]=len(np.unique(df[key]))\n",
    "    return levels_dict   \n",
    "\n",
    "def check_ordinal_variable_values(df, var):\n",
    "    unique_vals = set(df[var].dropna().unique())\n",
    "    num_classes = len(unique_vals)\n",
    "\n",
    "    if num_classes < 2:\n",
    "        raise ValueError(\n",
    "            f\"Variable '{var}' has fewer than 2 unique values: {unique_vals}. \"\n",
    "            \"Ordinal variables must have at least two distinct values.\"\n",
    "        )\n",
    "\n",
    "    if num_classes == 2:\n",
    "        if unique_vals != {0, 1}:\n",
    "            raise ValueError(\n",
    "                f\"Variable '{var}' is marked as ordinal with 2 classes, \"\n",
    "                f\"but values are {unique_vals}. Please provide binary ordinal variables as 0 and 1.\"\n",
    "            )\n",
    "    else:\n",
    "        if not all(isinstance(val, (int, np.integer)) for val in unique_vals):\n",
    "            raise ValueError(\n",
    "                f\"Variable '{var}' contains non-integer values: {unique_vals}. \"\n",
    "                \"Multiclass ordinal variables must contain only integer values.\"\n",
    "            )\n",
    "        expected_vals = set(range(num_classes))\n",
    "        if unique_vals != expected_vals:\n",
    "            raise ValueError(\n",
    "                f\"Variable '{var}' has values {sorted(unique_vals)}, \"\n",
    "                f\"but expected values are {sorted(expected_vals)} (0 to {num_classes - 1}). \"\n",
    "                \"Multiclass ordinal variables must be zero-indexed and contiguous.\"\n",
    "            )\n",
    "\n",
    "\n",
    "levels_dict=create_levels_dict(df,data_type)\n",
    "levels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649d5754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x1': {'Modelnr': 0,\n",
       "  'data_type': 'cont',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': -1.5316123093173848,\n",
       "  'max': 1.6860177542255808,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'ord_bin1': {'Modelnr': 1,\n",
       "  'data_type': 'ord',\n",
       "  'levels': 2,\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 0.0,\n",
       "  'max': 1.0,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'ord_multi': {'Modelnr': 2,\n",
       "  'data_type': 'ord',\n",
       "  'levels': 4,\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 0.0,\n",
       "  'max': 3.0,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'ord_bin2': {'Modelnr': 3,\n",
       "  'data_type': 'ord',\n",
       "  'levels': 2,\n",
       "  'node_type': 'sink',\n",
       "  'parents': ['x1', 'ord_bin1', 'ord_multi'],\n",
       "  'parents_datatype': {'x1': 'cont', 'ord_bin1': 'ord', 'ord_multi': 'ord'},\n",
       "  'transformation_terms_in_h()': {'x1': 'ls',\n",
       "   'ord_bin1': 'ls',\n",
       "   'ord_multi': 'ls'},\n",
       "  'min': 0.0,\n",
       "  'max': 1.0,\n",
       "  'transformation_term_nn_models_in_h()': {'x1': 'LinearShift',\n",
       "   'ord_bin1': 'LinearShift',\n",
       "   'ord_multi': 'LinearShift'}}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = list(data_type.keys())\n",
    "adj_matrix = np.full((len(columns), len(columns)), \"0\", dtype=object)\n",
    "\n",
    "for i in range(len(columns)-1):\n",
    "    adj_matrix[i, -1] = \"ls\"\n",
    "            \n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "\n",
    "\n",
    "target_nodes=create_node_dict_v2(adj_matrix, nn_names_matrix, data_type, min_vals, max_vals,levels_dict)\n",
    "target_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8304b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mGenericDataset_v4\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      4\u001b[0m         df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m         transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     ):\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m        df: pd.DataFrame\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m        target_col: str\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m        transformation_terms_in_h: dict for intercept logic\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class GenericDataset_v4(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        target_col,\n",
    "        target_nodes=None,\n",
    "        parents_dataype_dict=None,\n",
    "        transformation_terms_in_h=None,\n",
    "        transform=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        df: pd.DataFrame\n",
    "        target_col: str\n",
    "        target_nodes: dict mapping each node → metadata (including 'data_type')\n",
    "        parents_dataype_dict: dict var_name → \"cont\"|\"ord\"|\"other\"\n",
    "        transform: torchvision transform for images\n",
    "        transformation_terms_in_h: dict for intercept logic\n",
    "        \"\"\"\n",
    "                \n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.target_col = target_col\n",
    "        self.target_nodes = target_nodes or {}\n",
    "        self.parents_dataype_dict = parents_dataype_dict or {}\n",
    "        self.variables = list(self.parents_dataype_dict.keys())\n",
    "        self.transform = transform\n",
    "        self.transformation_terms_in_h = transformation_terms_in_h or {}\n",
    "        \n",
    "        self.target_is_source= True if target_nodes[self.target_col].get('node_type').lower() == \"source\" else False\n",
    "        self.h_needs_simple_intercept=True if all('i' not in str(v) for v in self.transformation_terms_in_h.values()) else False\n",
    "        self.ordinal_num_classes ={\n",
    "                        var: self.df[var].nunique() for var in self.variables\n",
    "                        if \"ordinal\" in self.parents_dataype_dict[var].lower() and \"Xn\" in self.parents_dataype_dict[var].lower()\n",
    "                        }\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.target_data_type=self.target_nodes[self.target_col].get('data_type').lower()\n",
    "        self.target_num_classes=self.target_nodes[self.target_col].get('levels') or None # should be none anywasy if levels not exits\n",
    "\n",
    "        #checks\n",
    "        self._check_multiclass_variables_of_df()\n",
    "        \n",
    "    def _transform_y(self,row):\n",
    "        #returns continous or onehot encoded target\n",
    "        if  self.target_data_type==\"continous\" or \"Yc\".lower() in self.target_data_type:\n",
    "                y = torch.tensor(row[self.target_col], dtype=torch.float32)\n",
    "                return y\n",
    "        elif self.target_num_classes is not None:\n",
    "                        raw = row[self.target_col]\n",
    "                        y_int = int(raw)\n",
    "                        y = F.one_hot(torch.tensor(y_int, dtype=torch.long), num_classes=self.target_num_classes).float()\n",
    "                        return y    \n",
    "\n",
    "        \n",
    "    def _check_multiclass_variables_of_df(self):\n",
    "        # checks whether the predictors here called varibles are ordinal and nominally modelled \n",
    "        for var in self.variables:\n",
    "            dtype = self.parents_dataype_dict[var]\n",
    "            if \"ordinal\" not in dtype:\n",
    "                continue\n",
    "            \n",
    "            elif \"Xn\".lower() in dtype.lower():\n",
    "                unique_vals = set(self.df[var].dropna().unique())\n",
    "                num_classes = len(unique_vals)\n",
    "\n",
    "                if not all(isinstance(val, (int, np.integer)) for val in unique_vals):\n",
    "                    raise ValueError(\n",
    "                        f\"Variable '{var}' contains non-integer values: {unique_vals}. \"\n",
    "                        \"Multiclass ordinal variables must contain only integer values.\"\n",
    "                    )\n",
    "\n",
    "                expected_vals = set(range(num_classes))\n",
    "                if unique_vals != expected_vals:\n",
    "                    raise ValueError(\n",
    "                        f\"Variable '{var}' has values {sorted(unique_vals)}, \"\n",
    "                        f\"but expected values are {sorted(expected_vals)} (0 to {num_classes - 1}). \"\n",
    "                        \"Multiclass ordinal variables must be zero-indexed and contiguous.\"\n",
    "                )\n",
    "            else:\n",
    "                continue\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # ---------------------------------------- SOURCE NODE: no parents → x = [1.0] ---\n",
    "        if  self.target_is_source:\n",
    "            x_data = [torch.tensor(1.0)]\n",
    "            y=self._transform_y(row)\n",
    "            return tuple(x_data), y\n",
    "\n",
    "        # ---------------------------------------- if not source prepare data  ---\n",
    "        x_data = []\n",
    "\n",
    "        # --- SIMPLE INTERCEPT if needed  first term in x is x = [1.0]---\n",
    "        if self.h_needs_simple_intercept:\n",
    "            x_data.append(torch.tensor(1.0))\n",
    "\n",
    "        # --- BUILD FEATURES ---\n",
    "        for var in self.variables:\n",
    "            dtype = self.parents_dataype_dict[var].lower()\n",
    "            ## Continous  feature\n",
    "            if dtype == \"continous\" or \"Xc\".lower() in dtype:\n",
    "                x_data.append(torch.tensor(row[var], dtype=torch.float32))\n",
    "                \n",
    "            ## Ordinal feature , if it has more thatn 2 classes it uses onehotencodig, if binary use just 0 and 1\n",
    "            elif \"ordinal\" in dtype and \"Xn\".lower() in dtype:\n",
    "                x_ord = int(row[var])\n",
    "                var_num_classes = self.ordinal_num_classes[var]\n",
    "                x_ord_onehot = F.one_hot(torch.tensor(x_ord, dtype=torch.long),num_classes=var_num_classes).float()\n",
    "                x_data.append(x_ord_onehot)\n",
    "\n",
    "            else:  # \"other\"\n",
    "                img = Image.open(row[var]).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                x_data.append(img)\n",
    "\n",
    "        # --- BUILD TARGET ---\n",
    "        y=self._transform_y(row)\n",
    "            \n",
    "            \n",
    "        return tuple(x_data), y\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloader_v4(node, target_nodes, train_df, val_df, batch_size=32, verbose=False):\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    parents_dataype_dict, transformation_terms_in_h, _ = ordered_parents(node, target_nodes)\n",
    "    if verbose:\n",
    "        print(f\"Parents dtype: {parents_dataype_dict}\")\n",
    "    train_ds = GenericDataset_v4(train_df,target_col=node,target_nodes=target_nodes,parents_dataype_dict=parents_dataype_dict,transform=transform,transformation_terms_in_h=transformation_terms_in_h)\n",
    "    val_ds = GenericDataset_v4(val_df,target_col=node,target_nodes=target_nodes,parents_dataype_dict=parents_dataype_dict,transform=transform,transformation_terms_in_h=transformation_terms_in_h)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8009c55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parents dtype: OrderedDict([('x1', 'cont'), ('ord_bin1', 'ord'), ('ord_multi', 'ord')])\n",
      "[tensor([1.]), tensor([-0.0634]), tensor([1]), tensor([[0., 0., 0., 1.]])]\n",
      "tensor([[1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "node='ord_bin2'\n",
    "\n",
    "train_loader, val_loader = get_dataloader_v4(node, target_nodes, train_df, val_df, batch_size=1, verbose=True)\n",
    "\n",
    "x,y =next(iter(train_loader))\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c646b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ordered_transformation_terms_in_h, _=ordered_parents(node, target_nodes)\n",
    "int_input, shift_list = preprocess_inputs(x, ordered_transformation_terms_in_h.values(), device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d858d62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c80a7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.4501]], device='cuda:0'),\n",
       " tensor([[1]], device='cuda:0'),\n",
       " tensor([[[0., 0., 1., 0.]]], device='cuda:0')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e7b746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for multiclass extend input fdeatures of the nn models to n classes\n",
    "\n",
    "\n",
    "def get_fully_specified_tram_model_v3(node: str, target_nodes: dict, verbose=True):\n",
    "    # Source nodes get a simple intercept only\n",
    "    if target_nodes[node]['node_type'] == 'source':\n",
    "        # if target node is ordinal we only need c-1 thetas\n",
    "        if target_nodes[node]['data_type']=='ord':\n",
    "            nn_int = SimpleIntercept(n_thetas=target_nodes[node]['levels']-1)\n",
    "        else:    \n",
    "            nn_int = SimpleIntercept()\n",
    "            \n",
    "        model = TramModel(nn_int, None)\n",
    "        if verbose:\n",
    "            print(\"Source → SimpleIntercept only\")\n",
    "        return model\n",
    "\n",
    "    # Otherwise gather terms and model names\n",
    "    _, terms_dict, model_names_dict = ordered_parents(node, target_nodes)\n",
    "    model_dict = merge_transformation_dicts(terms_dict, model_names_dict)\n",
    "\n",
    "    # Split intercepts vs. shifts\n",
    "    intercepts_dict = {\n",
    "        k: v for k, v in model_dict.items()\n",
    "        if \"ci\" in v['h_term'] or \"si\" in v['h_term']\n",
    "    }\n",
    "    shifts_dict = {\n",
    "        k: v for k, v in model_dict.items()\n",
    "        if \"ci\" not in v['h_term'] and \"si\" not in v['h_term']\n",
    "    }\n",
    "\n",
    "    ############################## INTERCEPT NETWORKS #####################################################\n",
    "    intercept_groups = group_by_base(intercepts_dict, prefixes=(\"ci\", \"si\"))\n",
    "    if not intercept_groups:\n",
    "        if target_nodes[node]['data_type']=='ord':\n",
    "            nn_int = SimpleIntercept(n_thetas=int(target_nodes[node]['levels'])-1)\n",
    "        else:    \n",
    "            nn_int = SimpleIntercept()\n",
    "    else:\n",
    "        if len(intercept_groups) > 1:\n",
    "            raise ValueError(\"Multiple intercept models detected; only one is supported.\")\n",
    "        feats = next(iter(intercept_groups.values()))\n",
    "        cls_name = feats[0][1]['class_name']\n",
    "        base = get_base_model_class(cls_name)\n",
    "        \n",
    "        if target_nodes[node]['data_type']=='ord':\n",
    "            nn_int = globals()[base](n_features=len(feats),n_thetas=int(target_nodes[node]['levels'])-1)\n",
    "            \n",
    "        else:    \n",
    "            nn_int = globals()[base](n_features=len(feats))\n",
    "        \n",
    "    ############################### SHIFT NETWORKS ########################################################\n",
    "    shift_groups = group_by_base(shifts_dict, prefixes=(\"cs\", \"ls\"))\n",
    "    nn_shifts = []\n",
    "    for feats in shift_groups.values():\n",
    "        cls_name = feats[0][1]['class_name']\n",
    "        base = get_base_model_class(cls_name)\n",
    "        nn_shifts.append(globals()[base](n_features=len(feats)))\n",
    "\n",
    "    # Combine into TramModel\n",
    "    tram_model = TramModel(nn_int, nn_shifts)\n",
    "    if verbose:\n",
    "        print(\"Constructed TRAM model:\", tram_model)\n",
    "    return tram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08fa9aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x1': {'Modelnr': 0,\n",
       "  'data_type': 'cont',\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': -1.523303818090823,\n",
       "  'max': 1.7558465123350986,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'ord_bin1': {'Modelnr': 1,\n",
       "  'data_type': 'ord',\n",
       "  'levels': 2,\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 0.0,\n",
       "  'max': 1.0,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'ord_multi': {'Modelnr': 2,\n",
       "  'data_type': 'ord',\n",
       "  'levels': 4,\n",
       "  'node_type': 'source',\n",
       "  'parents': [],\n",
       "  'parents_datatype': {},\n",
       "  'transformation_terms_in_h()': {},\n",
       "  'min': 0.0,\n",
       "  'max': 3.0,\n",
       "  'transformation_term_nn_models_in_h()': {}},\n",
       " 'ord_bin2': {'Modelnr': 3,\n",
       "  'data_type': 'ord',\n",
       "  'levels': 2,\n",
       "  'node_type': 'sink',\n",
       "  'parents': ['x1', 'ord_bin1', 'ord_multi'],\n",
       "  'parents_datatype': {'x1': 'cont', 'ord_bin1': 'ord', 'ord_multi': 'ord'},\n",
       "  'transformation_terms_in_h()': {'x1': 'ls',\n",
       "   'ord_bin1': 'ls',\n",
       "   'ord_multi': 'ls'},\n",
       "  'min': 0.0,\n",
       "  'max': 1.0,\n",
       "  'transformation_term_nn_models_in_h()': {'x1': 'LinearShift',\n",
       "   'ord_bin1': 'LinearShift',\n",
       "   'ord_multi': 'LinearShift'}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e82d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'x1': [('x1', {'h_term': 'ls', 'class_name': 'LinearShift'})], 'ord_bin1': [('ord_bin1', {'h_term': 'ls', 'class_name': 'LinearShift'})], 'ord_multi': [('ord_multi', {'h_term': 'ls', 'class_name': 'LinearShift'})]})\n",
      "[('x1', {'h_term': 'ls', 'class_name': 'LinearShift'})]\n",
      "[('ord_bin1', {'h_term': 'ls', 'class_name': 'LinearShift'})]\n",
      "[('ord_multi', {'h_term': 'ls', 'class_name': 'LinearShift'})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "node='ord_bin2'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Otherwise gather terms and model names\n",
    "_, terms_dict, model_names_dict = ordered_parents(node, target_nodes)\n",
    "model_dict = merge_transformation_dicts(terms_dict, model_names_dict)\n",
    "\n",
    "# Split intercepts vs. shifts\n",
    "intercepts_dict = {\n",
    "    k: v for k, v in model_dict.items()\n",
    "    if \"ci\" in v['h_term'] or \"si\" in v['h_term']\n",
    "}\n",
    "shifts_dict = {\n",
    "    k: v for k, v in model_dict.items()\n",
    "    if \"ci\" not in v['h_term'] and \"si\" not in v['h_term']\n",
    "}\n",
    "\n",
    "############################## INTERCEPT NETWORKS #####################################################\n",
    "intercept_groups = group_by_base(intercepts_dict, prefixes=(\"ci\", \"si\"))\n",
    "if not intercept_groups:\n",
    "    if target_nodes[node]['data_type']=='ord':\n",
    "        nn_int = SimpleIntercept(n_thetas=int(target_nodes[node]['levels'])-1)\n",
    "    else:    \n",
    "        nn_int = SimpleIntercept()\n",
    "\n",
    "else:\n",
    "    if len(intercept_groups) > 1:\n",
    "        raise ValueError(\"Multiple intercept models detected; only one is supported.\")\n",
    "    \n",
    "    \n",
    "    feats = next(iter(intercept_groups.values()))\n",
    "    \n",
    "    cls_name = feats[0][1]['class_name']\n",
    "    base = get_base_model_class(cls_name)\n",
    "    \n",
    "    if target_nodes[node]['data_type']=='ord':\n",
    "        \n",
    "        \n",
    "        nn_int = globals()[base](n_features=len(feats),n_thetas=int(target_nodes[node]['levels'])-1)\n",
    "        \n",
    "    else:    \n",
    "        nn_int = globals()[base](n_features=len(feats))\n",
    "    \n",
    "\n",
    "############################### SHIFT NETWORKS ########################################################\n",
    "shift_groups = group_by_base(shifts_dict, prefixes=(\"cs\", \"ls\"))\n",
    "print(shift_groups)\n",
    "nn_shifts = []\n",
    "for feats in shift_groups.values():\n",
    "    print(feats)\n",
    "\n",
    "    cls_name = feats[0][1]['class_name']\n",
    "    base = get_base_model_class(cls_name)\n",
    "    nn_shifts.append(globals()[base](n_features=len(feats)))\n",
    "\n",
    "# Combine into TramModel\n",
    "tram_model = TramModel(nn_int, nn_shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a04819a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TramModel(\n",
       "  (nn_int): SimpleIntercept(\n",
       "    (fc): Linear(in_features=1, out_features=1, bias=False)\n",
       "  )\n",
       "  (nn_shift): ModuleList(\n",
       "    (0-2): 3 x LinearShift(\n",
       "      (fc): Linear(in_features=1, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5824e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels_dict={'ord_multi':len(np.unique(df['ord_multi'])),'ord_bin':len(np.unique(df['ord_bin']))}  \n",
    "\n",
    "columns = list(data_type.keys())\n",
    "adj_matrix = np.full((len(columns), len(columns)), \"0\", dtype=object)\n",
    "\n",
    "# Set last column (edges *to* 'target') as \"ls\", excluding self-loop\n",
    "for i in range(len(columns)):\n",
    "    for j in range(len(columns)):\n",
    "        if j > i :\n",
    "            adj_matrix[i, j] = \"ls\"\n",
    "    \n",
    "    \n",
    "nn_names_matrix= create_nn_model_names(adj_matrix,data_type)\n",
    "nn_names_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b936579",
   "metadata": {},
   "outputs": [],
   "source": [
    "node='target'\n",
    "tram_model=get_fully_specified_tram_model_v2('target', target_nodes, verbose=True)\n",
    "tram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e48da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs =1000\n",
    "train_val_loop_v3(\n",
    "            node,\n",
    "            target_nodes,\n",
    "            NODE_DIR,\n",
    "            tram_model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            epochs,\n",
    "            optimizer,\n",
    "            use_scheduler=False,\n",
    "            scheduler=False,\n",
    "            save_linear_shifts=False,\n",
    "            verbose=1,\n",
    "            device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a57f405",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def7e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "## laoding the best model \n",
    "MODEL_PATH,LAST_MODEL_PATH,TRAIN_HIST_PATH,VAL_HIST_PATH=model_train_val_paths(NODE_DIR)\n",
    "\n",
    "if os.path.exists(MODEL_PATH) and os.path.exists(TRAIN_HIST_PATH) and os.path.exists(VAL_HIST_PATH):\n",
    "    print(\"Existing model found. Loading weights and history...\")\n",
    "    tram_model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate on testdata\n",
    "tram_model.eval()\n",
    "\n",
    "_, ordered_transformation_terms_in_h, _=ordered_parents(node, target_nodes)\n",
    "\n",
    "min_vals = torch.tensor(target_nodes[node]['min'], dtype=torch.float32).to(device)\n",
    "max_vals = torch.tensor(target_nodes[node]['max'], dtype=torch.float32).to(device)\n",
    "min_max = torch.stack([min_vals, max_vals], dim=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        y = y.to(device)\n",
    "        \n",
    "        int_input, shift_list = preprocess_inputs(x, ordered_transformation_terms_in_h.values(), device=device)\n",
    "        y_pred = tram_model(int_input=int_input, shift_input=shift_list)\n",
    "        # loss = contram_nll(y_pred, y, min_max=min_max)\n",
    "        pred_labels = get_pdf_ordinal(get_cdf_ordinal(y_pred)).argmax(dim=1)\n",
    "        true_labels = y.argmax(dim=1)\n",
    "        accuracy = (pred_labels == true_labels).float().mean().item()\n",
    "\n",
    "        print(f\"Accuracy: {accuracy*100:.1f}%\")  # → 100.0%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
