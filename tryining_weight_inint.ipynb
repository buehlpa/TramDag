{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a77f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "@torch.no_grad()\n",
    "def init_last_layer_increasing(module: nn.Module, start: float = -2.0, end: float = 2.0):\n",
    "    \"\"\"\n",
    "    Initialize the weights of the last nn.Linear in `module` so that the outputs\n",
    "    are linearly increasing between [start, end].\n",
    "\n",
    "    Assumes the last layer is nn.Linear with shape (n_thetas, in_features).\n",
    "    \"\"\"\n",
    "    # Find last Linear layer\n",
    "    last_linear = None\n",
    "    for m in reversed(list(module.modules())):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            last_linear = m\n",
    "            break\n",
    "    if last_linear is None:\n",
    "        raise ValueError(\"No nn.Linear layer found in module.\")\n",
    "\n",
    "    n_thetas = last_linear.out_features\n",
    "    in_features = last_linear.in_features\n",
    "\n",
    "    # Desired increasing values\n",
    "    values = torch.linspace(start, end, steps=n_thetas)\n",
    "\n",
    "    # If in_features > 1, just repeat values / distribute across input dims\n",
    "    w = torch.zeros((n_thetas, in_features))\n",
    "    w[:, 0] = values  # put increasing sequence in first input channel\n",
    "\n",
    "    last_linear.weight.copy_(w)\n",
    "\n",
    "    if last_linear.bias is not None:\n",
    "        last_linear.bias.zero_()  # optional: reset bias to 0\n",
    "\n",
    "    return last_linear\n",
    "\n",
    "\n",
    "class SimpleIntercept(nn.Module):\n",
    "    \"\"\"\n",
    "    Intercept term , hI()\n",
    "    Attributes:\n",
    "        n_thetas (int): how many output thetas, for ordinal target this is the number of classes - 1, thetas are order of bernsteinpol() in continous case\n",
    "    \"\"\"\n",
    "    def __init__(self, n_thetas=20):\n",
    "        super(SimpleIntercept, self).__init__()  \n",
    "        self.fc = nn.Linear(1,n_thetas, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0e432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([-3.0000, -2.6842, -2.3684, -2.0526, -1.7368, -1.4211, -1.1053, -0.7895,\n",
      "        -0.4737, -0.1579,  0.1579,  0.4737,  0.7895,  1.1053,  1.4211,  1.7368,\n",
      "         2.0526,  2.3684,  2.6842,  3.0000], grad_fn=<SqueezeBackward0>)\n",
      "Output : tensor([-3.0000, -2.6842, -2.3684, -2.0526, -1.7368, -1.4211, -1.1053, -0.7895,\n",
      "        -0.4737, -0.1579,  0.1579,  0.4737,  0.7895,  1.1053,  1.4211,  1.7368,\n",
      "         2.0526,  2.3684,  2.6842,  3.0000], grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "simple = SimpleIntercept(n_thetas=20)\n",
    "\n",
    "last_layer = init_last_layer_increasing(simple, start=-3.0, end=3.0)\n",
    "\n",
    "x = torch.ones(1, 1)\n",
    "out = simple(x)\n",
    "\n",
    "print(\"Weights:\", last_layer.weight.squeeze())\n",
    "print(\"Output :\", out.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d680e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexInterceptDefaultTabular(nn.Module):\n",
    "    \"\"\"\n",
    "    Complex shift term for tabular data. Can be any neural network architecture\n",
    "    Attributes:\n",
    "        n_thetas (int): number of features/predictors\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features=1,n_thetas=20):\n",
    "        super(ComplexInterceptDefaultTabular, self).__init__()\n",
    "        # Define the layers\n",
    "        self.fc1 = nn.Linear(n_features, 8)  # First hidden layer (X_i -> 8)\n",
    "        self.relu1 = nn.ReLU()               # ReLU activation\n",
    "        self.fc2 = nn.Linear(8, 8)           # Second hidden layer (8 -> 8)\n",
    "        self.relu2 = nn.ReLU()               # ReLU activation\n",
    "        self.fc3 = nn.Linear(8, n_thetas, bias=False)  # Output layer (8 -> n_thetas, no bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "938a512d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([-1.0000, -0.8947, -0.7895, -0.6842, -0.5789, -0.4737, -0.3684, -0.2632,\n",
      "        -0.1579, -0.0526,  0.0526,  0.1579,  0.2632,  0.3684,  0.4737,  0.5789,\n",
      "         0.6842,  0.7895,  0.8947,  1.0000], grad_fn=<SqueezeBackward0>)\n",
      "Output : tensor([ 0.0335,  0.0188, -0.0706,  0.0505, -0.0428, -0.0543,  0.0060,  0.0718,\n",
      "        -0.0494,  0.0033, -0.0310, -0.0633,  0.0071,  0.0415,  0.0021, -0.0073,\n",
      "        -0.0649,  0.0008,  0.0810, -0.0247], grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "complex = ComplexInterceptDefaultTabular(n_thetas=20)\n",
    "\n",
    "last_layer = init_last_layer_increasing(simple, start=-1.0, end=1.0)\n",
    "\n",
    "x = torch.ones(1, 1)\n",
    "out = complex(x)\n",
    "\n",
    "print(\"Weights:\", last_layer.weight.squeeze())\n",
    "print(\"Output :\", out.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61921964",
   "metadata": {},
   "source": [
    "## inverser transform of thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dd01f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_intercepts_continous(theta_tilde:torch.Tensor) -> torch.Tensor:\n",
    "    \n",
    "    \"\"\"\n",
    "    Transforms the unordered theta_tilde to ordered theta values for the bernstein polynomial\n",
    "    E.G: \n",
    "    theta_1 = theta_tilde_1\n",
    "    theta_2 = theta_tilde_1 + exp(theta_tilde_2)\n",
    "    ..\n",
    "    :param theta_tilde: The unordered theta_tilde values\n",
    "    :return: The ordered theta values\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the shift based on the last dimension size\n",
    "    last_dim_size = theta_tilde.shape[-1]\n",
    "    shift = torch.log(torch.tensor(2.0)) * last_dim_size / 2\n",
    "\n",
    "    # Get the width values by applying softplus from the second position onward\n",
    "    widths = torch.nn.functional.softplus(theta_tilde[..., 1:])\n",
    "\n",
    "    # Concatenate the first value (raw) with the softplus-transformed widths\n",
    "    widths = torch.cat([theta_tilde[..., [0]], widths], dim=-1)\n",
    "\n",
    "    # Return the cumulative sum minus the shift\n",
    "    return torch.cumsum(widths, dim=-1) - shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1799d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inverse_transform_intercepts_continous(theta: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Inverse of transform_intercepts_continous.\n",
    "    Recovers theta_tilde from theta with numerical stability.\n",
    "    Uses float64 internally, but returns the same dtype as input.\n",
    "    \"\"\"\n",
    "    # Store original dtype and cast to float64 for precision\n",
    "    orig_dtype = theta.dtype\n",
    "    theta = theta.to(torch.float64)\n",
    "\n",
    "    last_dim_size = theta.shape[-1]\n",
    "    shift = torch.log(torch.tensor(2.0, dtype=torch.float64)) * last_dim_size / 2\n",
    "\n",
    "    # Undo shift\n",
    "    theta_shifted = theta + shift\n",
    "\n",
    "    # Recover widths\n",
    "    widths = torch.empty_like(theta_shifted)\n",
    "    widths[..., 0] = theta_shifted[..., 0]\n",
    "    widths[..., 1:] = theta_shifted[..., 1:] - theta_shifted[..., :-1]\n",
    "\n",
    "    # Invert softplus (with clamp for numerical stability)\n",
    "    theta_tilde = torch.empty_like(theta_shifted)\n",
    "    theta_tilde[..., 0] = widths[..., 0]\n",
    "    theta_tilde[..., 1:] = torch.log(torch.expm1(torch.clamp(widths[..., 1:], min=eps)))\n",
    "\n",
    "    return theta_tilde.to(orig_dtype)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6063a5f5",
   "metadata": {},
   "source": [
    " thetas from COLR MODEL in R:\n",
    "\n",
    "```\n",
    "library(tram)\n",
    "weight_init <- read_csv(\"weight_init.csv\")\n",
    "weight_init$ones <- rep(1, nrow(weight_init))\n",
    "colr=Colr(x1~ones, data = weight_init,order=19)\n",
    "colr$weights\n",
    "colr$theta\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "thetas = torch.tensor([\n",
    "    -2.03300268, -1.43547929, -1.14005127, -1.14005125, -0.13044674,\n",
    "     0.04618960,  0.04618962,  0.04618963,  0.16484659,  0.16484661,\n",
    "     0.16484662,  0.16484664,  0.16484665,  0.16484667,  0.16484668,\n",
    "     0.16484670,  0.16484671,  0.40602064,  1.31082429,  2.38310636\n",
    "], dtype=torch.float64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57e8eacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovered theta_tilde:\n",
      " tensor([  4.8985,  -0.2014,  -1.0680, -17.7275,   0.5565,  -1.6440, -17.7275,\n",
      "        -18.4207,  -2.0716, -17.7275, -18.4207, -17.7275, -18.4207, -17.7275,\n",
      "        -18.4207, -17.7275, -18.4207,  -1.2992,   0.3862,   0.6534],\n",
      "       dtype=torch.float64)\n",
      "Recovered thetas_recovered:\n",
      " tensor([-2.0330, -1.4355, -1.1401, -1.1401, -0.1304,  0.0462,  0.0462,  0.0462,\n",
      "         0.1648,  0.1648,  0.1648,  0.1648,  0.1648,  0.1648,  0.1648,  0.1648,\n",
      "         0.1648,  0.4060,  1.3108,  2.3831], dtype=torch.float64)\n",
      "Round-trip error: tensor(1.9047e-08, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "thetas = torch.tensor([\n",
    "    -2.03300268, -1.43547929, -1.14005127, -1.14005125, -0.13044674,\n",
    "     0.04618960,  0.04618962,  0.04618963,  0.16484659,  0.16484661,\n",
    "     0.16484662,  0.16484664,  0.16484665,  0.16484667,  0.16484668,\n",
    "     0.16484670,  0.16484671,  0.40602064,  1.31082429,  2.38310636\n",
    "], dtype=torch.float64)\n",
    "\n",
    "theta_tilde_recovered = inverse_transform_intercepts_continous(thetas)\n",
    "\n",
    "\n",
    "thetas_recovered = transform_intercepts_continous(theta_tilde_recovered)\n",
    "\n",
    "print(\"Recovered theta_tilde:\\n\", theta_tilde_recovered)\n",
    "\n",
    "print(\"Recovered thetas_recovered:\\n\", thetas_recovered)\n",
    "\n",
    "\n",
    "print(\"Round-trip error:\", torch.max(torch.abs(thetas - thetas_recovered)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
