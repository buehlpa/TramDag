{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e076b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with GPU support.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from scipy.stats import logistic\n",
    "# from scipy.special import logit\n",
    "\n",
    "import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import Dataset\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Train with GPU support.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU found, train with CPU support.\")\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# own utils\n",
    "from utils.graph import *\n",
    "from utils.tram_models import *\n",
    "from utils.tram_model_helpers import *\n",
    "from utils.tram_data import *\n",
    "from utils.loss_continous import *\n",
    "from utils.sampling_tram_data import *\n",
    "\n",
    "\n",
    "\n",
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display, clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923b899",
   "metadata": {},
   "source": [
    "# 1. Experiments and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b984f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing configuration from /home/bule/TramDag/dev_experiment_logs/ordinal_dev/configuration.json\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"ordinal_dev\"   ## <--- set experiment name\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "\n",
    "LOG_DIR=\"/home/bule/TramDag/dev_experiment_logs\"\n",
    "EXPERIMENT_DIR = os.path.join(LOG_DIR, experiment_name)\n",
    "DATA_PATH = EXPERIMENT_DIR # <----------- change to different source if needed\n",
    "CONF_DICT_PATH = os.path.join(EXPERIMENT_DIR, f\"configuration.json\")\n",
    "\n",
    "os.makedirs(EXPERIMENT_DIR,exist_ok=True)\n",
    "# check if configration dict already exists if not create:\n",
    "\n",
    "if os.path.exists(CONF_DICT_PATH):\n",
    "    configuration_dict=load_configuration_dict(CONF_DICT_PATH)\n",
    "    print(f\"Loaded existing configuration from {CONF_DICT_PATH}\")\n",
    "else:\n",
    "    configuration_dict=create_and_write_new_configuration_dict(experiment_name,CONF_DICT_PATH,EXPERIMENT_DIR,DATA_PATH,LOG_DIR)\n",
    "    print(f\"Created new configuration file at {CONF_DICT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76549b97",
   "metadata": {},
   "source": [
    "# 2.  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1cc1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from /home/bule/TramDag/dev_experiment_logs/ordinal_dev/ordinal_dev.csv\n",
      "Configuration updated successfully.\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "def dgp_2vars_blob(n_obs=10000, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    # generate a 2D blob cluster\n",
    "    X, _ = make_blobs(n_samples=n_obs,\n",
    "                      centers=[(0, 0)],\n",
    "                      cluster_std=1.0,\n",
    "                      random_state=seed)\n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "\n",
    "    # logistic regression parameters\n",
    "    w = np.array([1.5, -1.0])    # weights for x1, x2\n",
    "    b = 0.2                      # intercept\n",
    "\n",
    "    # linear score and probability\n",
    "    linear_score = w[0] * x1 + w[1] * x2 + b\n",
    "    p = expit(linear_score)\n",
    "\n",
    "    # binary labels (but treated as ordinal)\n",
    "    y_ord = (p > 0.5).astype(int)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'x1': x1,\n",
    "        'x2': x2,\n",
    "        'x3': y_ord\n",
    "    })\n",
    "\n",
    "\n",
    "EXP_DATA_PATH=os.path.join(DATA_PATH, f\"{experiment_name}.csv\")\n",
    "if not os.path.exists(EXP_DATA_PATH):\n",
    "    df = dgp_2vars_blob()\n",
    "\n",
    "    print(df.head())\n",
    "    df.to_csv(EXP_DATA_PATH, index=False)\n",
    "else:\n",
    "    df = pd.read_csv(EXP_DATA_PATH)\n",
    "    print(f\"Loaded data from {EXP_DATA_PATH}\")\n",
    "\n",
    "data_type = {\n",
    "    'x1': 'cont',\n",
    "    'x2': 'cont',\n",
    "    'y_ord': 'ord'   # binary outcome labeled as ordinal\n",
    "}\n",
    "\n",
    "write_data_type_to_configuration(data_type, CONF_DICT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e5a0fb",
   "metadata": {},
   "source": [
    "## 2.1 train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f2733e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8000, Validation size: 1000, Test size: 1000\n"
     ]
    }
   ],
   "source": [
    "# 1. Split the data\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# 2. Compute quantiles from training data\n",
    "quantiles = train_df.quantile([0.05, 0.95])\n",
    "min_vals = quantiles.loc[0.05]\n",
    "max_vals = quantiles.loc[0.95]\n",
    "\n",
    "# 3. Normalize all sets using training quantiles\n",
    "def normalize_with_quantiles(df, min_vals, max_vals):\n",
    "    return (df - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "# train_df = normalize_with_quantiles(train_df, min_vals, max_vals)\n",
    "# val_df = normalize_with_quantiles(val_df, min_vals, max_vals)\n",
    "# test_df = normalize_with_quantiles(test_df, min_vals, max_vals)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b97f69",
   "metadata": {},
   "source": [
    "# 3. Define graph Structure\n",
    "\n",
    "- define graph and which shift and intercept terms to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fedcd902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matrix found. Please fill out the DAG and click 'Generate'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3054bb7e172f40b5b6642414a4b9fa9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=\"Fill in the adjacency matrix (upper triangle only). Use 'ls', 'cs', etc.\"), GridBo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_adj_matrix(CONF_DICT_PATH,seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7328e1",
   "metadata": {},
   "source": [
    "# 4. Configuration for the Models\n",
    "\n",
    "- all SI and LS model are generated outmatically since these are shallow NN's\n",
    "- CI and CS have to be defined by the User and can be Passed for each model, -> generate default networks which are generated automaitcally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314149ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_nn_names_matrix(CONF_DICT_PATH, seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_nodes_information_to_configuration(CONF_DICT_PATH, min_vals, max_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2799f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read configuration dict\n",
    "configuration_dict = load_configuration_dict(CONF_DICT_PATH)\n",
    "configuration_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_TRAINING=True\n",
    "train_list=['x1','x2','x3','x4','x5','x6','x7','x8']#['x2']#'x1','x2']#,'x3']#['x1']#['x1','x2','x3']#,#,['x1','x2','x3'] # <-  set the nodes which have to be trained , useful if further training is required else lsit all vars\n",
    "\n",
    "batch_size = 512#4112\n",
    "epochs = 4# <- if you want a higher numbe rof epochs, set the number higher and it loads the old model and starts from there\n",
    "learning_rate=0.01\n",
    "use_scheduler =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df0da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each NODE \n",
    "target_nodes = configuration_dict['nodes']\n",
    "\n",
    "for node in target_nodes:\n",
    "    \n",
    "    print(f'\\n----*----------*-------------*--------------- Node: {node} ------------*-----------------*-------------------*--')\n",
    "    ########################## 0. Skip nodes ###############################\n",
    "    if node not in train_list:# Skip if node is not in train_list\n",
    "        print(f\"Skipping node {node} as it's not in the training list.\")\n",
    "        continue\n",
    "    if (target_nodes[node]['node_type'] == 'source') and (target_nodes[node]['node_type'] == 'other'):# Skip unsupported types\n",
    "        print(f\"Node type : other , is not supported yet\")\n",
    "        continue\n",
    "\n",
    "    ########################## 1. Setup Paths ###############################\n",
    "    NODE_DIR = os.path.join(EXPERIMENT_DIR, f'{node}')\n",
    "    os.makedirs(NODE_DIR, exist_ok=True)\n",
    "    \n",
    "    # Check if training is complete\n",
    "    if not check_if_training_complete(node, NODE_DIR, epochs):\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    ########################## 2. Create Model ##############################\n",
    "    tram_model = get_fully_specified_tram_model(node, target_nodes, verbose=True).to(device)\n",
    "\n",
    "    \n",
    "    ########################## 3. Create Dataloaders ########################\n",
    "    train_loader, val_loader = get_dataloader(node, target_nodes, train_df, val_df, batch_size=batch_size, verbose=True)\n",
    "\n",
    "    ########################## 5. Optimizer & Scheduler ######################.\n",
    "    \n",
    "    optimizer =torch.optim.Adam(tram_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    ########################## 7. Training Loop ##############################\n",
    "    train_val_loop_v2(\n",
    "        epochs,\n",
    "        tram_model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        optimizer,\n",
    "        use_scheduler,\n",
    "        scheduler,\n",
    "        NODE_DIR,\n",
    "        save_linear_shifts=True,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0a25371",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_if_training_complete(node, NODE_DIR, epochs):\n",
    "    \"\"\"\n",
    "    Check if the training for the given node is complete.\n",
    "    Returns True if training is complete, False otherwise.\n",
    "    \"\"\"\n",
    "    MODEL_PATH, _, TRAIN_HIST_PATH, VAL_HIST_PATH = model_train_val_paths(NODE_DIR)\n",
    "    try:\n",
    "        if os.path.exists(MODEL_PATH) and os.path.exists(TRAIN_HIST_PATH) and os.path.exists(VAL_HIST_PATH):\n",
    "            with open(TRAIN_HIST_PATH, 'r') as f:\n",
    "                train_loss_hist = json.load(f)\n",
    "\n",
    "            start_epoch = len(train_loss_hist)\n",
    "            \n",
    "            # return false if already trained to skip to the next node \n",
    "            if start_epoch >= epochs:\n",
    "                print(f\"Node {node} already trained for {epochs} epochs. Skipping.\")\n",
    "                return False\n",
    "            else:\n",
    "                print(f\"Node {node} not trained yet or training incomplete. Starting from epoch {start_epoch}.\")\n",
    "                return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking training status for node {node}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def train_val_loop_v2(\n",
    "                   node,\n",
    "                   target_nodes,\n",
    "                   NODE_DIR,\n",
    "                   tram_model,\n",
    "                   train_loader,\n",
    "                   val_loader,\n",
    "                   epochs,\n",
    "                   optimizer,\n",
    "                   use_scheduler,\n",
    "                   scheduler,\n",
    "                   save_linear_shifts=False,\n",
    "                   verbose=1,\n",
    "                   device='cpu'):\n",
    "        # get all paths  for this training run\n",
    "        MODEL_PATH,LAST_MODEL_PATH,TRAIN_HIST_PATH,VAL_HIST_PATH=model_train_val_paths(NODE_DIR)\n",
    "        \n",
    "        # this is needed for the preprocessing of the inputs such that they are in the correct order\n",
    "        _, ordered_transformation_terms_in_h, _=ordered_parents(node, target_nodes)\n",
    "        \n",
    "        # this is needed for the scaling if there is a bernstein polynomial for contionous outcomes\n",
    "        min_vals = torch.tensor(target_nodes[node]['min'], dtype=torch.float32).to(device)\n",
    "        max_vals = torch.tensor(target_nodes[node]['max'], dtype=torch.float32).to(device)\n",
    "        min_max = torch.stack([min_vals, max_vals], dim=0)\n",
    "        \n",
    "        \n",
    "        ###### Load Model & History #####\n",
    "        if os.path.exists(MODEL_PATH) and os.path.exists(TRAIN_HIST_PATH) and os.path.exists(VAL_HIST_PATH):\n",
    "            print(\"Existing model found. Loading weights and history...\")\n",
    "            tram_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "            with open(TRAIN_HIST_PATH, 'r') as f:\n",
    "                train_loss_hist = json.load(f)\n",
    "            with open(VAL_HIST_PATH, 'r') as f:\n",
    "                val_loss_hist = json.load(f)\n",
    "\n",
    "            start_epoch = len(train_loss_hist)\n",
    "            best_val_loss = min(val_loss_hist)\n",
    "        else:\n",
    "            print(\"No existing model found. Starting fresh...\")\n",
    "            train_loss_hist, val_loss_hist = [], []\n",
    "            start_epoch = 0\n",
    "            best_val_loss = float('inf')\n",
    "        \n",
    "        ##### Training and Validation loop\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            epoch_start = time.time()\n",
    "\n",
    "            #####  Training #####\n",
    "            train_start = time.time()\n",
    "            train_loss = 0.0\n",
    "            tram_model.train()\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                y = y.to(device)\n",
    "                int_input, shift_list = preprocess_inputs(x, ordered_transformation_terms_in_h.values(), device=device)\n",
    "                y_pred = tram_model(int_input=int_input, shift_input=shift_list)\n",
    "                loss = contram_nll(y_pred, y, min_max=min_max)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "            if use_scheduler:\n",
    "                scheduler.step()\n",
    "                \n",
    "            train_time = time.time() - train_start\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            train_loss_hist.append(avg_train_loss)\n",
    "\n",
    "            ##### Validation #####\n",
    "            val_start = time.time()\n",
    "            val_loss = 0.0\n",
    "            tram_model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x, y in val_loader:\n",
    "                    y = y.to(device)\n",
    "                    int_input, shift_list = preprocess_inputs(x, ordered_transformation_terms_in_h.values(), device=device)\n",
    "                    y_pred = tram_model(int_input=int_input, shift_input=shift_list)\n",
    "                    loss = contram_nll(y_pred, y, min_max=min_max)\n",
    "                    val_loss += loss.item()\n",
    "            val_time = time.time() - val_start\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_loss_hist.append(avg_val_loss)\n",
    "\n",
    "            ##### Save linear shift weights #####\n",
    "\n",
    "            if save_linear_shifts and tram_model.nn_shift is not None:\n",
    "                # Define the path for the cumulative JSON file\n",
    "                shift_path = os.path.join(NODE_DIR, \"linear_shifts_all_epochs.json\")\n",
    "\n",
    "                # Load existing data if the file exists\n",
    "                if os.path.exists(shift_path):\n",
    "                    with open(shift_path, 'r') as f:\n",
    "                        all_shift_weights = json.load(f)\n",
    "                else:\n",
    "                    all_shift_weights = {}\n",
    "\n",
    "                # Prepare current epoch's shift weights\n",
    "                epoch_weights = {}\n",
    "                for i in range(len(tram_model.nn_shift)):\n",
    "                    shift_layer = tram_model.nn_shift[i]\n",
    "                    \n",
    "                    if hasattr(shift_layer, 'fc') and hasattr(shift_layer.fc, 'weight'):\n",
    "                        epoch_weights[f\"shift_{i}\"] = shift_layer.fc.weight.detach().cpu().tolist()\n",
    "                    else:\n",
    "                        print(f\"shift_{i}: 'fc' or 'weight' layer does not exist.\")\n",
    "                \n",
    "                # Add to the dictionary under current epoch\n",
    "                all_shift_weights[f\"epoch_{epoch+1}\"] = epoch_weights\n",
    "                \n",
    "                # Write back the updated dictionary\n",
    "                with open(shift_path, 'w') as f:\n",
    "                    json.dump(all_shift_weights, f)\n",
    "                if verbose > 1:\n",
    "                    print(f'shift weights: {epoch_weights}')\n",
    "                    print(f\"Appended linear shift weights for epoch {epoch+1} to: {shift_path}\")\n",
    "\n",
    "            ##### Saving #####\n",
    "            save_start = time.time()\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save(tram_model.state_dict(), MODEL_PATH)\n",
    "                if verbose > 0:\n",
    "                    print(\"Saved new best model.\")\n",
    "\n",
    "            torch.save(tram_model.state_dict(), LAST_MODEL_PATH)\n",
    "\n",
    "            with open(TRAIN_HIST_PATH, 'w') as f:\n",
    "                json.dump(train_loss_hist, f)\n",
    "            with open(VAL_HIST_PATH, 'w') as f:\n",
    "                json.dump(val_loss_hist, f)\n",
    "            save_time = time.time() - save_start\n",
    "\n",
    "            epoch_total = time.time() - epoch_start\n",
    "\n",
    "            ##### Epoch Summary #####\n",
    "            if verbose>0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "                print(f\"  [Train: {train_time:.2f}s | Val: {val_time:.2f}s | Save: {save_time:.2f}s | Total: {epoch_total:.2f}s]\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13c140",
   "metadata": {},
   "source": [
    "# 6 Inspect Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f4e2c7",
   "metadata": {},
   "source": [
    "## 6.1 Loss vs epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a93ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training_history(target_nodes,EXPERIMENT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b5922",
   "metadata": {},
   "source": [
    "## 6.2 inspect transformation function for source nodes h()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b76f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hdag_for_source_nodes(target_nodes,EXPERIMENT_DIR,device=device,xmin_plot=-2,xmax_plot=2) # TODO for other nodes funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04399f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_trafo_standart_logistic(target_nodes,EXPERIMENT_DIR,train_df,val_df,device,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e17a4b",
   "metadata": {},
   "source": [
    "### Coefficient estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f050b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # enable 3D plotting\n",
    "\n",
    "# --- Your existing setup ---\n",
    "verbose    = False\n",
    "batch_size = 4112\n",
    "device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# this x is only for overlaying your target curve in 2D plots\n",
    "x1d = torch.linspace(-1, 1, steps=1000).unsqueeze(1).to(device)  # (1000, 1)\n",
    "def f(x):\n",
    "    return 0.75 * np.arctan(5 * (x + 0.12))\n",
    "\n",
    "for node in target_nodes:\n",
    "    print(f\"\\n---- check CS of {node} ----\")\n",
    "    if target_nodes[node]['node_type'] == 'source':\n",
    "        print(\"Node type: source — skipping.\")\n",
    "        continue\n",
    "\n",
    "    # load your model\n",
    "    NODE_DIR   = os.path.join(EXPERIMENT_DIR, node)\n",
    "    model_path = os.path.join(NODE_DIR, \"best_model.pt\")\n",
    "    tram_model = get_fully_specified_tram_model(node, target_nodes, verbose=verbose).to(device)\n",
    "    tram_model.load_state_dict(torch.load(model_path))\n",
    "    tram_model.eval()\n",
    "\n",
    "\n",
    "    for i, module in enumerate(tram_model.nn_shift):\n",
    "        name = module.__class__.__name__\n",
    "        print(f\"\\nModule {i}: {name}\")\n",
    "        print(module)\n",
    "\n",
    "        if name == 'LinearShift':\n",
    "            print(\"  LinearShift weights:\")\n",
    "            print(module.fc.weight.data)\n",
    "            continue\n",
    "\n",
    "        # read wanted input dims\n",
    "        in_feats = module.fc1.in_features\n",
    "        print(f\"  expects input shape = (batch_size, {in_feats})\")\n",
    "\n",
    "        # 2-input case → 3D surface plot\n",
    "        if in_feats == 2:\n",
    "            N = 100\n",
    "            a = torch.linspace(-0, 2, steps=N, device=device)\n",
    "            b = torch.linspace(-0, 2, steps=N, device=device)\n",
    "            A, B = torch.meshgrid(a, b, indexing='ij')       # both (N, N)\n",
    "            grid = torch.stack([A, B], dim=-1).view(-1, 2)   # (N*N, 2)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                Y = module(grid).view(N, N).cpu().numpy()    # back to (N, N)\n",
    "\n",
    "            A_np = A.cpu().numpy()\n",
    "            B_np = B.cpu().numpy()\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax  = fig.add_subplot(111, projection='3d')\n",
    "            ax.plot_surface(A_np, B_np, Y, cmap='viridis', edgecolor='none')\n",
    "            ax.set_xlabel('input 1')\n",
    "            ax.set_ylabel('input 2')\n",
    "            ax.set_zlabel(f'{name}(x₁, x₂)')\n",
    "            ax.set_title(f\"{node} | Module {i}: {name} (3D surface)\")\n",
    "            plt.show()\n",
    "\n",
    "        # 1-input case → 2D scatter\n",
    "        else:\n",
    "            N = 1000\n",
    "            lin = torch.linspace(-1, 1, steps=N, device=device)\n",
    "            dummy = lin.unsqueeze(1).repeat(1, in_feats)      # (N, in_feats)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y = module(dummy).squeeze().cpu().numpy()\n",
    "\n",
    "            plt.figure()\n",
    "            plt.scatter(dummy[:, 0].cpu().numpy(), y, s=5, label=f\"{name} output\")\n",
    "            # overlay target\n",
    "            plt.scatter(x1d.cpu().numpy(), -f(x1d.cpu().numpy()), c=\"red\", s=5, label=\"target\")\n",
    "            plt.xlabel(\"input value\")\n",
    "            plt.ylabel(\"output value\")\n",
    "            plt.title(f\"{node} | Module {i}: {name} (2D)\")\n",
    "            plt.legend()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eeb428",
   "metadata": {},
   "source": [
    "# 7. Sample from Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_full_dag_chandru(target_nodes_dict,\n",
    "                            EXPERIMENT_DIR,\n",
    "                            device,\n",
    "                            do_interventions={},\n",
    "                            n= 10_000,\n",
    "                            batch_size = 32,\n",
    "                            delete_all_previously_sampled=True,\n",
    "                            verbose=True):\n",
    "    \"\"\"\n",
    "    Samples data for all nodes in a DAG defined by `conf_dict`, ensuring that each node's\n",
    "    parents are sampled before the node itself. Supports interventions on any subset of nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    conf_dict : dict\n",
    "        Dictionary defining the DAG. Each key is a node name, and each value is a config\n",
    "        dict that includes at least:\n",
    "            - 'node_type': str, either 'source' or other\n",
    "            - 'parents': list of parent node names\n",
    "            - 'min': float, minimum allowed value for the node\n",
    "            - 'max': float, maximum allowed value for the node\n",
    "\n",
    "    EXPERIMENT_DIR : str\n",
    "        Base directory where all per-node directories are located.\n",
    "\n",
    "    device : torch.device\n",
    "        The device to run computations on (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    do_interventions : dict, optional\n",
    "        A dictionary specifying interventions for some nodes. Keys are node names (str),\n",
    "        values are floats. For each intervened node, the specified value is used as the\n",
    "        sampled value for all samples, and the model is bypassed. e.g. {'x1':1.0}\n",
    "\n",
    "    n : int, optional\n",
    "        Number of samples to draw for each node (default is 10_000).\n",
    "\n",
    "    batch_size : int, optional\n",
    "        Batch size for model evaluation during sampling (default is 32).\n",
    "\n",
    "    delete_all_previously_sampled : bool, optional\n",
    "        If True, removes previously sampled data before starting (default is True).\n",
    "\n",
    "    verbose : bool, optional\n",
    "        If True, prints debug/status information (default is True).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function ensures that nodes are only sampled after their parents.\n",
    "    - Nodes with `node_type='source'` are treated as having no parents.\n",
    "    - If a node is in `do_interventions`, `sampled_chandrupatla.pt` and a dummy `latents.pt`\n",
    "      are created, enabling downstream nodes to proceed.\n",
    "    - Sampling is done using a vectorized root-finding method (Chandrupatla's algorithm).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # delete the previolusly sampled data\n",
    "    if delete_all_previously_sampled:\n",
    "        delete_all_samplings(target_nodes_dict, EXPERIMENT_DIR)\n",
    "    \n",
    "    \n",
    "    # repeat process until all nodes are sampled\n",
    "    processed_nodes=[] # stack\n",
    "    while set(processed_nodes) != set(target_nodes_dict.keys()): \n",
    "        for node in target_nodes_dict: # for each node in the conf dict\n",
    "            if node in processed_nodes:\n",
    "                if verbose :\n",
    "                    print('node is already  in sampled list')\n",
    "                continue\n",
    "            \n",
    "            _, ordered_transformation_terms_in_h, _=ordered_parents(node, target_nodes_dict)\n",
    "\n",
    "            \n",
    "            print(f'\\n----*----------*-------------*--------Sample Node: {node} ------------*-----------------*-------------------*--') \n",
    "            \n",
    "            ## 1. Paths \n",
    "            NODE_DIR = os.path.join(EXPERIMENT_DIR, f'{node}')\n",
    "            SAMPLING_DIR = os.path.join(NODE_DIR, 'sampling')\n",
    "            os.makedirs(SAMPLING_DIR, exist_ok=True)\n",
    "            \n",
    "            \n",
    "            ## 2. Check if sampled and latents already exist \n",
    "            if check_sampled_and_latents(NODE_DIR, rootfinder='chandrupatla', verbose=verbose):\n",
    "                processed_nodes.append(node)\n",
    "                continue\n",
    "            \n",
    "            ## 3. logic to make sure parents are always sampled first\n",
    "            skipping_node = False\n",
    "            if target_nodes_dict[node]['node_type'] != 'source':\n",
    "                for parent in target_nodes_dict[node]['parents']:\n",
    "                    if not check_sampled_and_latents(os.path.join(EXPERIMENT_DIR, parent), rootfinder='chandrupatla', verbose=verbose):\n",
    "                        skipping_node = True\n",
    "                        break\n",
    "                    \n",
    "            if skipping_node:\n",
    "                print(f\"Skipping {node} as parent {parent} is not sampled yet.\")\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            \n",
    "            ## INTERVENTION, if node is to be intervened on , data is just saved\n",
    "            if node in do_interventions.keys():\n",
    "                intervention_value = do_interventions[node]\n",
    "                intervention_vals = torch.full((n,), intervention_value)\n",
    "                sampled_path = os.path.join(SAMPLING_DIR, \"sampled_chandrupatla.pt\")\n",
    "                torch.save(intervention_vals, sampled_path)\n",
    "                ### dummy latents jsut for the check , not needed\n",
    "                dummy_latents = torch.full((n,), float('nan'))  \n",
    "                latents_path = os.path.join(SAMPLING_DIR, \"latents.pt\")\n",
    "                torch.save(dummy_latents, latents_path)\n",
    "                processed_nodes.append(node)\n",
    "                \n",
    "            ## no intervention, based on the sampled data from the parents though the latents for each node the observational distribution is generated    \n",
    "            else:\n",
    "                ### sampling latents\n",
    "                latent_sample = torch.tensor(logistic.rvs(size=n), dtype=torch.float32).to(device)\n",
    "                #latent_sample = truncated_logistic_sample(n=n, low=0, high=1, device=device)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"-- sampled latents\")\n",
    "                \n",
    "                ### load modelweights\n",
    "                model_path = os.path.join(NODE_DIR, \"best_model.pt\")\n",
    "                tram_model = get_fully_specified_tram_model(node, target_nodes_dict, verbose=verbose).to(device)\n",
    "                tram_model.load_state_dict(torch.load(model_path))\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"-- loaded modelweights\")\n",
    "                    \n",
    "                dataset = SamplingDataset(node=node, EXPERIMENT_DIR=EXPERIMENT_DIR, rootfinder='chandrupatla', number_of_samples=n, conf_dict=target_nodes_dict, transform=None)\n",
    "                sample_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "                \n",
    "                output_list = []\n",
    "                with torch.no_grad():\n",
    "                    for x in tqdm(sample_loader, desc=f\"h() for samples in  {node}\"):\n",
    "                        x = [xi.to(device) for xi in x]\n",
    "                        \n",
    "                        print(f\"x {x}\")\n",
    "                        int_input, shift_list = preprocess_inputs(x,ordered_transformation_terms_in_h.values(), device=device)\n",
    "                        \n",
    "                    \n",
    "                        if int_input is not None:\n",
    "                            print(f\"int_input {int_input.shape}\")\n",
    "                        if shift_list is not None:\n",
    "                            print(f\"shift_list {[s.shape for s in shift_list]}\") \n",
    "                        \n",
    "                        print([t.shape for t in x])\n",
    "                        \n",
    "                        model_outputs = tram_model(int_input=int_input, shift_input=shift_list)\n",
    "                        \n",
    "                        print(f\"model_outputs {model_outputs}\")\n",
    "                        \n",
    "                        output_list.append(model_outputs)\n",
    "                        \n",
    "                if target_nodes_dict[node]['node_type'] == 'source':\n",
    "                    if verbose:\n",
    "                        print(\"source node, Defaults to SI and 1 as inputs\")\n",
    "                    theta_single = output_list[0]['int_out'][0]\n",
    "                    theta_single = transform_intercepts_continous(theta_single)\n",
    "                    thetas_expanded = theta_single.repeat(n, 1)\n",
    "                    shifts = torch.zeros(n, device=device)\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(\"node has parents, previously sampled data is loaded for each pa(node)\")\n",
    "                    y_pred = merge_outputs(output_list, skip_nan=True)\n",
    "                    shifts = y_pred['shift_out']\n",
    "                    if shifts is None:\n",
    "                        print(\"shift_out was None; defaulting to zeros.\")\n",
    "                        shifts = torch.zeros(n, device=device)\n",
    "                    thetas = y_pred['int_out']\n",
    "                    thetas_expanded = transform_intercepts_continous(thetas).squeeze()\n",
    "                    shifts = shifts.squeeze()\n",
    "                \n",
    "                \n",
    "                \n",
    "                low = torch.full((n,), -1e5, device=device)\n",
    "                high = torch.full((n,), 1e5, device=device)\n",
    "                min_vals = torch.tensor(target_nodes_dict[node]['min'], dtype=torch.float32).to(device)\n",
    "                max_vals = torch.tensor(target_nodes_dict[node]['max'], dtype=torch.float32).to(device)\n",
    "                min_max = torch.stack([min_vals, max_vals], dim=0)\n",
    "                \n",
    "                ## Root finder using Chandrupatla's method\n",
    "                def f_vectorized(targets):\n",
    "                    return vectorized_object_function(\n",
    "                        thetas_expanded,\n",
    "                        targets,\n",
    "                        shifts,\n",
    "                        latent_sample,\n",
    "                        k_min=min_max[0],\n",
    "                        k_max=min_max[1]\n",
    "                    )\n",
    "                    \n",
    "                root = chandrupatla_root_finder(\n",
    "                    f_vectorized,\n",
    "                    low,\n",
    "                    high,\n",
    "                    max_iter=10_000,\n",
    "                    tol=1e-9\n",
    "                )\n",
    "                \n",
    "                ## Saving\n",
    "                sampled_path = os.path.join(SAMPLING_DIR, \"sampled_chandrupatla.pt\")\n",
    "                latents_path = os.path.join(SAMPLING_DIR, \"latents.pt\")\n",
    "                \n",
    "                if torch.isnan(root).any():\n",
    "                    print(f'Caution! Sampling for {node} consists of NaNs')\n",
    "                    \n",
    "                torch.save(root, sampled_path)\n",
    "                torch.save(latent_sample, latents_path)\n",
    "                \n",
    "                processed_nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_full_dag_chandru(target_nodes,\n",
    "                            EXPERIMENT_DIR,\n",
    "                            device,\n",
    "                            n= 100,\n",
    "                            batch_size = 2,\n",
    "                            delete_all_previously_sampled=True,\n",
    "                            verbose=True)     \n",
    "\n",
    "# TODO Fix bug with x6 sampling , training works, sampling has issue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0068c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training funktionerit \n",
    "#   (nn_int): ComplexInterceptDefaultTabular(\n",
    "#     (fc1): Linear(in_features=2, out_features=8, bias=True)\n",
    "#     (relu1): ReLU()\n",
    "#     (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
    "#     (relu2): ReLU()\n",
    "#     (fc3): Linear(in_features=8, out_features=20, bias=False)\n",
    "#   )\n",
    "#   (nn_shift): ModuleList(\n",
    "#     (0-1): 2 x ComplexShiftDefaultTabular(\n",
    "#       (fc1): Linear(in_features=1, out_features=64, bias=True)\n",
    "#       (relu1): ReLU()\n",
    "#       (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
    "#       (relu2): ReLU()\n",
    "#       (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
    "#       (relu3): ReLU()\n",
    "#       (fc4): Linear(in_features=64, out_features=1, bias=False)\n",
    "#     )\n",
    "#     (2): LinearShift(\n",
    "#       (fc): Linear(in_features=1, out_features=1, bias=False)\n",
    "#     )\n",
    "#   )\n",
    "# )\n",
    "# Existing model found. Loading weights and history...\n",
    "# Continuing training from epoch 3...\n",
    "# int_input torch.Size([512, 2])  \n",
    "# \n",
    "# shift_list [torch.Size([512, 1]), torch.Size([512, 1]), torch.Size([512, 1])]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## sampling does not work\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2fc3ba",
   "metadata": {},
   "source": [
    "## 7.3 Inspect Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples_vs_true(test_df,target_nodes,EXPERIMENT_DIR,rootfinder='chandrupatla')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6be30f",
   "metadata": {},
   "source": [
    "## 7.4 Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6986aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return -0.3*x  \n",
    "\n",
    "intervention_df=dgp(10_000, doX=[-1.0, None, None], seed=-1)\n",
    "sns.pairplot(intervention_df)\n",
    "plt.suptitle(\"\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_full_dag_chandru(target_nodes,\n",
    "                            EXPERIMENT_DIR,\n",
    "                            device,\n",
    "                            do_interventions={'x1':-1.0},\n",
    "                            n= 10_000,\n",
    "                            batch_size = 32,\n",
    "                            delete_all_previously_sampled=True,\n",
    "                            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cb87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples_vs_true(intervention_df,target_nodes,EXPERIMENT_DIR,rootfinder='chandrupatla')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tramdag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
